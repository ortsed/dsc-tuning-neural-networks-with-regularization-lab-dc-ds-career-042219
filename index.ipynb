{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks with Regularization - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that you had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with your previous machine learning work, you should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no \n",
    "\n",
    "In this lab, you'll use the a train-validate-test partition to get better insights of how to tune neural networks using regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, you'll see how to include a validation set. From there, you'll define and compile the model like before. However, this time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test set but also the validation set.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Apply dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing you'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Bank_complaints.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before you begin to practice some of your new tools regarding regularization and optimization, let's practice munging some data as you did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding your complaint text\n",
    "* Transforming your category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since you have quite a bit of data and training networks takes a substantial amount of time and resources, downsample in order to test your initial pipeline. Going forward, these can be interesting areas of investigation: how does your models performance change as you increase (or decrease) the size of your dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df.sample(random_state=123, n=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, you need to do some preprocessing and data manipulationg before building the neural network. \n",
    "\n",
    "Keep the 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Yyour code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(sample[\"Consumer complaint narrative\"])\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(sample[\"Consumer complaint narrative\"], mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "> **Note**: This is similar to your previous work with dummy variables. Each of the various product categories will be its own column, and each observation will be a row. In turn, each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29561</th>\n",
       "      <td>Consumer Loan</td>\n",
       "      <td>I want to file a \" Bait and Switch '' Complain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26640</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>I am an account holder for my personal, busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24498</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>they took my whole social security check i had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24594</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>This is in dispute of my Case number : XXXX. I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24249</th>\n",
       "      <td>Bank account or service</td>\n",
       "      <td>My Bluebird card that i used for bill pay was ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Product  \\\n",
       "29561            Consumer Loan   \n",
       "26640  Bank account or service   \n",
       "24498  Bank account or service   \n",
       "24594  Bank account or service   \n",
       "24249  Bank account or service   \n",
       "\n",
       "                            Consumer complaint narrative  \n",
       "29561  I want to file a \" Bait and Switch '' Complain...  \n",
       "26640  I am an account holder for my personal, busine...  \n",
       "24498  they took my whole social security check i had...  \n",
       "24594  This is in dispute of my Case number : XXXX. I...  \n",
       "24249  My Bluebird card that i used for bill pay was ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "le = LabelEncoder()\n",
    "le.fit(sample.Product)\n",
    "product_cat = le.transform(sample.Product) \n",
    "\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! \n",
    "Below, perform an appropriate train test split.\n",
    "> Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size = 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Yyour code here\n",
    "X_train = \n",
    "X_test = \n",
    "y_train = \n",
    "y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, you saw that in deep learning, you generally set aside a validation set, which is then used during hyperparameter tuning. Afterwards, when you have decided upon a final model, the test can then be used to define the final model perforance. \n",
    "\n",
    "In this example, take the first 1000 cases out of the training set to create a validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "np.random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 7)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that you used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because you are dealing with a multiclass problem (classifying the complaints into 7 classes), use a softmax classifyer in order to output 7 class probabilities per case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=\"SGD\", metrics=['accuracy'], validation_data=(val, label_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train your model! Note that this is where you also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 2.1745 - acc: 0.1721 - val_loss: 1.8687 - val_acc: 0.2150\n",
      "Epoch 2/120\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 1.8544 - acc: 0.2329 - val_loss: 1.8269 - val_acc: 0.2780\n",
      "Epoch 3/120\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.7945 - acc: 0.2984 - val_loss: 1.7748 - val_acc: 0.3130\n",
      "Epoch 4/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.7216 - acc: 0.3580 - val_loss: 1.7557 - val_acc: 0.3580\n",
      "Epoch 5/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 2.2691 - acc: 0.1687 - val_loss: 2.3151 - val_acc: 0.1220\n",
      "Epoch 6/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 2.2468 - acc: 0.1399 - val_loss: 2.2425 - val_acc: 0.1220\n",
      "Epoch 7/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 2.1902 - acc: 0.1399 - val_loss: 2.1963 - val_acc: 0.1220\n",
      "Epoch 8/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 2.1521 - acc: 0.1399 - val_loss: 2.1625 - val_acc: 0.1220\n",
      "Epoch 9/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 2.1236 - acc: 0.1399 - val_loss: 2.1364 - val_acc: 0.1220\n",
      "Epoch 10/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 2.1012 - acc: 0.1399 - val_loss: 2.1152 - val_acc: 0.1220\n",
      "Epoch 11/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 2.0827 - acc: 0.1399 - val_loss: 2.0972 - val_acc: 0.1220\n",
      "Epoch 12/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 2.0669 - acc: 0.1399 - val_loss: 2.0819 - val_acc: 0.1220\n",
      "Epoch 13/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 2.0535 - acc: 0.1399 - val_loss: 2.0684 - val_acc: 0.1220\n",
      "Epoch 14/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 2.0417 - acc: 0.1399 - val_loss: 2.0567 - val_acc: 0.1220\n",
      "Epoch 15/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2.0314 - acc: 0.1399 - val_loss: 2.0462 - val_acc: 0.1220\n",
      "Epoch 16/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 2.0221 - acc: 0.1399 - val_loss: 2.0367 - val_acc: 0.1220\n",
      "Epoch 17/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2.0138 - acc: 0.1399 - val_loss: 2.0282 - val_acc: 0.1220\n",
      "Epoch 18/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 2.0063 - acc: 0.1399 - val_loss: 2.0203 - val_acc: 0.1220\n",
      "Epoch 19/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.9994 - acc: 0.1399 - val_loss: 2.0131 - val_acc: 0.1220\n",
      "Epoch 20/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.9931 - acc: 0.1399 - val_loss: 2.0065 - val_acc: 0.1220\n",
      "Epoch 21/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.9872 - acc: 0.1399 - val_loss: 2.0003 - val_acc: 0.1220\n",
      "Epoch 22/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.9818 - acc: 0.1399 - val_loss: 1.9946 - val_acc: 0.1220\n",
      "Epoch 23/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.9768 - acc: 0.1399 - val_loss: 1.9893 - val_acc: 0.1220\n",
      "Epoch 24/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.9722 - acc: 0.1399 - val_loss: 1.9843 - val_acc: 0.1220\n",
      "Epoch 25/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.9679 - acc: 0.1399 - val_loss: 1.9797 - val_acc: 0.1220\n",
      "Epoch 26/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.9638 - acc: 0.1399 - val_loss: 1.9754 - val_acc: 0.1220\n",
      "Epoch 27/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.9601 - acc: 0.1399 - val_loss: 1.9713 - val_acc: 0.1220\n",
      "Epoch 28/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.9565 - acc: 0.1399 - val_loss: 1.9673 - val_acc: 0.1220\n",
      "Epoch 29/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.9532 - acc: 0.1399 - val_loss: 1.9637 - val_acc: 0.1220\n",
      "Epoch 30/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.9500 - acc: 0.1399 - val_loss: 1.9602 - val_acc: 0.1220\n",
      "Epoch 31/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.9471 - acc: 0.1399 - val_loss: 1.9569 - val_acc: 0.1220\n",
      "Epoch 32/120\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.9442 - acc: 0.1399 - val_loss: 1.9537 - val_acc: 0.1220\n",
      "Epoch 33/120\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.9415 - acc: 0.1399 - val_loss: 1.9507 - val_acc: 0.1220\n",
      "Epoch 34/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.9390 - acc: 0.1399 - val_loss: 1.9480 - val_acc: 0.1220\n",
      "Epoch 35/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.9367 - acc: 0.1399 - val_loss: 1.9452 - val_acc: 0.1220\n",
      "Epoch 36/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.9344 - acc: 0.1399 - val_loss: 1.9427 - val_acc: 0.1220\n",
      "Epoch 37/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.9323 - acc: 0.1399 - val_loss: 1.9403 - val_acc: 0.1220\n",
      "Epoch 38/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.9303 - acc: 0.1399 - val_loss: 1.9379 - val_acc: 0.1220\n",
      "Epoch 39/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 1.9283 - acc: 0.1399 - val_loss: 1.9357 - val_acc: 0.1220\n",
      "Epoch 40/120\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.9265 - acc: 0.1399 - val_loss: 1.9336 - val_acc: 0.1220\n",
      "Epoch 41/120\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.9247 - acc: 0.1399 - val_loss: 1.9316 - val_acc: 0.1220\n",
      "Epoch 42/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.9231 - acc: 0.1399 - val_loss: 1.9296 - val_acc: 0.1220\n",
      "Epoch 43/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.9215 - acc: 0.1399 - val_loss: 1.9277 - val_acc: 0.1220\n",
      "Epoch 44/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.9200 - acc: 0.1399 - val_loss: 1.9259 - val_acc: 0.1220\n",
      "Epoch 45/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.9185 - acc: 0.1399 - val_loss: 1.9242 - val_acc: 0.1220\n",
      "Epoch 46/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.9171 - acc: 0.1399 - val_loss: 1.9226 - val_acc: 0.1220\n",
      "Epoch 47/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.9158 - acc: 0.1399 - val_loss: 1.9210 - val_acc: 0.1220\n",
      "Epoch 48/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 1.9145 - acc: 0.1399 - val_loss: 1.9195 - val_acc: 0.1220\n",
      "Epoch 49/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.9133 - acc: 0.1399 - val_loss: 1.9180 - val_acc: 0.1220\n",
      "Epoch 50/120\n",
      "7000/7000 [==============================] - 0s 30us/step - loss: 1.9121 - acc: 0.1400 - val_loss: 1.9166 - val_acc: 0.1220\n",
      "Epoch 51/120\n",
      "7000/7000 [==============================] - ETA: 0s - loss: 1.9109 - acc: 0.140 - 0s 37us/step - loss: 1.9109 - acc: 0.1404 - val_loss: 1.9152 - val_acc: 0.1240\n",
      "Epoch 52/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.9098 - acc: 0.1411 - val_loss: 1.9139 - val_acc: 0.1270\n",
      "Epoch 53/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.9088 - acc: 0.1439 - val_loss: 1.9127 - val_acc: 0.1310\n",
      "Epoch 54/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.9078 - acc: 0.1513 - val_loss: 1.9115 - val_acc: 0.1380\n",
      "Epoch 55/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.9068 - acc: 0.1609 - val_loss: 1.9103 - val_acc: 0.1540\n",
      "Epoch 56/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.9059 - acc: 0.1757 - val_loss: 1.9092 - val_acc: 0.1770\n",
      "Epoch 57/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.9050 - acc: 0.1946 - val_loss: 1.9081 - val_acc: 0.1890\n",
      "Epoch 58/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.9041 - acc: 0.2090 - val_loss: 1.9070 - val_acc: 0.2070\n",
      "Epoch 59/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.9032 - acc: 0.2140 - val_loss: 1.9060 - val_acc: 0.2100\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.9024 - acc: 0.2123 - val_loss: 1.9050 - val_acc: 0.2090\n",
      "Epoch 61/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.9016 - acc: 0.2097 - val_loss: 1.9041 - val_acc: 0.2120\n",
      "Epoch 62/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.9008 - acc: 0.2053 - val_loss: 1.9031 - val_acc: 0.2110\n",
      "Epoch 63/120\n",
      "7000/7000 [==============================] - 0s 30us/step - loss: 1.9000 - acc: 0.1993 - val_loss: 1.9022 - val_acc: 0.2130\n",
      "Epoch 64/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.8993 - acc: 0.1956 - val_loss: 1.9012 - val_acc: 0.2110\n",
      "Epoch 65/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.8985 - acc: 0.1939 - val_loss: 1.9003 - val_acc: 0.2080\n",
      "Epoch 66/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.8978 - acc: 0.1921 - val_loss: 1.8995 - val_acc: 0.2060\n",
      "Epoch 67/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1.8971 - acc: 0.1899 - val_loss: 1.8986 - val_acc: 0.2070\n",
      "Epoch 68/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.8964 - acc: 0.1887 - val_loss: 1.8977 - val_acc: 0.2060\n",
      "Epoch 69/120\n",
      "7000/7000 [==============================] - 0s 30us/step - loss: 1.8956 - acc: 0.1876 - val_loss: 1.8969 - val_acc: 0.2050\n",
      "Epoch 70/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.8949 - acc: 0.1869 - val_loss: 1.8961 - val_acc: 0.2050\n",
      "Epoch 71/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8942 - acc: 0.1867 - val_loss: 1.8952 - val_acc: 0.2050\n",
      "Epoch 72/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.8935 - acc: 0.1861 - val_loss: 1.8944 - val_acc: 0.2040\n",
      "Epoch 73/120\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 1.8929 - acc: 0.1860 - val_loss: 1.8936 - val_acc: 0.2040\n",
      "Epoch 74/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.8922 - acc: 0.1864 - val_loss: 1.8928 - val_acc: 0.2040\n",
      "Epoch 75/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.8915 - acc: 0.1860 - val_loss: 1.8920 - val_acc: 0.2030\n",
      "Epoch 76/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.8908 - acc: 0.1857 - val_loss: 1.8913 - val_acc: 0.2030\n",
      "Epoch 77/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8901 - acc: 0.1856 - val_loss: 1.8905 - val_acc: 0.2030\n",
      "Epoch 78/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.8894 - acc: 0.1857 - val_loss: 1.8897 - val_acc: 0.2040\n",
      "Epoch 79/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.8887 - acc: 0.1860 - val_loss: 1.8889 - val_acc: 0.2040\n",
      "Epoch 80/120\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.8880 - acc: 0.1860 - val_loss: 1.8881 - val_acc: 0.2040\n",
      "Epoch 81/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.8873 - acc: 0.1859 - val_loss: 1.8873 - val_acc: 0.2040\n",
      "Epoch 82/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.8866 - acc: 0.1860 - val_loss: 1.8866 - val_acc: 0.2040\n",
      "Epoch 83/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.8859 - acc: 0.1859 - val_loss: 1.8858 - val_acc: 0.2040\n",
      "Epoch 84/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8852 - acc: 0.1860 - val_loss: 1.8850 - val_acc: 0.2040\n",
      "Epoch 85/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8844 - acc: 0.1860 - val_loss: 1.8842 - val_acc: 0.2040\n",
      "Epoch 86/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.8837 - acc: 0.1864 - val_loss: 1.8834 - val_acc: 0.2040\n",
      "Epoch 87/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.8830 - acc: 0.1866 - val_loss: 1.8826 - val_acc: 0.2040\n",
      "Epoch 88/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 1.8822 - acc: 0.1867 - val_loss: 1.8818 - val_acc: 0.2040\n",
      "Epoch 89/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 1.8814 - acc: 0.1866 - val_loss: 1.8810 - val_acc: 0.2050\n",
      "Epoch 90/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8806 - acc: 0.1866 - val_loss: 1.8801 - val_acc: 0.2050\n",
      "Epoch 91/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.8799 - acc: 0.1867 - val_loss: 1.8793 - val_acc: 0.2060\n",
      "Epoch 92/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.8790 - acc: 0.1867 - val_loss: 1.8784 - val_acc: 0.2060\n",
      "Epoch 93/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.8782 - acc: 0.1869 - val_loss: 1.8776 - val_acc: 0.2060\n",
      "Epoch 94/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.8774 - acc: 0.1869 - val_loss: 1.8767 - val_acc: 0.2050\n",
      "Epoch 95/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.8765 - acc: 0.1867 - val_loss: 1.8758 - val_acc: 0.2050\n",
      "Epoch 96/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8757 - acc: 0.1869 - val_loss: 1.8749 - val_acc: 0.2060\n",
      "Epoch 97/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.8748 - acc: 0.1869 - val_loss: 1.8740 - val_acc: 0.2060\n",
      "Epoch 98/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.8739 - acc: 0.1870 - val_loss: 1.8731 - val_acc: 0.2060\n",
      "Epoch 99/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.8730 - acc: 0.1873 - val_loss: 1.8721 - val_acc: 0.2060\n",
      "Epoch 100/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.8720 - acc: 0.1876 - val_loss: 1.8712 - val_acc: 0.2070\n",
      "Epoch 101/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.8711 - acc: 0.1876 - val_loss: 1.8702 - val_acc: 0.2070\n",
      "Epoch 102/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.8701 - acc: 0.1877 - val_loss: 1.8692 - val_acc: 0.2070\n",
      "Epoch 103/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.8691 - acc: 0.1879 - val_loss: 1.8681 - val_acc: 0.2070\n",
      "Epoch 104/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.8681 - acc: 0.1886 - val_loss: 1.8671 - val_acc: 0.2090\n",
      "Epoch 105/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.8670 - acc: 0.1887 - val_loss: 1.8660 - val_acc: 0.2090\n",
      "Epoch 106/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.8659 - acc: 0.1890 - val_loss: 1.8649 - val_acc: 0.2090\n",
      "Epoch 107/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.8648 - acc: 0.1890 - val_loss: 1.8638 - val_acc: 0.2090\n",
      "Epoch 108/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8637 - acc: 0.1893 - val_loss: 1.8627 - val_acc: 0.2110\n",
      "Epoch 109/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.8625 - acc: 0.1901 - val_loss: 1.8615 - val_acc: 0.2130\n",
      "Epoch 110/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 1.8614 - acc: 0.1906 - val_loss: 1.8603 - val_acc: 0.2140\n",
      "Epoch 111/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.8601 - acc: 0.1913 - val_loss: 1.8591 - val_acc: 0.2140\n",
      "Epoch 112/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.8589 - acc: 0.1927 - val_loss: 1.8578 - val_acc: 0.2140\n",
      "Epoch 113/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8576 - acc: 0.1941 - val_loss: 1.8565 - val_acc: 0.2140\n",
      "Epoch 114/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8562 - acc: 0.1951 - val_loss: 1.8552 - val_acc: 0.2150\n",
      "Epoch 115/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.8549 - acc: 0.1967 - val_loss: 1.8538 - val_acc: 0.2160\n",
      "Epoch 116/120\n",
      "7000/7000 [==============================] - 0s 29us/step - loss: 1.8535 - acc: 0.1986 - val_loss: 1.8524 - val_acc: 0.2170\n",
      "Epoch 117/120\n",
      "7000/7000 [==============================] - 0s 30us/step - loss: 1.8520 - acc: 0.1987 - val_loss: 1.8509 - val_acc: 0.2170\n",
      "Epoch 118/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.8505 - acc: 0.2007 - val_loss: 1.8494 - val_acc: 0.2180\n",
      "Epoch 119/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.8490 - acc: 0.2024 - val_loss: 1.8479 - val_acc: 0.2190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 1.8474 - acc: 0.2034 - val_loss: 1.8463 - val_acc: 0.2210\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 56us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 72us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.846470379965646, 0.2047142857142857]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.8545295238494872, 0.2]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! you remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss function versus the number of epochs. Be sure to include the training and the validation loss in the same plot. Then, create a second plot comparing training and validation accuracy to the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a limit around the 60th epoch. This means that you're probably **overfitting** the model to the training data when you train for many epochs past this dropoff point of around 40 epochs. Luckily, you learned how to tackle overfitting in the previous lecture! Since it seems clear that you are training too long, include early stopping at the 60th epoch first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping\n",
    "\n",
    "Below, observe how to update the model to include an earlier cutoff point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7000/7000 [==============================] - 0s 68us/step - loss: 1.9643 - acc: 0.1583 - val_loss: 1.9249 - val_acc: 0.2010\n",
      "Epoch 2/60\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.9347 - acc: 0.1833 - val_loss: 1.9114 - val_acc: 0.2230\n",
      "Epoch 3/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.9180 - acc: 0.2067 - val_loss: 1.8989 - val_acc: 0.2460\n",
      "Epoch 4/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.9028 - acc: 0.2247 - val_loss: 1.8846 - val_acc: 0.2620\n",
      "Epoch 5/60\n",
      "7000/7000 [==============================] - 0s 30us/step - loss: 1.8870 - acc: 0.2421 - val_loss: 1.8680 - val_acc: 0.2710\n",
      "Epoch 6/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.8697 - acc: 0.2550 - val_loss: 1.8503 - val_acc: 0.2870\n",
      "Epoch 7/60\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.8508 - acc: 0.2690 - val_loss: 1.8307 - val_acc: 0.2880\n",
      "Epoch 8/60\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.8302 - acc: 0.2850 - val_loss: 1.8101 - val_acc: 0.3090\n",
      "Epoch 9/60\n",
      "7000/7000 [==============================] - 0s 56us/step - loss: 1.8075 - acc: 0.3010 - val_loss: 1.7888 - val_acc: 0.3330\n",
      "Epoch 10/60\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.7830 - acc: 0.3220 - val_loss: 1.7660 - val_acc: 0.3590\n",
      "Epoch 11/60\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.7558 - acc: 0.3466 - val_loss: 1.7371 - val_acc: 0.3720\n",
      "Epoch 12/60\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.7263 - acc: 0.3587 - val_loss: 1.7105 - val_acc: 0.3960\n",
      "Epoch 13/60\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.6940 - acc: 0.3880 - val_loss: 1.6771 - val_acc: 0.4080\n",
      "Epoch 14/60\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.6586 - acc: 0.4074 - val_loss: 1.6440 - val_acc: 0.4250\n",
      "Epoch 15/60\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.6207 - acc: 0.4349 - val_loss: 1.6066 - val_acc: 0.4480\n",
      "Epoch 16/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.5797 - acc: 0.4573 - val_loss: 1.5688 - val_acc: 0.4750\n",
      "Epoch 17/60\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.5365 - acc: 0.4981 - val_loss: 1.5275 - val_acc: 0.4920\n",
      "Epoch 18/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.4914 - acc: 0.5221 - val_loss: 1.4839 - val_acc: 0.5180\n",
      "Epoch 19/60\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.4448 - acc: 0.5471 - val_loss: 1.4400 - val_acc: 0.5460\n",
      "Epoch 20/60\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.3968 - acc: 0.5681 - val_loss: 1.3975 - val_acc: 0.5700\n",
      "Epoch 21/60\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.3486 - acc: 0.5859 - val_loss: 1.3582 - val_acc: 0.5900\n",
      "Epoch 22/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.3006 - acc: 0.6106 - val_loss: 1.3103 - val_acc: 0.6140\n",
      "Epoch 23/60\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.2536 - acc: 0.6293 - val_loss: 1.2651 - val_acc: 0.6170\n",
      "Epoch 24/60\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.2084 - acc: 0.6401 - val_loss: 1.2280 - val_acc: 0.6300\n",
      "Epoch 25/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.1647 - acc: 0.6556 - val_loss: 1.1899 - val_acc: 0.6350\n",
      "Epoch 26/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.1230 - acc: 0.6706 - val_loss: 1.1520 - val_acc: 0.6470\n",
      "Epoch 27/60\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.0843 - acc: 0.6800 - val_loss: 1.1183 - val_acc: 0.6450\n",
      "Epoch 28/60\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1.0478 - acc: 0.6890 - val_loss: 1.0852 - val_acc: 0.6490\n",
      "Epoch 29/60\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.0147 - acc: 0.6974 - val_loss: 1.0550 - val_acc: 0.6600\n",
      "Epoch 30/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9829 - acc: 0.7037 - val_loss: 1.0318 - val_acc: 0.6590\n",
      "Epoch 31/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.9537 - acc: 0.7107 - val_loss: 1.0089 - val_acc: 0.6710\n",
      "Epoch 32/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.9268 - acc: 0.7153 - val_loss: 0.9819 - val_acc: 0.6820\n",
      "Epoch 33/60\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9023 - acc: 0.7196 - val_loss: 0.9620 - val_acc: 0.6860\n",
      "Epoch 34/60\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 0.8789 - acc: 0.7226 - val_loss: 0.9438 - val_acc: 0.6860\n",
      "Epoch 35/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8579 - acc: 0.7277 - val_loss: 0.9239 - val_acc: 0.6890\n",
      "Epoch 36/60\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.8383 - acc: 0.7321 - val_loss: 0.9106 - val_acc: 0.6970\n",
      "Epoch 37/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8194 - acc: 0.7351 - val_loss: 0.8977 - val_acc: 0.6930\n",
      "Epoch 38/60\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8027 - acc: 0.7383 - val_loss: 0.8822 - val_acc: 0.6900\n",
      "Epoch 39/60\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.7869 - acc: 0.7431 - val_loss: 0.8715 - val_acc: 0.6980\n",
      "Epoch 40/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.7719 - acc: 0.7459 - val_loss: 0.8587 - val_acc: 0.6990\n",
      "Epoch 41/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.7582 - acc: 0.7476 - val_loss: 0.8472 - val_acc: 0.6950\n",
      "Epoch 42/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.7453 - acc: 0.7521 - val_loss: 0.8379 - val_acc: 0.7000\n",
      "Epoch 43/60\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.7329 - acc: 0.7519 - val_loss: 0.8336 - val_acc: 0.7010\n",
      "Epoch 44/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.7212 - acc: 0.7573 - val_loss: 0.8221 - val_acc: 0.7010\n",
      "Epoch 45/60\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.7106 - acc: 0.7604 - val_loss: 0.8142 - val_acc: 0.7020\n",
      "Epoch 46/60\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.7002 - acc: 0.7644 - val_loss: 0.8086 - val_acc: 0.7040\n",
      "Epoch 47/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.6902 - acc: 0.7659 - val_loss: 0.7989 - val_acc: 0.7040\n",
      "Epoch 48/60\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.6809 - acc: 0.7697 - val_loss: 0.7954 - val_acc: 0.7060\n",
      "Epoch 49/60\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 0.6719 - acc: 0.7703 - val_loss: 0.7897 - val_acc: 0.7080\n",
      "Epoch 50/60\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.6634 - acc: 0.7741 - val_loss: 0.7856 - val_acc: 0.7100\n",
      "Epoch 51/60\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.6554 - acc: 0.7800 - val_loss: 0.7803 - val_acc: 0.7130\n",
      "Epoch 52/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.6474 - acc: 0.7773 - val_loss: 0.7744 - val_acc: 0.7140\n",
      "Epoch 53/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.6398 - acc: 0.7804 - val_loss: 0.7710 - val_acc: 0.7120\n",
      "Epoch 54/60\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.6328 - acc: 0.7843 - val_loss: 0.7705 - val_acc: 0.7150\n",
      "Epoch 55/60\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.6256 - acc: 0.7870 - val_loss: 0.7704 - val_acc: 0.7150\n",
      "Epoch 56/60\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.6187 - acc: 0.7897 - val_loss: 0.7595 - val_acc: 0.7140\n",
      "Epoch 57/60\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.6124 - acc: 0.7899 - val_loss: 0.7634 - val_acc: 0.7200\n",
      "Epoch 58/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.6055 - acc: 0.7943 - val_loss: 0.7593 - val_acc: 0.7220\n",
      "Epoch 59/60\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.5997 - acc: 0.7957 - val_loss: 0.7504 - val_acc: 0.7210\n",
      "Epoch 60/60\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.5935 - acc: 0.7989 - val_loss: 0.7526 - val_acc: 0.7190\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 55us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 0s 112us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5888252187797002, 0.7981428570747375]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7261311588287354, 0.727]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! your test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs model you originally fit.\n",
    "\n",
    "Now, take a look at how regularization techniques can further improve your model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, take a look at L2 regularization. Keras makes L2 regularization easy. Simply add the `kernel_regularizer=kernel_regulizers.l2(lamda_coeff)` parameter to any model layer. The lambda_coeff parameter determines the strength of the regularization you wish to perform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7000/7000 [==============================] - 0s 68us/step - loss: 2.6191 - acc: 0.1583 - val_loss: 2.5778 - val_acc: 0.2010\n",
      "Epoch 2/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 2.5857 - acc: 0.1834 - val_loss: 2.5607 - val_acc: 0.2230\n",
      "Epoch 3/120\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 2.5656 - acc: 0.2069 - val_loss: 2.5449 - val_acc: 0.2450\n",
      "Epoch 4/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 2.5471 - acc: 0.2250 - val_loss: 2.5275 - val_acc: 0.2620\n",
      "Epoch 5/120\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 2.5283 - acc: 0.2417 - val_loss: 2.5080 - val_acc: 0.2730\n",
      "Epoch 6/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 2.5082 - acc: 0.2559 - val_loss: 2.4874 - val_acc: 0.2870\n",
      "Epoch 7/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 2.4865 - acc: 0.2693 - val_loss: 2.4652 - val_acc: 0.2900\n",
      "Epoch 8/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2.4634 - acc: 0.2837 - val_loss: 2.4423 - val_acc: 0.3090\n",
      "Epoch 9/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 2.4385 - acc: 0.3003 - val_loss: 2.4189 - val_acc: 0.3320\n",
      "Epoch 10/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 2.4122 - acc: 0.3179 - val_loss: 2.3942 - val_acc: 0.3610\n",
      "Epoch 11/120\n",
      "7000/7000 [==============================] - 0s 69us/step - loss: 2.3835 - acc: 0.3440 - val_loss: 2.3640 - val_acc: 0.3720\n",
      "Epoch 12/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 2.3528 - acc: 0.3563 - val_loss: 2.3358 - val_acc: 0.3920\n",
      "Epoch 13/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 2.3197 - acc: 0.3836 - val_loss: 2.3019 - val_acc: 0.4060\n",
      "Epoch 14/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2.2839 - acc: 0.4036 - val_loss: 2.2682 - val_acc: 0.4250\n",
      "Epoch 15/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2.2460 - acc: 0.4291 - val_loss: 2.2309 - val_acc: 0.4440\n",
      "Epoch 16/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2.2054 - acc: 0.4537 - val_loss: 2.1934 - val_acc: 0.4650\n",
      "Epoch 17/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 2.1631 - acc: 0.4897 - val_loss: 2.1528 - val_acc: 0.4880\n",
      "Epoch 18/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2.1190 - acc: 0.5151 - val_loss: 2.1099 - val_acc: 0.5200\n",
      "Epoch 19/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 2.0737 - acc: 0.5419 - val_loss: 2.0671 - val_acc: 0.5450\n",
      "Epoch 20/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 2.0270 - acc: 0.5616 - val_loss: 2.0254 - val_acc: 0.5710\n",
      "Epoch 21/120\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.9798 - acc: 0.5801 - val_loss: 1.9863 - val_acc: 0.5890\n",
      "Epoch 22/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.9328 - acc: 0.6047 - val_loss: 1.9394 - val_acc: 0.6100\n",
      "Epoch 23/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.8864 - acc: 0.6229 - val_loss: 1.8946 - val_acc: 0.6150\n",
      "Epoch 24/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.8417 - acc: 0.6323 - val_loss: 1.8572 - val_acc: 0.6270\n",
      "Epoch 25/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.7981 - acc: 0.6503 - val_loss: 1.8189 - val_acc: 0.6340\n",
      "Epoch 26/120\n",
      "7000/7000 [==============================] - 0s 57us/step - loss: 1.7562 - acc: 0.6657 - val_loss: 1.7803 - val_acc: 0.6420\n",
      "Epoch 27/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.7169 - acc: 0.6756 - val_loss: 1.7457 - val_acc: 0.6490\n",
      "Epoch 28/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.6797 - acc: 0.6843 - val_loss: 1.7111 - val_acc: 0.6530\n",
      "Epoch 29/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 1.6455 - acc: 0.6923 - val_loss: 1.6794 - val_acc: 0.6510\n",
      "Epoch 30/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.6124 - acc: 0.6993 - val_loss: 1.6541 - val_acc: 0.6620\n",
      "Epoch 31/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.5816 - acc: 0.7041 - val_loss: 1.6295 - val_acc: 0.6670\n",
      "Epoch 32/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.5530 - acc: 0.7120 - val_loss: 1.6003 - val_acc: 0.6770\n",
      "Epoch 33/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 1.5268 - acc: 0.7174 - val_loss: 1.5783 - val_acc: 0.6850\n",
      "Epoch 34/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.5017 - acc: 0.7199 - val_loss: 1.5578 - val_acc: 0.6810\n",
      "Epoch 35/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.4789 - acc: 0.7250 - val_loss: 1.5355 - val_acc: 0.6900\n",
      "Epoch 36/120\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.4576 - acc: 0.7299 - val_loss: 1.5201 - val_acc: 0.6930\n",
      "Epoch 37/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.4369 - acc: 0.7331 - val_loss: 1.5049 - val_acc: 0.6930\n",
      "Epoch 38/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.4184 - acc: 0.7359 - val_loss: 1.4869 - val_acc: 0.6960\n",
      "Epoch 39/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.4008 - acc: 0.7420 - val_loss: 1.4740 - val_acc: 0.6970\n",
      "Epoch 40/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.3841 - acc: 0.7439 - val_loss: 1.4594 - val_acc: 0.7050\n",
      "Epoch 41/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.3685 - acc: 0.7453 - val_loss: 1.4458 - val_acc: 0.7020\n",
      "Epoch 42/120\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 1.3539 - acc: 0.7483 - val_loss: 1.4344 - val_acc: 0.7020\n",
      "Epoch 43/120\n",
      "7000/7000 [==============================] - 0s 58us/step - loss: 1.3399 - acc: 0.7503 - val_loss: 1.4279 - val_acc: 0.7030\n",
      "Epoch 44/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.3264 - acc: 0.7534 - val_loss: 1.4138 - val_acc: 0.7020\n",
      "Epoch 45/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.3141 - acc: 0.7547 - val_loss: 1.4038 - val_acc: 0.7040\n",
      "Epoch 46/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.3020 - acc: 0.7596 - val_loss: 1.3964 - val_acc: 0.7020\n",
      "Epoch 47/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.2903 - acc: 0.7596 - val_loss: 1.3848 - val_acc: 0.7020\n",
      "Epoch 48/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.2793 - acc: 0.7653 - val_loss: 1.3791 - val_acc: 0.7070\n",
      "Epoch 49/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.2687 - acc: 0.7651 - val_loss: 1.3708 - val_acc: 0.7000\n",
      "Epoch 50/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.2586 - acc: 0.7701 - val_loss: 1.3647 - val_acc: 0.7090\n",
      "Epoch 51/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.2489 - acc: 0.7710 - val_loss: 1.3576 - val_acc: 0.7120\n",
      "Epoch 52/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.2393 - acc: 0.7730 - val_loss: 1.3500 - val_acc: 0.7120\n",
      "Epoch 53/120\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 1.2301 - acc: 0.7756 - val_loss: 1.3439 - val_acc: 0.7170\n",
      "Epoch 54/120\n",
      "7000/7000 [==============================] - 0s 56us/step - loss: 1.2215 - acc: 0.7786 - val_loss: 1.3407 - val_acc: 0.7150\n",
      "Epoch 55/120\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 1.2127 - acc: 0.7813 - val_loss: 1.3395 - val_acc: 0.7150\n",
      "Epoch 56/120\n",
      "7000/7000 [==============================] - 0s 55us/step - loss: 1.2042 - acc: 0.7841 - val_loss: 1.3263 - val_acc: 0.7140\n",
      "Epoch 57/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.1964 - acc: 0.7831 - val_loss: 1.3277 - val_acc: 0.7210\n",
      "Epoch 58/120\n",
      "7000/7000 [==============================] - 0s 57us/step - loss: 1.1880 - acc: 0.7874 - val_loss: 1.3214 - val_acc: 0.7220\n",
      "Epoch 59/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.1807 - acc: 0.7880 - val_loss: 1.3105 - val_acc: 0.7190\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.1730 - acc: 0.7901 - val_loss: 1.3101 - val_acc: 0.7200\n",
      "Epoch 61/120\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 1.1659 - acc: 0.7939 - val_loss: 1.3034 - val_acc: 0.7210\n",
      "Epoch 62/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.1591 - acc: 0.7949 - val_loss: 1.2974 - val_acc: 0.7230\n",
      "Epoch 63/120\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.1516 - acc: 0.7959 - val_loss: 1.2956 - val_acc: 0.7240\n",
      "Epoch 64/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.1450 - acc: 0.7959 - val_loss: 1.2869 - val_acc: 0.7240\n",
      "Epoch 65/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.1383 - acc: 0.7994 - val_loss: 1.2834 - val_acc: 0.7210\n",
      "Epoch 66/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1.1314 - acc: 0.8040 - val_loss: 1.2773 - val_acc: 0.7240\n",
      "Epoch 67/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.1251 - acc: 0.8050 - val_loss: 1.2751 - val_acc: 0.7290\n",
      "Epoch 68/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.1186 - acc: 0.8043 - val_loss: 1.2774 - val_acc: 0.7290\n",
      "Epoch 69/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1.1125 - acc: 0.8086 - val_loss: 1.2701 - val_acc: 0.7310\n",
      "Epoch 70/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.1063 - acc: 0.8096 - val_loss: 1.2680 - val_acc: 0.7260\n",
      "Epoch 71/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.1004 - acc: 0.8119 - val_loss: 1.2576 - val_acc: 0.7290\n",
      "Epoch 72/120\n",
      "7000/7000 [==============================] - 1s 75us/step - loss: 1.0943 - acc: 0.8111 - val_loss: 1.2593 - val_acc: 0.7310\n",
      "Epoch 73/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.0884 - acc: 0.8141 - val_loss: 1.2519 - val_acc: 0.7320\n",
      "Epoch 74/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.0828 - acc: 0.8147 - val_loss: 1.2517 - val_acc: 0.7350\n",
      "Epoch 75/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.0770 - acc: 0.8193 - val_loss: 1.2471 - val_acc: 0.7320\n",
      "Epoch 76/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.0715 - acc: 0.8213 - val_loss: 1.2429 - val_acc: 0.7310\n",
      "Epoch 77/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.0658 - acc: 0.8216 - val_loss: 1.2423 - val_acc: 0.7360\n",
      "Epoch 78/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.0604 - acc: 0.8226 - val_loss: 1.2367 - val_acc: 0.7360\n",
      "Epoch 79/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.0550 - acc: 0.8233 - val_loss: 1.2379 - val_acc: 0.7370\n",
      "Epoch 80/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.0498 - acc: 0.8260 - val_loss: 1.2328 - val_acc: 0.7390\n",
      "Epoch 81/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.0447 - acc: 0.8249 - val_loss: 1.2275 - val_acc: 0.7350\n",
      "Epoch 82/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.0391 - acc: 0.8284 - val_loss: 1.2239 - val_acc: 0.7370\n",
      "Epoch 83/120\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 1.0344 - acc: 0.8303 - val_loss: 1.2213 - val_acc: 0.7320\n",
      "Epoch 84/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.0293 - acc: 0.8303 - val_loss: 1.2186 - val_acc: 0.7400\n",
      "Epoch 85/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.0240 - acc: 0.8321 - val_loss: 1.2211 - val_acc: 0.7400\n",
      "Epoch 86/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.0192 - acc: 0.8323 - val_loss: 1.2129 - val_acc: 0.7390\n",
      "Epoch 87/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.0144 - acc: 0.8374 - val_loss: 1.2182 - val_acc: 0.7380\n",
      "Epoch 88/120\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 1.0098 - acc: 0.8350 - val_loss: 1.2095 - val_acc: 0.7360\n",
      "Epoch 89/120\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.0051 - acc: 0.8384 - val_loss: 1.2121 - val_acc: 0.7410\n",
      "Epoch 90/120\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 1.0000 - acc: 0.8391 - val_loss: 1.2014 - val_acc: 0.7390\n",
      "Epoch 91/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.9953 - acc: 0.8399 - val_loss: 1.2019 - val_acc: 0.7450\n",
      "Epoch 92/120\n",
      "7000/7000 [==============================] - 0s 55us/step - loss: 0.9902 - acc: 0.8434 - val_loss: 1.1957 - val_acc: 0.7400\n",
      "Epoch 93/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.9857 - acc: 0.8434 - val_loss: 1.1933 - val_acc: 0.7310\n",
      "Epoch 94/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.9816 - acc: 0.8426 - val_loss: 1.1982 - val_acc: 0.7430\n",
      "Epoch 95/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9770 - acc: 0.8464 - val_loss: 1.1920 - val_acc: 0.7400\n",
      "Epoch 96/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.9721 - acc: 0.8457 - val_loss: 1.1923 - val_acc: 0.7370\n",
      "Epoch 97/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.9683 - acc: 0.8480 - val_loss: 1.1862 - val_acc: 0.7380\n",
      "Epoch 98/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9636 - acc: 0.8477 - val_loss: 1.1832 - val_acc: 0.7410\n",
      "Epoch 99/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.9596 - acc: 0.8491 - val_loss: 1.1803 - val_acc: 0.7370\n",
      "Epoch 100/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9549 - acc: 0.8504 - val_loss: 1.1841 - val_acc: 0.7300\n",
      "Epoch 101/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.9510 - acc: 0.8523 - val_loss: 1.1846 - val_acc: 0.7380\n",
      "Epoch 102/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9471 - acc: 0.8530 - val_loss: 1.1742 - val_acc: 0.7410\n",
      "Epoch 103/120\n",
      "7000/7000 [==============================] - 0s 56us/step - loss: 0.9423 - acc: 0.8547 - val_loss: 1.1758 - val_acc: 0.7460\n",
      "Epoch 104/120\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.9382 - acc: 0.8540 - val_loss: 1.1744 - val_acc: 0.7360\n",
      "Epoch 105/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9346 - acc: 0.8556 - val_loss: 1.1717 - val_acc: 0.7410\n",
      "Epoch 106/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9300 - acc: 0.8569 - val_loss: 1.1768 - val_acc: 0.7410\n",
      "Epoch 107/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.9264 - acc: 0.8561 - val_loss: 1.1633 - val_acc: 0.7400\n",
      "Epoch 108/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.9226 - acc: 0.8597 - val_loss: 1.1680 - val_acc: 0.7310\n",
      "Epoch 109/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.9184 - acc: 0.8606 - val_loss: 1.1639 - val_acc: 0.7360\n",
      "Epoch 110/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.9147 - acc: 0.8601 - val_loss: 1.1608 - val_acc: 0.7340\n",
      "Epoch 111/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.9102 - acc: 0.8614 - val_loss: 1.1621 - val_acc: 0.7380\n",
      "Epoch 112/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.9070 - acc: 0.8640 - val_loss: 1.1535 - val_acc: 0.7440\n",
      "Epoch 113/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 0.9030 - acc: 0.8620 - val_loss: 1.1539 - val_acc: 0.7350\n",
      "Epoch 114/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8988 - acc: 0.8643 - val_loss: 1.1549 - val_acc: 0.7480\n",
      "Epoch 115/120\n",
      "7000/7000 [==============================] - 0s 57us/step - loss: 0.8954 - acc: 0.8649 - val_loss: 1.1518 - val_acc: 0.7360\n",
      "Epoch 116/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8914 - acc: 0.8666 - val_loss: 1.1482 - val_acc: 0.7390\n",
      "Epoch 117/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.8878 - acc: 0.8674 - val_loss: 1.1478 - val_acc: 0.7400\n",
      "Epoch 118/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8840 - acc: 0.8666 - val_loss: 1.1429 - val_acc: 0.7450\n",
      "Epoch 119/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 0.8804 - acc: 0.8681 - val_loss: 1.1458 - val_acc: 0.7440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.8768 - acc: 0.8701 - val_loss: 1.1406 - val_acc: 0.7470\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. Notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 16.0684 - acc: 0.1584 - val_loss: 15.6604 - val_acc: 0.1980\n",
      "Epoch 2/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 15.3470 - acc: 0.1844 - val_loss: 14.9664 - val_acc: 0.2260\n",
      "Epoch 3/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 14.6584 - acc: 0.2073 - val_loss: 14.2921 - val_acc: 0.2470\n",
      "Epoch 4/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 13.9905 - acc: 0.2246 - val_loss: 13.6356 - val_acc: 0.2640\n",
      "Epoch 5/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 13.3415 - acc: 0.2453 - val_loss: 12.9955 - val_acc: 0.2710\n",
      "Epoch 6/120\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 12.7098 - acc: 0.2553 - val_loss: 12.3733 - val_acc: 0.2820\n",
      "Epoch 7/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 12.0954 - acc: 0.2679 - val_loss: 11.7683 - val_acc: 0.2880\n",
      "Epoch 8/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 11.4990 - acc: 0.2790 - val_loss: 11.1820 - val_acc: 0.3070\n",
      "Epoch 9/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 10.9202 - acc: 0.2909 - val_loss: 10.6146 - val_acc: 0.3260\n",
      "Epoch 10/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 10.3596 - acc: 0.3054 - val_loss: 10.0653 - val_acc: 0.3520\n",
      "Epoch 11/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 9.8158 - acc: 0.3314 - val_loss: 9.5296 - val_acc: 0.3520\n",
      "Epoch 12/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 9.2895 - acc: 0.3353 - val_loss: 9.0160 - val_acc: 0.3830\n",
      "Epoch 13/120\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 8.7817 - acc: 0.3650 - val_loss: 8.5186 - val_acc: 0.3870\n",
      "Epoch 14/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 8.2926 - acc: 0.3801 - val_loss: 8.0405 - val_acc: 0.4160\n",
      "Epoch 15/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 7.8220 - acc: 0.4129 - val_loss: 7.5804 - val_acc: 0.4350\n",
      "Epoch 16/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 7.3696 - acc: 0.4307 - val_loss: 7.1401 - val_acc: 0.4660\n",
      "Epoch 17/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 6.9361 - acc: 0.4601 - val_loss: 6.7171 - val_acc: 0.4850\n",
      "Epoch 18/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 6.5211 - acc: 0.4859 - val_loss: 6.3129 - val_acc: 0.4950\n",
      "Epoch 19/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 6.1250 - acc: 0.5049 - val_loss: 5.9277 - val_acc: 0.5130\n",
      "Epoch 20/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 5.7467 - acc: 0.5210 - val_loss: 5.5608 - val_acc: 0.5450\n",
      "Epoch 21/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 5.3867 - acc: 0.5413 - val_loss: 5.2167 - val_acc: 0.5660\n",
      "Epoch 22/120\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 5.0462 - acc: 0.5633 - val_loss: 4.8846 - val_acc: 0.5750\n",
      "Epoch 23/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 4.7252 - acc: 0.5774 - val_loss: 4.5730 - val_acc: 0.5740\n",
      "Epoch 24/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 4.4238 - acc: 0.5820 - val_loss: 4.2837 - val_acc: 0.5860\n",
      "Epoch 25/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 4.1411 - acc: 0.5973 - val_loss: 4.0135 - val_acc: 0.5960\n",
      "Epoch 26/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 3.8776 - acc: 0.6071 - val_loss: 3.7595 - val_acc: 0.5940\n",
      "Epoch 27/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 3.6340 - acc: 0.6106 - val_loss: 3.5274 - val_acc: 0.6040\n",
      "Epoch 28/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 3.4090 - acc: 0.6211 - val_loss: 3.3111 - val_acc: 0.6030\n",
      "Epoch 29/120\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 3.2031 - acc: 0.6219 - val_loss: 3.1142 - val_acc: 0.6060\n",
      "Epoch 30/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 3.0152 - acc: 0.6311 - val_loss: 2.9392 - val_acc: 0.6180\n",
      "Epoch 31/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 2.8466 - acc: 0.6366 - val_loss: 2.7820 - val_acc: 0.6280\n",
      "Epoch 32/120\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 2.6967 - acc: 0.6397 - val_loss: 2.6390 - val_acc: 0.6340\n",
      "Epoch 33/120\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 2.5657 - acc: 0.6451 - val_loss: 2.5192 - val_acc: 0.6300\n",
      "Epoch 34/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2.4517 - acc: 0.6481 - val_loss: 2.4148 - val_acc: 0.6290\n",
      "Epoch 35/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 2.3554 - acc: 0.6499 - val_loss: 2.3255 - val_acc: 0.6340\n",
      "Epoch 36/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 2.2759 - acc: 0.6546 - val_loss: 2.2561 - val_acc: 0.6450\n",
      "Epoch 37/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 2.2107 - acc: 0.6560 - val_loss: 2.1981 - val_acc: 0.6410\n",
      "Epoch 38/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 2.1607 - acc: 0.6610 - val_loss: 2.1535 - val_acc: 0.6460\n",
      "Epoch 39/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 2.1234 - acc: 0.6609 - val_loss: 2.1208 - val_acc: 0.6450\n",
      "Epoch 40/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2.0937 - acc: 0.6634 - val_loss: 2.0946 - val_acc: 0.6510\n",
      "Epoch 41/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2.0678 - acc: 0.6669 - val_loss: 2.0711 - val_acc: 0.6440\n",
      "Epoch 42/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 2.0454 - acc: 0.6699 - val_loss: 2.0479 - val_acc: 0.6490\n",
      "Epoch 43/120\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 2.0237 - acc: 0.6687 - val_loss: 2.0315 - val_acc: 0.6540\n",
      "Epoch 44/120\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 2.0037 - acc: 0.6714 - val_loss: 2.0090 - val_acc: 0.6560\n",
      "Epoch 45/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.9849 - acc: 0.6754 - val_loss: 1.9902 - val_acc: 0.6520\n",
      "Epoch 46/120\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 1.9670 - acc: 0.6771 - val_loss: 1.9763 - val_acc: 0.6590\n",
      "Epoch 47/120\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 1.9491 - acc: 0.6797 - val_loss: 1.9554 - val_acc: 0.6600\n",
      "Epoch 48/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.9327 - acc: 0.6817 - val_loss: 1.9428 - val_acc: 0.6630\n",
      "Epoch 49/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.9169 - acc: 0.6836 - val_loss: 1.9240 - val_acc: 0.6630\n",
      "Epoch 50/120\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.9012 - acc: 0.6869 - val_loss: 1.9117 - val_acc: 0.6620\n",
      "Epoch 51/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.8864 - acc: 0.6859 - val_loss: 1.8962 - val_acc: 0.6630\n",
      "Epoch 52/120\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 1.8719 - acc: 0.6870 - val_loss: 1.8827 - val_acc: 0.6640\n",
      "Epoch 53/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.8582 - acc: 0.6873 - val_loss: 1.8698 - val_acc: 0.6630\n",
      "Epoch 54/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.8451 - acc: 0.6886 - val_loss: 1.8571 - val_acc: 0.6710\n",
      "Epoch 55/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.8320 - acc: 0.6901 - val_loss: 1.8495 - val_acc: 0.6670\n",
      "Epoch 56/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.8194 - acc: 0.6909 - val_loss: 1.8349 - val_acc: 0.6690\n",
      "Epoch 57/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.8076 - acc: 0.6909 - val_loss: 1.8303 - val_acc: 0.6710\n",
      "Epoch 58/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.7956 - acc: 0.6923 - val_loss: 1.8099 - val_acc: 0.6750\n",
      "Epoch 59/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.7842 - acc: 0.6936 - val_loss: 1.7967 - val_acc: 0.6730\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.7727 - acc: 0.6944 - val_loss: 1.7897 - val_acc: 0.6750\n",
      "Epoch 61/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.7623 - acc: 0.6940 - val_loss: 1.7816 - val_acc: 0.6760\n",
      "Epoch 62/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.7516 - acc: 0.6957 - val_loss: 1.7654 - val_acc: 0.6750\n",
      "Epoch 63/120\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.7407 - acc: 0.6966 - val_loss: 1.7603 - val_acc: 0.6740\n",
      "Epoch 64/120\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 1.7310 - acc: 0.6949 - val_loss: 1.7485 - val_acc: 0.6800\n",
      "Epoch 65/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.7210 - acc: 0.6960 - val_loss: 1.7384 - val_acc: 0.6780\n",
      "Epoch 66/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.7111 - acc: 0.6994 - val_loss: 1.7263 - val_acc: 0.6680\n",
      "Epoch 67/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.7021 - acc: 0.6990 - val_loss: 1.7178 - val_acc: 0.6780\n",
      "Epoch 68/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.6926 - acc: 0.7000 - val_loss: 1.7239 - val_acc: 0.6780\n",
      "Epoch 69/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.6839 - acc: 0.6994 - val_loss: 1.7009 - val_acc: 0.6770\n",
      "Epoch 70/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 1.6747 - acc: 0.6999 - val_loss: 1.6946 - val_acc: 0.6750\n",
      "Epoch 71/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.6657 - acc: 0.7020 - val_loss: 1.6813 - val_acc: 0.6710\n",
      "Epoch 72/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.6571 - acc: 0.7024 - val_loss: 1.6756 - val_acc: 0.6810\n",
      "Epoch 73/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.6484 - acc: 0.7014 - val_loss: 1.6674 - val_acc: 0.6800\n",
      "Epoch 74/120\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 1.6397 - acc: 0.7036 - val_loss: 1.6596 - val_acc: 0.6790\n",
      "Epoch 75/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.6317 - acc: 0.7037 - val_loss: 1.6503 - val_acc: 0.6820\n",
      "Epoch 76/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.6230 - acc: 0.7044 - val_loss: 1.6440 - val_acc: 0.6790\n",
      "Epoch 77/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.6153 - acc: 0.7044 - val_loss: 1.6363 - val_acc: 0.6810\n",
      "Epoch 78/120\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.6078 - acc: 0.7057 - val_loss: 1.6308 - val_acc: 0.6810\n",
      "Epoch 79/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.5997 - acc: 0.7067 - val_loss: 1.6203 - val_acc: 0.6810\n",
      "Epoch 80/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.5921 - acc: 0.7053 - val_loss: 1.6132 - val_acc: 0.6850\n",
      "Epoch 81/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.5852 - acc: 0.7070 - val_loss: 1.6047 - val_acc: 0.6830\n",
      "Epoch 82/120\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.5773 - acc: 0.7083 - val_loss: 1.5968 - val_acc: 0.6830\n",
      "Epoch 83/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1.5697 - acc: 0.7096 - val_loss: 1.5897 - val_acc: 0.6810\n",
      "Epoch 84/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.5624 - acc: 0.7096 - val_loss: 1.5835 - val_acc: 0.6810\n",
      "Epoch 85/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.5551 - acc: 0.7086 - val_loss: 1.5839 - val_acc: 0.6850\n",
      "Epoch 86/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.5485 - acc: 0.7121 - val_loss: 1.5713 - val_acc: 0.6860\n",
      "Epoch 87/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.5411 - acc: 0.7109 - val_loss: 1.5718 - val_acc: 0.6850\n",
      "Epoch 88/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.5344 - acc: 0.7110 - val_loss: 1.5566 - val_acc: 0.6780\n",
      "Epoch 89/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.5278 - acc: 0.7099 - val_loss: 1.5590 - val_acc: 0.6840\n",
      "Epoch 90/120\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.5213 - acc: 0.7094 - val_loss: 1.5421 - val_acc: 0.6870\n",
      "Epoch 91/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.5142 - acc: 0.7119 - val_loss: 1.5438 - val_acc: 0.6830\n",
      "Epoch 92/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.5072 - acc: 0.7121 - val_loss: 1.5272 - val_acc: 0.6840\n",
      "Epoch 93/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.5009 - acc: 0.7143 - val_loss: 1.5243 - val_acc: 0.6820\n",
      "Epoch 94/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.4948 - acc: 0.7130 - val_loss: 1.5188 - val_acc: 0.6870\n",
      "Epoch 95/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.4881 - acc: 0.7136 - val_loss: 1.5117 - val_acc: 0.6850\n",
      "Epoch 96/120\n",
      "7000/7000 [==============================] - 0s 70us/step - loss: 1.4813 - acc: 0.7146 - val_loss: 1.5100 - val_acc: 0.6850\n",
      "Epoch 97/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.4757 - acc: 0.7149 - val_loss: 1.4991 - val_acc: 0.6850\n",
      "Epoch 98/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.4694 - acc: 0.7149 - val_loss: 1.4932 - val_acc: 0.6870\n",
      "Epoch 99/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.4632 - acc: 0.7154 - val_loss: 1.4868 - val_acc: 0.6900\n",
      "Epoch 100/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.4570 - acc: 0.7160 - val_loss: 1.4891 - val_acc: 0.6880\n",
      "Epoch 101/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.4515 - acc: 0.7149 - val_loss: 1.4900 - val_acc: 0.6890\n",
      "Epoch 102/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.4464 - acc: 0.7151 - val_loss: 1.4718 - val_acc: 0.6900\n",
      "Epoch 103/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.4392 - acc: 0.7160 - val_loss: 1.4696 - val_acc: 0.6940\n",
      "Epoch 104/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.4338 - acc: 0.7181 - val_loss: 1.4658 - val_acc: 0.6950\n",
      "Epoch 105/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.4286 - acc: 0.7167 - val_loss: 1.4567 - val_acc: 0.6920\n",
      "Epoch 106/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.4222 - acc: 0.7164 - val_loss: 1.4528 - val_acc: 0.6870\n",
      "Epoch 107/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.4170 - acc: 0.7177 - val_loss: 1.4445 - val_acc: 0.6950\n",
      "Epoch 108/120\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 1.4119 - acc: 0.7184 - val_loss: 1.4494 - val_acc: 0.6940\n",
      "Epoch 109/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.4061 - acc: 0.7187 - val_loss: 1.4374 - val_acc: 0.6910\n",
      "Epoch 110/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.4008 - acc: 0.7167 - val_loss: 1.4315 - val_acc: 0.7010\n",
      "Epoch 111/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.3953 - acc: 0.7207 - val_loss: 1.4339 - val_acc: 0.6930\n",
      "Epoch 112/120\n",
      "7000/7000 [==============================] - 0s 55us/step - loss: 1.3909 - acc: 0.7197 - val_loss: 1.4177 - val_acc: 0.6930\n",
      "Epoch 113/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.3848 - acc: 0.7207 - val_loss: 1.4181 - val_acc: 0.6950\n",
      "Epoch 114/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.3794 - acc: 0.7217 - val_loss: 1.4097 - val_acc: 0.7000\n",
      "Epoch 115/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.3744 - acc: 0.7216 - val_loss: 1.4088 - val_acc: 0.6940\n",
      "Epoch 116/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1.3698 - acc: 0.7213 - val_loss: 1.3994 - val_acc: 0.6950\n",
      "Epoch 117/120\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 1.3649 - acc: 0.7210 - val_loss: 1.3984 - val_acc: 0.6970\n",
      "Epoch 118/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.3595 - acc: 0.7217 - val_loss: 1.3860 - val_acc: 0.6940\n",
      "Epoch 119/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.3547 - acc: 0.7224 - val_loss: 1.3863 - val_acc: 0.6960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.3493 - acc: 0.7217 - val_loss: 1.3814 - val_acc: 0.6970\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the training and validation accuracy don't diverge as much as before. Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like you can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7000/7000 [==============================] - 1s 90us/step - loss: 16.0684 - acc: 0.1584 - val_loss: 15.6604 - val_acc: 0.1980\n",
      "Epoch 2/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 15.3470 - acc: 0.1844 - val_loss: 14.9664 - val_acc: 0.2260\n",
      "Epoch 3/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 14.6584 - acc: 0.2073 - val_loss: 14.2921 - val_acc: 0.2470\n",
      "Epoch 4/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 13.9905 - acc: 0.2246 - val_loss: 13.6356 - val_acc: 0.2640\n",
      "Epoch 5/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 13.3415 - acc: 0.2451 - val_loss: 12.9956 - val_acc: 0.2710\n",
      "Epoch 6/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 12.7098 - acc: 0.2551 - val_loss: 12.3733 - val_acc: 0.2820\n",
      "Epoch 7/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 12.0955 - acc: 0.2681 - val_loss: 11.7683 - val_acc: 0.2890\n",
      "Epoch 8/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 11.4990 - acc: 0.2793 - val_loss: 11.1820 - val_acc: 0.3070\n",
      "Epoch 9/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 10.9203 - acc: 0.2909 - val_loss: 10.6146 - val_acc: 0.3270\n",
      "Epoch 10/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 10.3597 - acc: 0.3053 - val_loss: 10.0654 - val_acc: 0.3520\n",
      "Epoch 11/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 9.8159 - acc: 0.3313 - val_loss: 9.5296 - val_acc: 0.3520\n",
      "Epoch 12/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 9.2896 - acc: 0.3347 - val_loss: 9.0160 - val_acc: 0.3830\n",
      "Epoch 13/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 8.7818 - acc: 0.3651 - val_loss: 8.5186 - val_acc: 0.3860\n",
      "Epoch 14/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 8.2927 - acc: 0.3800 - val_loss: 8.0405 - val_acc: 0.4150\n",
      "Epoch 15/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 7.8221 - acc: 0.4124 - val_loss: 7.5805 - val_acc: 0.4360\n",
      "Epoch 16/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 7.3697 - acc: 0.4304 - val_loss: 7.1401 - val_acc: 0.4660\n",
      "Epoch 17/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 6.9362 - acc: 0.4600 - val_loss: 6.7172 - val_acc: 0.4850\n",
      "Epoch 18/1000\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 6.5212 - acc: 0.4854 - val_loss: 6.3130 - val_acc: 0.4940\n",
      "Epoch 19/1000\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 6.1251 - acc: 0.5047 - val_loss: 5.9278 - val_acc: 0.5140\n",
      "Epoch 20/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 5.7467 - acc: 0.5214 - val_loss: 5.5608 - val_acc: 0.5440\n",
      "Epoch 21/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 5.3867 - acc: 0.5413 - val_loss: 5.2167 - val_acc: 0.5660\n",
      "Epoch 22/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 5.0462 - acc: 0.5634 - val_loss: 4.8846 - val_acc: 0.5750\n",
      "Epoch 23/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 4.7252 - acc: 0.5773 - val_loss: 4.5731 - val_acc: 0.5740\n",
      "Epoch 24/1000\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 4.4239 - acc: 0.5824 - val_loss: 4.2837 - val_acc: 0.5870\n",
      "Epoch 25/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 4.1411 - acc: 0.5970 - val_loss: 4.0135 - val_acc: 0.5970\n",
      "Epoch 26/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 3.8777 - acc: 0.6069 - val_loss: 3.7595 - val_acc: 0.5940\n",
      "Epoch 27/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 3.6340 - acc: 0.6101 - val_loss: 3.5275 - val_acc: 0.6040\n",
      "Epoch 28/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 3.4091 - acc: 0.6210 - val_loss: 3.3111 - val_acc: 0.6030\n",
      "Epoch 29/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 3.2031 - acc: 0.6221 - val_loss: 3.1143 - val_acc: 0.6060\n",
      "Epoch 30/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 3.0152 - acc: 0.6313 - val_loss: 2.9392 - val_acc: 0.6180\n",
      "Epoch 31/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 2.8466 - acc: 0.6366 - val_loss: 2.7821 - val_acc: 0.6280\n",
      "Epoch 32/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2.6967 - acc: 0.6397 - val_loss: 2.6390 - val_acc: 0.6340\n",
      "Epoch 33/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2.5657 - acc: 0.6451 - val_loss: 2.5192 - val_acc: 0.6280\n",
      "Epoch 34/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2.4518 - acc: 0.6480 - val_loss: 2.4149 - val_acc: 0.6290\n",
      "Epoch 35/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2.3555 - acc: 0.6497 - val_loss: 2.3255 - val_acc: 0.6340\n",
      "Epoch 36/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 2.2760 - acc: 0.6546 - val_loss: 2.2562 - val_acc: 0.6460\n",
      "Epoch 37/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 2.2107 - acc: 0.6559 - val_loss: 2.1981 - val_acc: 0.6400\n",
      "Epoch 38/1000\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 2.1607 - acc: 0.6607 - val_loss: 2.1535 - val_acc: 0.6460\n",
      "Epoch 39/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 2.1235 - acc: 0.6604 - val_loss: 2.1207 - val_acc: 0.6450\n",
      "Epoch 40/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2.0937 - acc: 0.6636 - val_loss: 2.0946 - val_acc: 0.6510\n",
      "Epoch 41/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2.0678 - acc: 0.6670 - val_loss: 2.0712 - val_acc: 0.6440\n",
      "Epoch 42/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 2.0454 - acc: 0.6700 - val_loss: 2.0479 - val_acc: 0.6490\n",
      "Epoch 43/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 2.0237 - acc: 0.6690 - val_loss: 2.0314 - val_acc: 0.6530\n",
      "Epoch 44/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 2.0037 - acc: 0.6714 - val_loss: 2.0090 - val_acc: 0.6560\n",
      "Epoch 45/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.9848 - acc: 0.6754 - val_loss: 1.9901 - val_acc: 0.6520\n",
      "Epoch 46/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.9670 - acc: 0.6771 - val_loss: 1.9762 - val_acc: 0.6590\n",
      "Epoch 47/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.9491 - acc: 0.6799 - val_loss: 1.9554 - val_acc: 0.6600\n",
      "Epoch 48/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.9327 - acc: 0.6817 - val_loss: 1.9428 - val_acc: 0.6630\n",
      "Epoch 49/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.9169 - acc: 0.6834 - val_loss: 1.9240 - val_acc: 0.6630\n",
      "Epoch 50/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.9012 - acc: 0.6870 - val_loss: 1.9116 - val_acc: 0.6620\n",
      "Epoch 51/1000\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 1.8864 - acc: 0.6859 - val_loss: 1.8961 - val_acc: 0.6620\n",
      "Epoch 52/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.8718 - acc: 0.6874 - val_loss: 1.8826 - val_acc: 0.6640\n",
      "Epoch 53/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.8582 - acc: 0.6871 - val_loss: 1.8698 - val_acc: 0.6630\n",
      "Epoch 54/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.8451 - acc: 0.6884 - val_loss: 1.8571 - val_acc: 0.6710\n",
      "Epoch 55/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.8320 - acc: 0.6901 - val_loss: 1.8496 - val_acc: 0.6670\n",
      "Epoch 56/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 1.8194 - acc: 0.6907 - val_loss: 1.8349 - val_acc: 0.6690\n",
      "Epoch 57/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.8076 - acc: 0.6909 - val_loss: 1.8303 - val_acc: 0.6710\n",
      "Epoch 58/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.7956 - acc: 0.6923 - val_loss: 1.8099 - val_acc: 0.6750\n",
      "Epoch 59/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.7842 - acc: 0.6937 - val_loss: 1.7967 - val_acc: 0.6730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.7727 - acc: 0.6946 - val_loss: 1.7897 - val_acc: 0.6750\n",
      "Epoch 61/1000\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.7623 - acc: 0.6943 - val_loss: 1.7816 - val_acc: 0.6760\n",
      "Epoch 62/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.7515 - acc: 0.6957 - val_loss: 1.7654 - val_acc: 0.6750\n",
      "Epoch 63/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.7407 - acc: 0.6967 - val_loss: 1.7603 - val_acc: 0.6740\n",
      "Epoch 64/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.7309 - acc: 0.6949 - val_loss: 1.7485 - val_acc: 0.6800\n",
      "Epoch 65/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.7210 - acc: 0.6960 - val_loss: 1.7383 - val_acc: 0.6780\n",
      "Epoch 66/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.7111 - acc: 0.6997 - val_loss: 1.7263 - val_acc: 0.6690\n",
      "Epoch 67/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.7021 - acc: 0.6989 - val_loss: 1.7177 - val_acc: 0.6780\n",
      "Epoch 68/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.6926 - acc: 0.7000 - val_loss: 1.7238 - val_acc: 0.6780\n",
      "Epoch 69/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.6838 - acc: 0.6991 - val_loss: 1.7009 - val_acc: 0.6770\n",
      "Epoch 70/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.6747 - acc: 0.6999 - val_loss: 1.6946 - val_acc: 0.6750\n",
      "Epoch 71/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.6657 - acc: 0.7020 - val_loss: 1.6814 - val_acc: 0.6720\n",
      "Epoch 72/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.6571 - acc: 0.7023 - val_loss: 1.6756 - val_acc: 0.6810\n",
      "Epoch 73/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.6484 - acc: 0.7017 - val_loss: 1.6675 - val_acc: 0.6800\n",
      "Epoch 74/1000\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 1.6397 - acc: 0.7036 - val_loss: 1.6596 - val_acc: 0.6790\n",
      "Epoch 75/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.6317 - acc: 0.7039 - val_loss: 1.6502 - val_acc: 0.6820\n",
      "Epoch 76/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.6230 - acc: 0.7041 - val_loss: 1.6441 - val_acc: 0.6790\n",
      "Epoch 77/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.6153 - acc: 0.7043 - val_loss: 1.6364 - val_acc: 0.6810\n",
      "Epoch 78/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.6078 - acc: 0.7059 - val_loss: 1.6309 - val_acc: 0.6810\n",
      "Epoch 79/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.5997 - acc: 0.7067 - val_loss: 1.6203 - val_acc: 0.6810\n",
      "Epoch 80/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.5920 - acc: 0.7051 - val_loss: 1.6131 - val_acc: 0.6850\n",
      "Epoch 81/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.5851 - acc: 0.7069 - val_loss: 1.6046 - val_acc: 0.6830\n",
      "Epoch 82/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.5773 - acc: 0.7080 - val_loss: 1.5968 - val_acc: 0.6830\n",
      "Epoch 83/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.5697 - acc: 0.7097 - val_loss: 1.5897 - val_acc: 0.6810\n",
      "Epoch 84/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.5624 - acc: 0.7096 - val_loss: 1.5835 - val_acc: 0.6810\n",
      "Epoch 85/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.5551 - acc: 0.7089 - val_loss: 1.5839 - val_acc: 0.6850\n",
      "Epoch 86/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.5485 - acc: 0.7119 - val_loss: 1.5713 - val_acc: 0.6860\n",
      "Epoch 87/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.5411 - acc: 0.7110 - val_loss: 1.5718 - val_acc: 0.6840\n",
      "Epoch 88/1000\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1.5343 - acc: 0.7113 - val_loss: 1.5565 - val_acc: 0.6780\n",
      "Epoch 89/1000\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1.5278 - acc: 0.7096 - val_loss: 1.5590 - val_acc: 0.6840\n",
      "Epoch 90/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.5213 - acc: 0.7091 - val_loss: 1.5421 - val_acc: 0.6870\n",
      "Epoch 91/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.5142 - acc: 0.7119 - val_loss: 1.5438 - val_acc: 0.6830\n",
      "Epoch 92/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.5071 - acc: 0.7123 - val_loss: 1.5272 - val_acc: 0.6840\n",
      "Epoch 93/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.5009 - acc: 0.7146 - val_loss: 1.5244 - val_acc: 0.6820\n",
      "Epoch 94/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.4948 - acc: 0.7133 - val_loss: 1.5187 - val_acc: 0.6870\n",
      "Epoch 95/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.4881 - acc: 0.7136 - val_loss: 1.5117 - val_acc: 0.6850\n",
      "Epoch 96/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.4813 - acc: 0.7146 - val_loss: 1.5101 - val_acc: 0.6850\n",
      "Epoch 97/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.4756 - acc: 0.7147 - val_loss: 1.4991 - val_acc: 0.6850\n",
      "Epoch 98/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.4694 - acc: 0.7149 - val_loss: 1.4932 - val_acc: 0.6880\n",
      "Epoch 99/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.4632 - acc: 0.7154 - val_loss: 1.4868 - val_acc: 0.6890\n",
      "Epoch 100/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.4570 - acc: 0.7161 - val_loss: 1.4891 - val_acc: 0.6880\n",
      "Epoch 101/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 1.4515 - acc: 0.7150 - val_loss: 1.4899 - val_acc: 0.6890\n",
      "Epoch 102/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.4464 - acc: 0.7151 - val_loss: 1.4718 - val_acc: 0.6890\n",
      "Epoch 103/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.4392 - acc: 0.7160 - val_loss: 1.4696 - val_acc: 0.6940\n",
      "Epoch 104/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.4337 - acc: 0.7181 - val_loss: 1.4658 - val_acc: 0.6950\n",
      "Epoch 105/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.4285 - acc: 0.7166 - val_loss: 1.4567 - val_acc: 0.6930\n",
      "Epoch 106/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.4222 - acc: 0.7163 - val_loss: 1.4528 - val_acc: 0.6880\n",
      "Epoch 107/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 1.4170 - acc: 0.7177 - val_loss: 1.4446 - val_acc: 0.6950\n",
      "Epoch 108/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.4119 - acc: 0.7184 - val_loss: 1.4493 - val_acc: 0.6940\n",
      "Epoch 109/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.4061 - acc: 0.7186 - val_loss: 1.4374 - val_acc: 0.6910\n",
      "Epoch 110/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.4008 - acc: 0.7169 - val_loss: 1.4315 - val_acc: 0.6990\n",
      "Epoch 111/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.3953 - acc: 0.7203 - val_loss: 1.4339 - val_acc: 0.6930\n",
      "Epoch 112/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.3909 - acc: 0.7197 - val_loss: 1.4177 - val_acc: 0.6930\n",
      "Epoch 113/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.3848 - acc: 0.7207 - val_loss: 1.4181 - val_acc: 0.6950\n",
      "Epoch 114/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.3794 - acc: 0.7217 - val_loss: 1.4097 - val_acc: 0.7000\n",
      "Epoch 115/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.3744 - acc: 0.7217 - val_loss: 1.4087 - val_acc: 0.6930\n",
      "Epoch 116/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.3698 - acc: 0.7213 - val_loss: 1.3994 - val_acc: 0.6950\n",
      "Epoch 117/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.3649 - acc: 0.7210 - val_loss: 1.3985 - val_acc: 0.6970\n",
      "Epoch 118/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.3595 - acc: 0.7220 - val_loss: 1.3860 - val_acc: 0.6940\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.3547 - acc: 0.7224 - val_loss: 1.3863 - val_acc: 0.6960\n",
      "Epoch 120/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.3493 - acc: 0.7217 - val_loss: 1.3814 - val_acc: 0.6970\n",
      "Epoch 121/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.3452 - acc: 0.7219 - val_loss: 1.3731 - val_acc: 0.6970\n",
      "Epoch 122/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.3402 - acc: 0.7227 - val_loss: 1.3761 - val_acc: 0.6970\n",
      "Epoch 123/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.3353 - acc: 0.7223 - val_loss: 1.3699 - val_acc: 0.6970\n",
      "Epoch 124/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.3303 - acc: 0.7249 - val_loss: 1.3623 - val_acc: 0.6930\n",
      "Epoch 125/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 1.3264 - acc: 0.7233 - val_loss: 1.3654 - val_acc: 0.7000\n",
      "Epoch 126/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.3217 - acc: 0.7251 - val_loss: 1.3496 - val_acc: 0.7010\n",
      "Epoch 127/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.3173 - acc: 0.7234 - val_loss: 1.3471 - val_acc: 0.7020\n",
      "Epoch 128/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.3125 - acc: 0.7244 - val_loss: 1.3456 - val_acc: 0.7080\n",
      "Epoch 129/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.3078 - acc: 0.7256 - val_loss: 1.3394 - val_acc: 0.6990\n",
      "Epoch 130/1000\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 1.3041 - acc: 0.7251 - val_loss: 1.3375 - val_acc: 0.7020\n",
      "Epoch 131/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.2997 - acc: 0.7259 - val_loss: 1.3313 - val_acc: 0.7030\n",
      "Epoch 132/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.2952 - acc: 0.7263 - val_loss: 1.3339 - val_acc: 0.7040\n",
      "Epoch 133/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2910 - acc: 0.7249 - val_loss: 1.3255 - val_acc: 0.7070\n",
      "Epoch 134/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2867 - acc: 0.7261 - val_loss: 1.3192 - val_acc: 0.7040\n",
      "Epoch 135/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.2823 - acc: 0.7271 - val_loss: 1.3152 - val_acc: 0.7020\n",
      "Epoch 136/1000\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 1.2791 - acc: 0.7261 - val_loss: 1.3138 - val_acc: 0.7050\n",
      "Epoch 137/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2745 - acc: 0.7276 - val_loss: 1.3039 - val_acc: 0.7000\n",
      "Epoch 138/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.2702 - acc: 0.7276 - val_loss: 1.3018 - val_acc: 0.7000\n",
      "Epoch 139/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2662 - acc: 0.7283 - val_loss: 1.3062 - val_acc: 0.7050\n",
      "Epoch 140/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2624 - acc: 0.7276 - val_loss: 1.3104 - val_acc: 0.7000\n",
      "Epoch 141/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.2596 - acc: 0.7271 - val_loss: 1.2923 - val_acc: 0.7030\n",
      "Epoch 142/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.2546 - acc: 0.7304 - val_loss: 1.2912 - val_acc: 0.7110\n",
      "Epoch 143/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.2506 - acc: 0.7280 - val_loss: 1.2917 - val_acc: 0.7010\n",
      "Epoch 144/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2469 - acc: 0.7293 - val_loss: 1.2865 - val_acc: 0.7080\n",
      "Epoch 145/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2440 - acc: 0.7300 - val_loss: 1.2807 - val_acc: 0.7080\n",
      "Epoch 146/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.2395 - acc: 0.7316 - val_loss: 1.2727 - val_acc: 0.7040\n",
      "Epoch 147/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.2362 - acc: 0.7296 - val_loss: 1.2699 - val_acc: 0.7050\n",
      "Epoch 148/1000\n",
      "7000/7000 [==============================] - 0s 58us/step - loss: 1.2324 - acc: 0.7304 - val_loss: 1.2848 - val_acc: 0.6970\n",
      "Epoch 149/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 1.2301 - acc: 0.7300 - val_loss: 1.2688 - val_acc: 0.7080\n",
      "Epoch 150/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2258 - acc: 0.7303 - val_loss: 1.2700 - val_acc: 0.7040\n",
      "Epoch 151/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.2232 - acc: 0.7304 - val_loss: 1.2643 - val_acc: 0.7040\n",
      "Epoch 152/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.2191 - acc: 0.7314 - val_loss: 1.2551 - val_acc: 0.7110\n",
      "Epoch 153/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.2154 - acc: 0.7316 - val_loss: 1.2555 - val_acc: 0.7130\n",
      "Epoch 154/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2122 - acc: 0.7313 - val_loss: 1.2505 - val_acc: 0.7090\n",
      "Epoch 155/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.2090 - acc: 0.7320 - val_loss: 1.2469 - val_acc: 0.7090\n",
      "Epoch 156/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.2061 - acc: 0.7320 - val_loss: 1.2424 - val_acc: 0.7070\n",
      "Epoch 157/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.2027 - acc: 0.7333 - val_loss: 1.2404 - val_acc: 0.7110\n",
      "Epoch 158/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.1998 - acc: 0.7323 - val_loss: 1.2377 - val_acc: 0.7120\n",
      "Epoch 159/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.1961 - acc: 0.7334 - val_loss: 1.2316 - val_acc: 0.7090\n",
      "Epoch 160/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.1936 - acc: 0.7333 - val_loss: 1.2385 - val_acc: 0.7130\n",
      "Epoch 161/1000\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 1.1908 - acc: 0.7323 - val_loss: 1.2310 - val_acc: 0.7110\n",
      "Epoch 162/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.1880 - acc: 0.7334 - val_loss: 1.2255 - val_acc: 0.7110\n",
      "Epoch 163/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.1856 - acc: 0.7327 - val_loss: 1.2249 - val_acc: 0.7120\n",
      "Epoch 164/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.1818 - acc: 0.7347 - val_loss: 1.2211 - val_acc: 0.7090\n",
      "Epoch 165/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.1794 - acc: 0.7353 - val_loss: 1.2167 - val_acc: 0.7150\n",
      "Epoch 166/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.1770 - acc: 0.7350 - val_loss: 1.2165 - val_acc: 0.7160\n",
      "Epoch 167/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.1741 - acc: 0.7334 - val_loss: 1.2135 - val_acc: 0.7170\n",
      "Epoch 168/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.1712 - acc: 0.7353 - val_loss: 1.2089 - val_acc: 0.7110\n",
      "Epoch 169/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.1684 - acc: 0.7354 - val_loss: 1.2079 - val_acc: 0.7150\n",
      "Epoch 170/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.1649 - acc: 0.7366 - val_loss: 1.2163 - val_acc: 0.7040\n",
      "Epoch 171/1000\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.1629 - acc: 0.7371 - val_loss: 1.2019 - val_acc: 0.7140\n",
      "Epoch 172/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.1602 - acc: 0.7350 - val_loss: 1.2025 - val_acc: 0.7160\n",
      "Epoch 173/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.1579 - acc: 0.7380 - val_loss: 1.1965 - val_acc: 0.7130\n",
      "Epoch 174/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.1551 - acc: 0.7373 - val_loss: 1.1946 - val_acc: 0.7110\n",
      "Epoch 175/1000\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.1528 - acc: 0.7376 - val_loss: 1.1979 - val_acc: 0.7130\n",
      "Epoch 176/1000\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.1507 - acc: 0.7374 - val_loss: 1.1906 - val_acc: 0.7150\n",
      "Epoch 177/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.1482 - acc: 0.7380 - val_loss: 1.1925 - val_acc: 0.7110\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.1455 - acc: 0.7396 - val_loss: 1.1836 - val_acc: 0.7140\n",
      "Epoch 179/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.1437 - acc: 0.7371 - val_loss: 1.1847 - val_acc: 0.7180\n",
      "Epoch 180/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.1415 - acc: 0.7391 - val_loss: 1.1839 - val_acc: 0.7150\n",
      "Epoch 181/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.1389 - acc: 0.7411 - val_loss: 1.1834 - val_acc: 0.7140\n",
      "Epoch 182/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.1369 - acc: 0.7414 - val_loss: 1.1821 - val_acc: 0.7160\n",
      "Epoch 183/1000\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 1.1339 - acc: 0.7409 - val_loss: 1.1789 - val_acc: 0.7150\n",
      "Epoch 184/1000\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 1.1329 - acc: 0.7414 - val_loss: 1.1776 - val_acc: 0.7130\n",
      "Epoch 185/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.1315 - acc: 0.7403 - val_loss: 1.1726 - val_acc: 0.7180\n",
      "Epoch 186/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.1291 - acc: 0.7410 - val_loss: 1.1718 - val_acc: 0.7150\n",
      "Epoch 187/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.1265 - acc: 0.7409 - val_loss: 1.1758 - val_acc: 0.7070\n",
      "Epoch 188/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.1247 - acc: 0.7404 - val_loss: 1.1838 - val_acc: 0.7060\n",
      "Epoch 189/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.1229 - acc: 0.7403 - val_loss: 1.1737 - val_acc: 0.7110\n",
      "Epoch 190/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.1207 - acc: 0.7411 - val_loss: 1.1667 - val_acc: 0.7150\n",
      "Epoch 191/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.1189 - acc: 0.7421 - val_loss: 1.1620 - val_acc: 0.7200\n",
      "Epoch 192/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.1159 - acc: 0.7429 - val_loss: 1.1614 - val_acc: 0.7160\n",
      "Epoch 193/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.1150 - acc: 0.7427 - val_loss: 1.1619 - val_acc: 0.7120\n",
      "Epoch 194/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.1134 - acc: 0.7416 - val_loss: 1.1545 - val_acc: 0.7200\n",
      "Epoch 195/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.1118 - acc: 0.7421 - val_loss: 1.1555 - val_acc: 0.7200\n",
      "Epoch 196/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 1.1100 - acc: 0.7424 - val_loss: 1.1529 - val_acc: 0.7190\n",
      "Epoch 197/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.1077 - acc: 0.7430 - val_loss: 1.1535 - val_acc: 0.7130\n",
      "Epoch 198/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.1066 - acc: 0.7440 - val_loss: 1.1525 - val_acc: 0.7170\n",
      "Epoch 199/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.1054 - acc: 0.7433 - val_loss: 1.1482 - val_acc: 0.7200\n",
      "Epoch 200/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.1036 - acc: 0.7436 - val_loss: 1.1570 - val_acc: 0.7130\n",
      "Epoch 201/1000\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 1.1016 - acc: 0.7454 - val_loss: 1.1494 - val_acc: 0.7170\n",
      "Epoch 202/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.1004 - acc: 0.7446 - val_loss: 1.1494 - val_acc: 0.7190\n",
      "Epoch 203/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.0991 - acc: 0.7451 - val_loss: 1.1513 - val_acc: 0.7130\n",
      "Epoch 204/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0968 - acc: 0.7444 - val_loss: 1.1438 - val_acc: 0.7220\n",
      "Epoch 205/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.0958 - acc: 0.7444 - val_loss: 1.1441 - val_acc: 0.7160\n",
      "Epoch 206/1000\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 1.0944 - acc: 0.7439 - val_loss: 1.1395 - val_acc: 0.7180\n",
      "Epoch 207/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.0921 - acc: 0.7449 - val_loss: 1.1422 - val_acc: 0.7200\n",
      "Epoch 208/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.0915 - acc: 0.7451 - val_loss: 1.1444 - val_acc: 0.7160\n",
      "Epoch 209/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0901 - acc: 0.7437 - val_loss: 1.1404 - val_acc: 0.7120\n",
      "Epoch 210/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.0896 - acc: 0.7441 - val_loss: 1.1364 - val_acc: 0.7190\n",
      "Epoch 211/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0875 - acc: 0.7460 - val_loss: 1.1421 - val_acc: 0.7120\n",
      "Epoch 212/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0855 - acc: 0.7464 - val_loss: 1.1468 - val_acc: 0.7150\n",
      "Epoch 213/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0851 - acc: 0.7479 - val_loss: 1.1509 - val_acc: 0.7090\n",
      "Epoch 214/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0835 - acc: 0.7476 - val_loss: 1.1326 - val_acc: 0.7190\n",
      "Epoch 215/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0819 - acc: 0.7467 - val_loss: 1.1335 - val_acc: 0.7200\n",
      "Epoch 216/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.0811 - acc: 0.7477 - val_loss: 1.1308 - val_acc: 0.7160\n",
      "Epoch 217/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.0791 - acc: 0.7486 - val_loss: 1.1339 - val_acc: 0.7230\n",
      "Epoch 218/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.0801 - acc: 0.7480 - val_loss: 1.1320 - val_acc: 0.7150\n",
      "Epoch 219/1000\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 1.0776 - acc: 0.7474 - val_loss: 1.1252 - val_acc: 0.7220\n",
      "Epoch 220/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 1.0762 - acc: 0.7490 - val_loss: 1.1300 - val_acc: 0.7260\n",
      "Epoch 221/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.0749 - acc: 0.7480 - val_loss: 1.1259 - val_acc: 0.7200\n",
      "Epoch 222/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.0740 - acc: 0.7467 - val_loss: 1.1255 - val_acc: 0.7140\n",
      "Epoch 223/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0726 - acc: 0.7483 - val_loss: 1.1209 - val_acc: 0.7230\n",
      "Epoch 224/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0712 - acc: 0.7503 - val_loss: 1.1251 - val_acc: 0.7210\n",
      "Epoch 225/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0700 - acc: 0.7470 - val_loss: 1.1155 - val_acc: 0.7240\n",
      "Epoch 226/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0684 - acc: 0.7491 - val_loss: 1.1529 - val_acc: 0.7040\n",
      "Epoch 227/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0693 - acc: 0.7501 - val_loss: 1.1296 - val_acc: 0.7180\n",
      "Epoch 228/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.0670 - acc: 0.7496 - val_loss: 1.1199 - val_acc: 0.7240\n",
      "Epoch 229/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0649 - acc: 0.7500 - val_loss: 1.1228 - val_acc: 0.7170\n",
      "Epoch 230/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0638 - acc: 0.7500 - val_loss: 1.1158 - val_acc: 0.7270\n",
      "Epoch 231/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0636 - acc: 0.7496 - val_loss: 1.1110 - val_acc: 0.7260\n",
      "Epoch 232/1000\n",
      "7000/7000 [==============================] - 0s 55us/step - loss: 1.0612 - acc: 0.7524 - val_loss: 1.1201 - val_acc: 0.7180\n",
      "Epoch 233/1000\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.0612 - acc: 0.7501 - val_loss: 1.1202 - val_acc: 0.7230\n",
      "Epoch 234/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.0597 - acc: 0.7500 - val_loss: 1.1149 - val_acc: 0.7260\n",
      "Epoch 235/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.0586 - acc: 0.7526 - val_loss: 1.1089 - val_acc: 0.7270\n",
      "Epoch 236/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0570 - acc: 0.7517 - val_loss: 1.1174 - val_acc: 0.7190\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 47us/step - loss: 1.0560 - acc: 0.7497 - val_loss: 1.1146 - val_acc: 0.7270\n",
      "Epoch 238/1000\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.0548 - acc: 0.7523 - val_loss: 1.1108 - val_acc: 0.7240\n",
      "Epoch 239/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0539 - acc: 0.7520 - val_loss: 1.1100 - val_acc: 0.7240\n",
      "Epoch 240/1000\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.0527 - acc: 0.7519 - val_loss: 1.1052 - val_acc: 0.7230\n",
      "Epoch 241/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.0521 - acc: 0.7530 - val_loss: 1.1042 - val_acc: 0.7180\n",
      "Epoch 242/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.0511 - acc: 0.7523 - val_loss: 1.1032 - val_acc: 0.7250\n",
      "Epoch 243/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.0496 - acc: 0.7544 - val_loss: 1.1101 - val_acc: 0.7260\n",
      "Epoch 244/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0487 - acc: 0.7526 - val_loss: 1.1179 - val_acc: 0.7230\n",
      "Epoch 245/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0475 - acc: 0.7520 - val_loss: 1.1150 - val_acc: 0.7240\n",
      "Epoch 246/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0459 - acc: 0.7534 - val_loss: 1.1069 - val_acc: 0.7300\n",
      "Epoch 247/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0450 - acc: 0.7533 - val_loss: 1.1023 - val_acc: 0.7280\n",
      "Epoch 248/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0455 - acc: 0.7520 - val_loss: 1.0971 - val_acc: 0.7260\n",
      "Epoch 249/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.0422 - acc: 0.7553 - val_loss: 1.1084 - val_acc: 0.7250\n",
      "Epoch 250/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0419 - acc: 0.7539 - val_loss: 1.1007 - val_acc: 0.7200\n",
      "Epoch 251/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0413 - acc: 0.7536 - val_loss: 1.0957 - val_acc: 0.7260\n",
      "Epoch 252/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0402 - acc: 0.7549 - val_loss: 1.0997 - val_acc: 0.7220\n",
      "Epoch 253/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.0391 - acc: 0.7543 - val_loss: 1.0963 - val_acc: 0.7280\n",
      "Epoch 254/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.0378 - acc: 0.7543 - val_loss: 1.0937 - val_acc: 0.7260\n",
      "Epoch 255/1000\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 1.0372 - acc: 0.7550 - val_loss: 1.0991 - val_acc: 0.7300\n",
      "Epoch 256/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.0354 - acc: 0.7556 - val_loss: 1.0891 - val_acc: 0.7240\n",
      "Epoch 257/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.0349 - acc: 0.7553 - val_loss: 1.0895 - val_acc: 0.7300\n",
      "Epoch 258/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.0344 - acc: 0.7551 - val_loss: 1.0911 - val_acc: 0.7280\n",
      "Epoch 259/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.0335 - acc: 0.7561 - val_loss: 1.0921 - val_acc: 0.7340\n",
      "Epoch 260/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0327 - acc: 0.7550 - val_loss: 1.0839 - val_acc: 0.7290\n",
      "Epoch 261/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0308 - acc: 0.7557 - val_loss: 1.0925 - val_acc: 0.7240\n",
      "Epoch 262/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0320 - acc: 0.7560 - val_loss: 1.0856 - val_acc: 0.7280\n",
      "Epoch 263/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0287 - acc: 0.7557 - val_loss: 1.0910 - val_acc: 0.7280\n",
      "Epoch 264/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.0293 - acc: 0.7563 - val_loss: 1.0859 - val_acc: 0.7330\n",
      "Epoch 265/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.0279 - acc: 0.7571 - val_loss: 1.0997 - val_acc: 0.7220\n",
      "Epoch 266/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.0270 - acc: 0.7560 - val_loss: 1.0911 - val_acc: 0.7250\n",
      "Epoch 267/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.0270 - acc: 0.7561 - val_loss: 1.0809 - val_acc: 0.7290\n",
      "Epoch 268/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0244 - acc: 0.7567 - val_loss: 1.0859 - val_acc: 0.7280\n",
      "Epoch 269/1000\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 1.0237 - acc: 0.7563 - val_loss: 1.0835 - val_acc: 0.7310\n",
      "Epoch 270/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.0230 - acc: 0.7559 - val_loss: 1.0810 - val_acc: 0.7250\n",
      "Epoch 271/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.0223 - acc: 0.7567 - val_loss: 1.0789 - val_acc: 0.7320\n",
      "Epoch 272/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0217 - acc: 0.7580 - val_loss: 1.0778 - val_acc: 0.7270\n",
      "Epoch 273/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.0200 - acc: 0.7586 - val_loss: 1.0772 - val_acc: 0.7280\n",
      "Epoch 274/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 1.0197 - acc: 0.7574 - val_loss: 1.0742 - val_acc: 0.7280\n",
      "Epoch 275/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.0188 - acc: 0.7589 - val_loss: 1.0882 - val_acc: 0.7290\n",
      "Epoch 276/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 1.0174 - acc: 0.7583 - val_loss: 1.0752 - val_acc: 0.7280\n",
      "Epoch 277/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 1.0163 - acc: 0.7591 - val_loss: 1.0836 - val_acc: 0.7260\n",
      "Epoch 278/1000\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 1.0159 - acc: 0.7576 - val_loss: 1.0777 - val_acc: 0.7270\n",
      "Epoch 279/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0150 - acc: 0.7593 - val_loss: 1.0719 - val_acc: 0.7270\n",
      "Epoch 280/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.0143 - acc: 0.7617 - val_loss: 1.0742 - val_acc: 0.7290\n",
      "Epoch 281/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0135 - acc: 0.7587 - val_loss: 1.0689 - val_acc: 0.7280\n",
      "Epoch 282/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0127 - acc: 0.7593 - val_loss: 1.0705 - val_acc: 0.7270\n",
      "Epoch 283/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0115 - acc: 0.7617 - val_loss: 1.0739 - val_acc: 0.7260\n",
      "Epoch 284/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.0110 - acc: 0.7606 - val_loss: 1.0746 - val_acc: 0.7250\n",
      "Epoch 285/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0097 - acc: 0.7594 - val_loss: 1.0730 - val_acc: 0.7280\n",
      "Epoch 286/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0097 - acc: 0.7607 - val_loss: 1.0655 - val_acc: 0.7260\n",
      "Epoch 287/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0090 - acc: 0.7604 - val_loss: 1.0722 - val_acc: 0.7280\n",
      "Epoch 288/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.0070 - acc: 0.7604 - val_loss: 1.0648 - val_acc: 0.7280\n",
      "Epoch 289/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.0069 - acc: 0.7617 - val_loss: 1.0749 - val_acc: 0.7240\n",
      "Epoch 290/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.0062 - acc: 0.7610 - val_loss: 1.0646 - val_acc: 0.7320\n",
      "Epoch 291/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0051 - acc: 0.7600 - val_loss: 1.0639 - val_acc: 0.7330\n",
      "Epoch 292/1000\n",
      "7000/7000 [==============================] - 0s 55us/step - loss: 1.0042 - acc: 0.7637 - val_loss: 1.0754 - val_acc: 0.7220\n",
      "Epoch 293/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 1.0039 - acc: 0.7620 - val_loss: 1.0657 - val_acc: 0.7310\n",
      "Epoch 294/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 1.0032 - acc: 0.7613 - val_loss: 1.0613 - val_acc: 0.7300\n",
      "Epoch 295/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.0026 - acc: 0.7627 - val_loss: 1.0629 - val_acc: 0.7280\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.0014 - acc: 0.7616 - val_loss: 1.0605 - val_acc: 0.7290\n",
      "Epoch 297/1000\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 1.0004 - acc: 0.7634 - val_loss: 1.0701 - val_acc: 0.7280\n",
      "Epoch 298/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 1.0017 - acc: 0.7617 - val_loss: 1.0625 - val_acc: 0.7270\n",
      "Epoch 299/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9991 - acc: 0.7611 - val_loss: 1.0600 - val_acc: 0.7300\n",
      "Epoch 300/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9989 - acc: 0.7640 - val_loss: 1.0643 - val_acc: 0.7320\n",
      "Epoch 301/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9979 - acc: 0.7617 - val_loss: 1.0609 - val_acc: 0.7280\n",
      "Epoch 302/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9976 - acc: 0.7621 - val_loss: 1.0589 - val_acc: 0.7310\n",
      "Epoch 303/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9960 - acc: 0.7634 - val_loss: 1.0658 - val_acc: 0.7240\n",
      "Epoch 304/1000\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 0.9957 - acc: 0.7627 - val_loss: 1.0579 - val_acc: 0.7330\n",
      "Epoch 305/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9946 - acc: 0.7631 - val_loss: 1.0584 - val_acc: 0.7310\n",
      "Epoch 306/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9949 - acc: 0.7627 - val_loss: 1.0637 - val_acc: 0.7300\n",
      "Epoch 307/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9930 - acc: 0.7630 - val_loss: 1.0654 - val_acc: 0.7200\n",
      "Epoch 308/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9932 - acc: 0.7630 - val_loss: 1.0657 - val_acc: 0.7190\n",
      "Epoch 309/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9920 - acc: 0.7650 - val_loss: 1.0587 - val_acc: 0.7240\n",
      "Epoch 310/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9917 - acc: 0.7641 - val_loss: 1.0493 - val_acc: 0.7280\n",
      "Epoch 311/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9907 - acc: 0.7667 - val_loss: 1.0562 - val_acc: 0.7340\n",
      "Epoch 312/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9898 - acc: 0.7654 - val_loss: 1.0531 - val_acc: 0.7320\n",
      "Epoch 313/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9890 - acc: 0.7637 - val_loss: 1.0635 - val_acc: 0.7210\n",
      "Epoch 314/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9889 - acc: 0.7644 - val_loss: 1.0603 - val_acc: 0.7290\n",
      "Epoch 315/1000\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.9875 - acc: 0.7650 - val_loss: 1.0535 - val_acc: 0.7330\n",
      "Epoch 316/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.9873 - acc: 0.7686 - val_loss: 1.0530 - val_acc: 0.7340\n",
      "Epoch 317/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9877 - acc: 0.7656 - val_loss: 1.0539 - val_acc: 0.7330\n",
      "Epoch 318/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9865 - acc: 0.7643 - val_loss: 1.0534 - val_acc: 0.7330\n",
      "Epoch 319/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9851 - acc: 0.7680 - val_loss: 1.0605 - val_acc: 0.7230\n",
      "Epoch 320/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9845 - acc: 0.7656 - val_loss: 1.0632 - val_acc: 0.7230\n",
      "Epoch 321/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9847 - acc: 0.7651 - val_loss: 1.0608 - val_acc: 0.7250\n",
      "Epoch 322/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9842 - acc: 0.7659 - val_loss: 1.0582 - val_acc: 0.7240\n",
      "Epoch 323/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9833 - acc: 0.7664 - val_loss: 1.0465 - val_acc: 0.7310\n",
      "Epoch 324/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9818 - acc: 0.7657 - val_loss: 1.0594 - val_acc: 0.7360\n",
      "Epoch 325/1000\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.9833 - acc: 0.7666 - val_loss: 1.0430 - val_acc: 0.7300\n",
      "Epoch 326/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.9807 - acc: 0.7671 - val_loss: 1.0480 - val_acc: 0.7340\n",
      "Epoch 327/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9801 - acc: 0.7673 - val_loss: 1.0542 - val_acc: 0.7250\n",
      "Epoch 328/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9799 - acc: 0.7671 - val_loss: 1.0456 - val_acc: 0.7370\n",
      "Epoch 329/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9792 - acc: 0.7659 - val_loss: 1.0556 - val_acc: 0.7280\n",
      "Epoch 330/1000\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.9791 - acc: 0.7700 - val_loss: 1.0433 - val_acc: 0.7350\n",
      "Epoch 331/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 0.9787 - acc: 0.7649 - val_loss: 1.0407 - val_acc: 0.7310\n",
      "Epoch 332/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9778 - acc: 0.7677 - val_loss: 1.0481 - val_acc: 0.7320\n",
      "Epoch 333/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9773 - acc: 0.7669 - val_loss: 1.0430 - val_acc: 0.7310\n",
      "Epoch 334/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9774 - acc: 0.7659 - val_loss: 1.0434 - val_acc: 0.7320\n",
      "Epoch 335/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9758 - acc: 0.7689 - val_loss: 1.0413 - val_acc: 0.7300\n",
      "Epoch 336/1000\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.9753 - acc: 0.7689 - val_loss: 1.0448 - val_acc: 0.7270\n",
      "Epoch 337/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9738 - acc: 0.7679 - val_loss: 1.0393 - val_acc: 0.7380\n",
      "Epoch 338/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9735 - acc: 0.7684 - val_loss: 1.0374 - val_acc: 0.7310\n",
      "Epoch 339/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9723 - acc: 0.7684 - val_loss: 1.0445 - val_acc: 0.7310\n",
      "Epoch 340/1000\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.9734 - acc: 0.7689 - val_loss: 1.0427 - val_acc: 0.7340\n",
      "Epoch 341/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.9727 - acc: 0.7693 - val_loss: 1.0563 - val_acc: 0.7300\n",
      "Epoch 342/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9726 - acc: 0.7700 - val_loss: 1.0478 - val_acc: 0.7290\n",
      "Epoch 343/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 0.9714 - acc: 0.7683 - val_loss: 1.0437 - val_acc: 0.7310\n",
      "Epoch 344/1000\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 0.9708 - acc: 0.7687 - val_loss: 1.0376 - val_acc: 0.7340\n",
      "Epoch 345/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9695 - acc: 0.7709 - val_loss: 1.0501 - val_acc: 0.7290\n",
      "Epoch 346/1000\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.9699 - acc: 0.7679 - val_loss: 1.0484 - val_acc: 0.7220\n",
      "Epoch 347/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 0.9689 - acc: 0.7683 - val_loss: 1.0352 - val_acc: 0.7350\n",
      "Epoch 348/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9685 - acc: 0.7696 - val_loss: 1.0410 - val_acc: 0.7310\n",
      "Epoch 349/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9676 - acc: 0.7697 - val_loss: 1.0305 - val_acc: 0.7350\n",
      "Epoch 350/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9681 - acc: 0.7693 - val_loss: 1.0383 - val_acc: 0.7340\n",
      "Epoch 351/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9660 - acc: 0.7701 - val_loss: 1.0382 - val_acc: 0.7300\n",
      "Epoch 352/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9661 - acc: 0.7696 - val_loss: 1.0367 - val_acc: 0.7280\n",
      "Epoch 353/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9658 - acc: 0.7696 - val_loss: 1.0310 - val_acc: 0.7330\n",
      "Epoch 354/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9651 - acc: 0.7719 - val_loss: 1.0398 - val_acc: 0.7300\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9657 - acc: 0.7711 - val_loss: 1.0369 - val_acc: 0.7340\n",
      "Epoch 356/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9647 - acc: 0.7711 - val_loss: 1.0300 - val_acc: 0.7320\n",
      "Epoch 357/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9626 - acc: 0.7726 - val_loss: 1.0461 - val_acc: 0.7250\n",
      "Epoch 358/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9632 - acc: 0.7696 - val_loss: 1.0457 - val_acc: 0.7290\n",
      "Epoch 359/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9630 - acc: 0.7693 - val_loss: 1.0335 - val_acc: 0.7350\n",
      "Epoch 360/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9618 - acc: 0.7720 - val_loss: 1.0332 - val_acc: 0.7280\n",
      "Epoch 361/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9608 - acc: 0.7716 - val_loss: 1.0282 - val_acc: 0.7300\n",
      "Epoch 362/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9612 - acc: 0.7716 - val_loss: 1.0346 - val_acc: 0.7360\n",
      "Epoch 363/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9599 - acc: 0.7730 - val_loss: 1.0541 - val_acc: 0.7190\n",
      "Epoch 364/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9606 - acc: 0.7721 - val_loss: 1.0286 - val_acc: 0.7350\n",
      "Epoch 365/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.9596 - acc: 0.7716 - val_loss: 1.0312 - val_acc: 0.7330\n",
      "Epoch 366/1000\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.9595 - acc: 0.7743 - val_loss: 1.0306 - val_acc: 0.7310\n",
      "Epoch 367/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9582 - acc: 0.7724 - val_loss: 1.0356 - val_acc: 0.7280\n",
      "Epoch 368/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9580 - acc: 0.7717 - val_loss: 1.0370 - val_acc: 0.7320\n",
      "Epoch 369/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9578 - acc: 0.7727 - val_loss: 1.0251 - val_acc: 0.7340\n",
      "Epoch 370/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9573 - acc: 0.7707 - val_loss: 1.0295 - val_acc: 0.7340\n",
      "Epoch 371/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9560 - acc: 0.7717 - val_loss: 1.0245 - val_acc: 0.7340\n",
      "Epoch 372/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9567 - acc: 0.7716 - val_loss: 1.0229 - val_acc: 0.7370\n",
      "Epoch 373/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9556 - acc: 0.7741 - val_loss: 1.0508 - val_acc: 0.7160\n",
      "Epoch 374/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9557 - acc: 0.7731 - val_loss: 1.0208 - val_acc: 0.7350\n",
      "Epoch 375/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9546 - acc: 0.7733 - val_loss: 1.0237 - val_acc: 0.7370\n",
      "Epoch 376/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9539 - acc: 0.7730 - val_loss: 1.0339 - val_acc: 0.7230\n",
      "Epoch 377/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9532 - acc: 0.7723 - val_loss: 1.0318 - val_acc: 0.7300\n",
      "Epoch 378/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9540 - acc: 0.7731 - val_loss: 1.0253 - val_acc: 0.7350\n",
      "Epoch 379/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9536 - acc: 0.7713 - val_loss: 1.0306 - val_acc: 0.7360\n",
      "Epoch 380/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.9547 - acc: 0.7730 - val_loss: 1.0185 - val_acc: 0.7320\n",
      "Epoch 381/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 0.9511 - acc: 0.7734 - val_loss: 1.0334 - val_acc: 0.7400\n",
      "Epoch 382/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9514 - acc: 0.7753 - val_loss: 1.0250 - val_acc: 0.7350\n",
      "Epoch 383/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.9518 - acc: 0.7736 - val_loss: 1.0540 - val_acc: 0.7110\n",
      "Epoch 384/1000\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 0.9504 - acc: 0.7737 - val_loss: 1.0218 - val_acc: 0.7350\n",
      "Epoch 385/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9502 - acc: 0.7737 - val_loss: 1.0327 - val_acc: 0.7290\n",
      "Epoch 386/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9493 - acc: 0.7733 - val_loss: 1.0260 - val_acc: 0.7310\n",
      "Epoch 387/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9495 - acc: 0.7726 - val_loss: 1.0203 - val_acc: 0.7310\n",
      "Epoch 388/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.9492 - acc: 0.7736 - val_loss: 1.0195 - val_acc: 0.7360\n",
      "Epoch 389/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 0.9477 - acc: 0.7741 - val_loss: 1.0182 - val_acc: 0.7350\n",
      "Epoch 390/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.9476 - acc: 0.7720 - val_loss: 1.0171 - val_acc: 0.7380\n",
      "Epoch 391/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.9468 - acc: 0.7756 - val_loss: 1.0394 - val_acc: 0.7220\n",
      "Epoch 392/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9480 - acc: 0.7734 - val_loss: 1.0271 - val_acc: 0.7320\n",
      "Epoch 393/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9465 - acc: 0.7751 - val_loss: 1.0298 - val_acc: 0.7330\n",
      "Epoch 394/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9463 - acc: 0.7749 - val_loss: 1.0178 - val_acc: 0.7350\n",
      "Epoch 395/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9451 - acc: 0.7747 - val_loss: 1.0279 - val_acc: 0.7280\n",
      "Epoch 396/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.9447 - acc: 0.7769 - val_loss: 1.0313 - val_acc: 0.7220\n",
      "Epoch 397/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9439 - acc: 0.7760 - val_loss: 1.0423 - val_acc: 0.7380\n",
      "Epoch 398/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9444 - acc: 0.7766 - val_loss: 1.0215 - val_acc: 0.7260\n",
      "Epoch 399/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9440 - acc: 0.7776 - val_loss: 1.0215 - val_acc: 0.7310\n",
      "Epoch 400/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9440 - acc: 0.7754 - val_loss: 1.0276 - val_acc: 0.7290\n",
      "Epoch 401/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9422 - acc: 0.7774 - val_loss: 1.0121 - val_acc: 0.7370\n",
      "Epoch 402/1000\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 0.9426 - acc: 0.7756 - val_loss: 1.0146 - val_acc: 0.7390\n",
      "Epoch 403/1000\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 0.9428 - acc: 0.7747 - val_loss: 1.0232 - val_acc: 0.7280\n",
      "Epoch 404/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9414 - acc: 0.7766 - val_loss: 1.0159 - val_acc: 0.7350\n",
      "Epoch 405/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9418 - acc: 0.7770 - val_loss: 1.0220 - val_acc: 0.7340\n",
      "Epoch 406/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9415 - acc: 0.7764 - val_loss: 1.0112 - val_acc: 0.7330\n",
      "Epoch 407/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9405 - acc: 0.7769 - val_loss: 1.0137 - val_acc: 0.7350\n",
      "Epoch 408/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9394 - acc: 0.7769 - val_loss: 1.0100 - val_acc: 0.7350\n",
      "Epoch 409/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9404 - acc: 0.7777 - val_loss: 1.0172 - val_acc: 0.7300\n",
      "Epoch 410/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9402 - acc: 0.7767 - val_loss: 1.0096 - val_acc: 0.7320\n",
      "Epoch 411/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9394 - acc: 0.7771 - val_loss: 1.0182 - val_acc: 0.7330\n",
      "Epoch 412/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9387 - acc: 0.7771 - val_loss: 1.0267 - val_acc: 0.7250\n",
      "Epoch 413/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9383 - acc: 0.7757 - val_loss: 1.0120 - val_acc: 0.7340\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9383 - acc: 0.7767 - val_loss: 1.0227 - val_acc: 0.7290\n",
      "Epoch 415/1000\n",
      "7000/7000 [==============================] - 0s 55us/step - loss: 0.9385 - acc: 0.7771 - val_loss: 1.0110 - val_acc: 0.7350\n",
      "Epoch 416/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9362 - acc: 0.7777 - val_loss: 1.0162 - val_acc: 0.7370\n",
      "Epoch 417/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9374 - acc: 0.7781 - val_loss: 1.0134 - val_acc: 0.7360\n",
      "Epoch 418/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9374 - acc: 0.7780 - val_loss: 1.0136 - val_acc: 0.7330\n",
      "Epoch 419/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9368 - acc: 0.7773 - val_loss: 1.0212 - val_acc: 0.7310\n",
      "Epoch 420/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9357 - acc: 0.7789 - val_loss: 1.0203 - val_acc: 0.7340\n",
      "Epoch 421/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9352 - acc: 0.7776 - val_loss: 1.0129 - val_acc: 0.7380\n",
      "Epoch 422/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9346 - acc: 0.7797 - val_loss: 1.0158 - val_acc: 0.7290\n",
      "Epoch 423/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9344 - acc: 0.7766 - val_loss: 1.0099 - val_acc: 0.7320\n",
      "Epoch 424/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 0.9348 - acc: 0.7780 - val_loss: 1.0160 - val_acc: 0.7320\n",
      "Epoch 425/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.9332 - acc: 0.7773 - val_loss: 1.0238 - val_acc: 0.7310\n",
      "Epoch 426/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9348 - acc: 0.7781 - val_loss: 1.0260 - val_acc: 0.7230\n",
      "Epoch 427/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 0.9343 - acc: 0.7799 - val_loss: 1.0221 - val_acc: 0.7250\n",
      "Epoch 428/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9331 - acc: 0.7793 - val_loss: 1.0057 - val_acc: 0.7370\n",
      "Epoch 429/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9326 - acc: 0.7786 - val_loss: 1.0104 - val_acc: 0.7310\n",
      "Epoch 430/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9321 - acc: 0.7766 - val_loss: 1.0240 - val_acc: 0.7270\n",
      "Epoch 431/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9314 - acc: 0.7800 - val_loss: 1.0066 - val_acc: 0.7340\n",
      "Epoch 432/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9315 - acc: 0.7774 - val_loss: 1.0132 - val_acc: 0.7380\n",
      "Epoch 433/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9306 - acc: 0.7813 - val_loss: 1.0066 - val_acc: 0.7360\n",
      "Epoch 434/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9300 - acc: 0.7796 - val_loss: 1.0119 - val_acc: 0.7330\n",
      "Epoch 435/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9311 - acc: 0.7776 - val_loss: 1.0157 - val_acc: 0.7290\n",
      "Epoch 436/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9307 - acc: 0.7771 - val_loss: 1.0092 - val_acc: 0.7320\n",
      "Epoch 437/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9296 - acc: 0.7800 - val_loss: 1.0160 - val_acc: 0.7270\n",
      "Epoch 438/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9304 - acc: 0.7797 - val_loss: 1.0267 - val_acc: 0.7270\n",
      "Epoch 439/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9303 - acc: 0.7791 - val_loss: 1.0302 - val_acc: 0.7210\n",
      "Epoch 440/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9292 - acc: 0.7787 - val_loss: 1.0096 - val_acc: 0.7290\n",
      "Epoch 441/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9284 - acc: 0.7810 - val_loss: 1.0095 - val_acc: 0.7270\n",
      "Epoch 442/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9280 - acc: 0.7789 - val_loss: 1.0104 - val_acc: 0.7300\n",
      "Epoch 443/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9290 - acc: 0.7794 - val_loss: 1.0123 - val_acc: 0.7280\n",
      "Epoch 444/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9273 - acc: 0.7819 - val_loss: 1.0138 - val_acc: 0.7290\n",
      "Epoch 445/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9261 - acc: 0.7814 - val_loss: 1.0163 - val_acc: 0.7250\n",
      "Epoch 446/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9268 - acc: 0.7807 - val_loss: 1.0138 - val_acc: 0.7340\n",
      "Epoch 447/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9259 - acc: 0.7777 - val_loss: 1.0018 - val_acc: 0.7340\n",
      "Epoch 448/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9261 - acc: 0.7821 - val_loss: 1.0116 - val_acc: 0.7340\n",
      "Epoch 449/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9257 - acc: 0.7803 - val_loss: 1.0021 - val_acc: 0.7370\n",
      "Epoch 450/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9264 - acc: 0.7806 - val_loss: 1.0051 - val_acc: 0.7350\n",
      "Epoch 451/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9249 - acc: 0.7781 - val_loss: 1.0130 - val_acc: 0.7390\n",
      "Epoch 452/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9245 - acc: 0.7803 - val_loss: 0.9993 - val_acc: 0.7350\n",
      "Epoch 453/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9239 - acc: 0.7781 - val_loss: 1.0042 - val_acc: 0.7350\n",
      "Epoch 454/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9239 - acc: 0.7819 - val_loss: 1.0177 - val_acc: 0.7250\n",
      "Epoch 455/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9238 - acc: 0.7803 - val_loss: 1.0027 - val_acc: 0.7290\n",
      "Epoch 456/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9231 - acc: 0.7787 - val_loss: 1.0044 - val_acc: 0.7390\n",
      "Epoch 457/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9232 - acc: 0.7830 - val_loss: 1.0343 - val_acc: 0.7180\n",
      "Epoch 458/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9230 - acc: 0.7819 - val_loss: 1.0049 - val_acc: 0.7330\n",
      "Epoch 459/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.9214 - acc: 0.7819 - val_loss: 1.0291 - val_acc: 0.7270\n",
      "Epoch 460/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.9234 - acc: 0.7783 - val_loss: 1.0061 - val_acc: 0.7290\n",
      "Epoch 461/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9213 - acc: 0.7814 - val_loss: 1.0016 - val_acc: 0.7300\n",
      "Epoch 462/1000\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 0.9221 - acc: 0.7837 - val_loss: 1.0125 - val_acc: 0.7310\n",
      "Epoch 463/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9224 - acc: 0.7790 - val_loss: 1.0071 - val_acc: 0.7310\n",
      "Epoch 464/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 0.9198 - acc: 0.7826 - val_loss: 1.0037 - val_acc: 0.7350\n",
      "Epoch 465/1000\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.9201 - acc: 0.7821 - val_loss: 1.0017 - val_acc: 0.7400\n",
      "Epoch 466/1000\n",
      "7000/7000 [==============================] - 0s 58us/step - loss: 0.9203 - acc: 0.7827 - val_loss: 0.9972 - val_acc: 0.7350\n",
      "Epoch 467/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9207 - acc: 0.7823 - val_loss: 1.0088 - val_acc: 0.7400\n",
      "Epoch 468/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9206 - acc: 0.7807 - val_loss: 1.0276 - val_acc: 0.7240\n",
      "Epoch 469/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9203 - acc: 0.7807 - val_loss: 1.0198 - val_acc: 0.7260\n",
      "Epoch 470/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9193 - acc: 0.7806 - val_loss: 0.9971 - val_acc: 0.7320\n",
      "Epoch 471/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9185 - acc: 0.7810 - val_loss: 1.0007 - val_acc: 0.7310\n",
      "Epoch 472/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9194 - acc: 0.7821 - val_loss: 1.0122 - val_acc: 0.7310\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9180 - acc: 0.7847 - val_loss: 1.0164 - val_acc: 0.7250\n",
      "Epoch 474/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9178 - acc: 0.7817 - val_loss: 0.9968 - val_acc: 0.7370\n",
      "Epoch 475/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9184 - acc: 0.7833 - val_loss: 1.0264 - val_acc: 0.7190\n",
      "Epoch 476/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9168 - acc: 0.7836 - val_loss: 1.0005 - val_acc: 0.7350\n",
      "Epoch 477/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9173 - acc: 0.7846 - val_loss: 1.0132 - val_acc: 0.7260\n",
      "Epoch 478/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9175 - acc: 0.7817 - val_loss: 0.9943 - val_acc: 0.7330\n",
      "Epoch 479/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9163 - acc: 0.7830 - val_loss: 0.9968 - val_acc: 0.7340\n",
      "Epoch 480/1000\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.9156 - acc: 0.7814 - val_loss: 1.0357 - val_acc: 0.7140\n",
      "Epoch 481/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9158 - acc: 0.7820 - val_loss: 0.9990 - val_acc: 0.7360\n",
      "Epoch 482/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9152 - acc: 0.7840 - val_loss: 0.9958 - val_acc: 0.7320\n",
      "Epoch 483/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9155 - acc: 0.7857 - val_loss: 0.9999 - val_acc: 0.7370\n",
      "Epoch 484/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9152 - acc: 0.7799 - val_loss: 1.0039 - val_acc: 0.7290\n",
      "Epoch 485/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9143 - acc: 0.7837 - val_loss: 0.9959 - val_acc: 0.7340\n",
      "Epoch 486/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9138 - acc: 0.7854 - val_loss: 1.0186 - val_acc: 0.7220\n",
      "Epoch 487/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9146 - acc: 0.7827 - val_loss: 1.0486 - val_acc: 0.7200\n",
      "Epoch 488/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9156 - acc: 0.7834 - val_loss: 1.0141 - val_acc: 0.7360\n",
      "Epoch 489/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9134 - acc: 0.7849 - val_loss: 1.0064 - val_acc: 0.7300\n",
      "Epoch 490/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9124 - acc: 0.7834 - val_loss: 0.9972 - val_acc: 0.7330\n",
      "Epoch 491/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9118 - acc: 0.7831 - val_loss: 1.0147 - val_acc: 0.7220\n",
      "Epoch 492/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9123 - acc: 0.7827 - val_loss: 1.0021 - val_acc: 0.7330\n",
      "Epoch 493/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9123 - acc: 0.7839 - val_loss: 1.0061 - val_acc: 0.7320\n",
      "Epoch 494/1000\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 0.9115 - acc: 0.7837 - val_loss: 1.0143 - val_acc: 0.7300\n",
      "Epoch 495/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9123 - acc: 0.7810 - val_loss: 0.9971 - val_acc: 0.7360\n",
      "Epoch 496/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9094 - acc: 0.7829 - val_loss: 1.0019 - val_acc: 0.7370\n",
      "Epoch 497/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9109 - acc: 0.7850 - val_loss: 0.9935 - val_acc: 0.7330\n",
      "Epoch 498/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9106 - acc: 0.7841 - val_loss: 1.0020 - val_acc: 0.7320\n",
      "Epoch 499/1000\n",
      "7000/7000 [==============================] - 0s 55us/step - loss: 0.9112 - acc: 0.7836 - val_loss: 0.9997 - val_acc: 0.7370\n",
      "Epoch 500/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9107 - acc: 0.7843 - val_loss: 0.9951 - val_acc: 0.7320\n",
      "Epoch 501/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9090 - acc: 0.7837 - val_loss: 1.0082 - val_acc: 0.7280\n",
      "Epoch 502/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9096 - acc: 0.7871 - val_loss: 1.0086 - val_acc: 0.7260\n",
      "Epoch 503/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9089 - acc: 0.7843 - val_loss: 1.0119 - val_acc: 0.7280\n",
      "Epoch 504/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9108 - acc: 0.7856 - val_loss: 0.9931 - val_acc: 0.7360\n",
      "Epoch 505/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.9093 - acc: 0.7844 - val_loss: 1.0040 - val_acc: 0.7360\n",
      "Epoch 506/1000\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.9080 - acc: 0.7857 - val_loss: 1.0038 - val_acc: 0.7270\n",
      "Epoch 507/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9075 - acc: 0.7851 - val_loss: 0.9930 - val_acc: 0.7370\n",
      "Epoch 508/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9075 - acc: 0.7871 - val_loss: 1.0037 - val_acc: 0.7260\n",
      "Epoch 509/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.9068 - acc: 0.7859 - val_loss: 1.0138 - val_acc: 0.7240\n",
      "Epoch 510/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.9073 - acc: 0.7869 - val_loss: 0.9962 - val_acc: 0.7290\n",
      "Epoch 511/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9075 - acc: 0.7850 - val_loss: 1.0078 - val_acc: 0.7260\n",
      "Epoch 512/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9061 - acc: 0.7846 - val_loss: 1.0051 - val_acc: 0.7420\n",
      "Epoch 513/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.9061 - acc: 0.7863 - val_loss: 1.0042 - val_acc: 0.7340\n",
      "Epoch 514/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9067 - acc: 0.7861 - val_loss: 1.0039 - val_acc: 0.7420\n",
      "Epoch 515/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9062 - acc: 0.7883 - val_loss: 1.0222 - val_acc: 0.7190\n",
      "Epoch 516/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9051 - acc: 0.7861 - val_loss: 0.9947 - val_acc: 0.7330\n",
      "Epoch 517/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9048 - acc: 0.7854 - val_loss: 0.9920 - val_acc: 0.7310\n",
      "Epoch 518/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9039 - acc: 0.7853 - val_loss: 1.0002 - val_acc: 0.7350\n",
      "Epoch 519/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9050 - acc: 0.7870 - val_loss: 0.9912 - val_acc: 0.7360\n",
      "Epoch 520/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9052 - acc: 0.7866 - val_loss: 1.0030 - val_acc: 0.7300\n",
      "Epoch 521/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9048 - acc: 0.7861 - val_loss: 0.9898 - val_acc: 0.7330\n",
      "Epoch 522/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9044 - acc: 0.7853 - val_loss: 0.9970 - val_acc: 0.7330\n",
      "Epoch 523/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9038 - acc: 0.7860 - val_loss: 0.9967 - val_acc: 0.7370\n",
      "Epoch 524/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9036 - acc: 0.7879 - val_loss: 0.9945 - val_acc: 0.7370\n",
      "Epoch 525/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9025 - acc: 0.7859 - val_loss: 0.9883 - val_acc: 0.7330\n",
      "Epoch 526/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9026 - acc: 0.7873 - val_loss: 1.0240 - val_acc: 0.7230\n",
      "Epoch 527/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9026 - acc: 0.7856 - val_loss: 1.0022 - val_acc: 0.7390\n",
      "Epoch 528/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9029 - acc: 0.7877 - val_loss: 0.9978 - val_acc: 0.7330\n",
      "Epoch 529/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9020 - acc: 0.7861 - val_loss: 1.0107 - val_acc: 0.7280\n",
      "Epoch 530/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.9024 - acc: 0.7874 - val_loss: 0.9936 - val_acc: 0.7350\n",
      "Epoch 531/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.9017 - acc: 0.7856 - val_loss: 0.9993 - val_acc: 0.7310\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.9002 - acc: 0.7884 - val_loss: 0.9929 - val_acc: 0.7300\n",
      "Epoch 533/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.9008 - acc: 0.7880 - val_loss: 1.0029 - val_acc: 0.7290\n",
      "Epoch 534/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.9005 - acc: 0.7859 - val_loss: 1.0091 - val_acc: 0.7270\n",
      "Epoch 535/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8994 - acc: 0.7894 - val_loss: 0.9945 - val_acc: 0.7330\n",
      "Epoch 536/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.9001 - acc: 0.7893 - val_loss: 1.0067 - val_acc: 0.7250\n",
      "Epoch 537/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8995 - acc: 0.7869 - val_loss: 0.9908 - val_acc: 0.7380\n",
      "Epoch 538/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8987 - acc: 0.7879 - val_loss: 0.9932 - val_acc: 0.7280\n",
      "Epoch 539/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8993 - acc: 0.7880 - val_loss: 0.9941 - val_acc: 0.7330\n",
      "Epoch 540/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8987 - acc: 0.7903 - val_loss: 0.9881 - val_acc: 0.7330\n",
      "Epoch 541/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8999 - acc: 0.7879 - val_loss: 1.0017 - val_acc: 0.7280\n",
      "Epoch 542/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8992 - acc: 0.7879 - val_loss: 0.9970 - val_acc: 0.7310\n",
      "Epoch 543/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8979 - acc: 0.7881 - val_loss: 0.9873 - val_acc: 0.7330\n",
      "Epoch 544/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8988 - acc: 0.7866 - val_loss: 1.0187 - val_acc: 0.7250\n",
      "Epoch 545/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8985 - acc: 0.7886 - val_loss: 0.9888 - val_acc: 0.7340\n",
      "Epoch 546/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8971 - acc: 0.7884 - val_loss: 0.9897 - val_acc: 0.7290\n",
      "Epoch 547/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8988 - acc: 0.7870 - val_loss: 0.9867 - val_acc: 0.7310\n",
      "Epoch 548/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8962 - acc: 0.7880 - val_loss: 0.9859 - val_acc: 0.7320\n",
      "Epoch 549/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8976 - acc: 0.7879 - val_loss: 0.9895 - val_acc: 0.7300\n",
      "Epoch 550/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8960 - acc: 0.7877 - val_loss: 1.0070 - val_acc: 0.7250\n",
      "Epoch 551/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8958 - acc: 0.7891 - val_loss: 0.9995 - val_acc: 0.7280\n",
      "Epoch 552/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8961 - acc: 0.7881 - val_loss: 1.0166 - val_acc: 0.7370\n",
      "Epoch 553/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8984 - acc: 0.7864 - val_loss: 1.0157 - val_acc: 0.7310\n",
      "Epoch 554/1000\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 0.8961 - acc: 0.7867 - val_loss: 0.9947 - val_acc: 0.7330\n",
      "Epoch 555/1000\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 0.8937 - acc: 0.7911 - val_loss: 0.9915 - val_acc: 0.7300\n",
      "Epoch 556/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8949 - acc: 0.7906 - val_loss: 0.9916 - val_acc: 0.7400\n",
      "Epoch 557/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8948 - acc: 0.7870 - val_loss: 1.0036 - val_acc: 0.7270\n",
      "Epoch 558/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8939 - acc: 0.7871 - val_loss: 0.9911 - val_acc: 0.7360\n",
      "Epoch 559/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8929 - acc: 0.7903 - val_loss: 1.0129 - val_acc: 0.7200\n",
      "Epoch 560/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8940 - acc: 0.7894 - val_loss: 0.9897 - val_acc: 0.7370\n",
      "Epoch 561/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8933 - acc: 0.7881 - val_loss: 0.9894 - val_acc: 0.7340\n",
      "Epoch 562/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8932 - acc: 0.7886 - val_loss: 0.9897 - val_acc: 0.7280\n",
      "Epoch 563/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8921 - acc: 0.7896 - val_loss: 0.9937 - val_acc: 0.7320\n",
      "Epoch 564/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8924 - acc: 0.7897 - val_loss: 0.9844 - val_acc: 0.7300\n",
      "Epoch 565/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8912 - acc: 0.7897 - val_loss: 0.9909 - val_acc: 0.7250\n",
      "Epoch 566/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8918 - acc: 0.7894 - val_loss: 1.0085 - val_acc: 0.7300\n",
      "Epoch 567/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8914 - acc: 0.7891 - val_loss: 0.9888 - val_acc: 0.7290\n",
      "Epoch 568/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 0.8899 - acc: 0.7907 - val_loss: 1.0012 - val_acc: 0.7280\n",
      "Epoch 569/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8908 - acc: 0.7896 - val_loss: 0.9943 - val_acc: 0.7250\n",
      "Epoch 570/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8920 - acc: 0.7887 - val_loss: 0.9831 - val_acc: 0.7340\n",
      "Epoch 571/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8911 - acc: 0.7890 - val_loss: 0.9837 - val_acc: 0.7340\n",
      "Epoch 572/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8905 - acc: 0.7890 - val_loss: 0.9909 - val_acc: 0.7260\n",
      "Epoch 573/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 0.8900 - acc: 0.7900 - val_loss: 0.9842 - val_acc: 0.7330\n",
      "Epoch 574/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8898 - acc: 0.7900 - val_loss: 0.9969 - val_acc: 0.7300\n",
      "Epoch 575/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8914 - acc: 0.7893 - val_loss: 0.9892 - val_acc: 0.7300\n",
      "Epoch 576/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8898 - acc: 0.7893 - val_loss: 0.9824 - val_acc: 0.7320\n",
      "Epoch 577/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8897 - acc: 0.7910 - val_loss: 0.9964 - val_acc: 0.7340\n",
      "Epoch 578/1000\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 0.8889 - acc: 0.7923 - val_loss: 0.9932 - val_acc: 0.7290\n",
      "Epoch 579/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.8885 - acc: 0.7913 - val_loss: 0.9915 - val_acc: 0.7280\n",
      "Epoch 580/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8890 - acc: 0.7900 - val_loss: 0.9897 - val_acc: 0.7320\n",
      "Epoch 581/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8873 - acc: 0.7920 - val_loss: 0.9858 - val_acc: 0.7320\n",
      "Epoch 582/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8876 - acc: 0.7909 - val_loss: 0.9813 - val_acc: 0.7380\n",
      "Epoch 583/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8876 - acc: 0.7906 - val_loss: 1.0052 - val_acc: 0.7230\n",
      "Epoch 584/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8880 - acc: 0.7904 - val_loss: 0.9904 - val_acc: 0.7280\n",
      "Epoch 585/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8869 - acc: 0.7883 - val_loss: 1.0202 - val_acc: 0.7310\n",
      "Epoch 586/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8872 - acc: 0.7924 - val_loss: 0.9839 - val_acc: 0.7310\n",
      "Epoch 587/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8851 - acc: 0.7924 - val_loss: 0.9840 - val_acc: 0.7280\n",
      "Epoch 588/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8863 - acc: 0.7906 - val_loss: 0.9887 - val_acc: 0.7370\n",
      "Epoch 589/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8876 - acc: 0.7904 - val_loss: 1.0020 - val_acc: 0.7300\n",
      "Epoch 590/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8853 - acc: 0.7914 - val_loss: 0.9956 - val_acc: 0.7250\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8853 - acc: 0.7901 - val_loss: 0.9811 - val_acc: 0.7330\n",
      "Epoch 592/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8872 - acc: 0.7903 - val_loss: 1.0020 - val_acc: 0.7360\n",
      "Epoch 593/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8860 - acc: 0.7911 - val_loss: 0.9951 - val_acc: 0.7230\n",
      "Epoch 594/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8831 - acc: 0.7904 - val_loss: 0.9836 - val_acc: 0.7310\n",
      "Epoch 595/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8845 - acc: 0.7909 - val_loss: 1.0065 - val_acc: 0.7330\n",
      "Epoch 596/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8850 - acc: 0.7911 - val_loss: 0.9973 - val_acc: 0.7280\n",
      "Epoch 597/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8838 - acc: 0.7923 - val_loss: 0.9858 - val_acc: 0.7280\n",
      "Epoch 598/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8838 - acc: 0.7897 - val_loss: 0.9909 - val_acc: 0.7360\n",
      "Epoch 599/1000\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.8818 - acc: 0.7909 - val_loss: 0.9910 - val_acc: 0.7340\n",
      "Epoch 600/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8841 - acc: 0.7916 - val_loss: 0.9949 - val_acc: 0.7330\n",
      "Epoch 601/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8826 - acc: 0.7921 - val_loss: 1.0211 - val_acc: 0.7180\n",
      "Epoch 602/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8839 - acc: 0.7901 - val_loss: 0.9899 - val_acc: 0.7270\n",
      "Epoch 603/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8825 - acc: 0.7933 - val_loss: 0.9909 - val_acc: 0.7310\n",
      "Epoch 604/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8831 - acc: 0.7929 - val_loss: 0.9998 - val_acc: 0.7260\n",
      "Epoch 605/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8828 - acc: 0.7913 - val_loss: 0.9792 - val_acc: 0.7350\n",
      "Epoch 606/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8827 - acc: 0.7937 - val_loss: 1.0181 - val_acc: 0.7220\n",
      "Epoch 607/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8820 - acc: 0.7904 - val_loss: 1.0018 - val_acc: 0.7220\n",
      "Epoch 608/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8815 - acc: 0.7893 - val_loss: 0.9778 - val_acc: 0.7320\n",
      "Epoch 609/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8812 - acc: 0.7949 - val_loss: 0.9954 - val_acc: 0.7380\n",
      "Epoch 610/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8813 - acc: 0.7916 - val_loss: 1.0006 - val_acc: 0.7250\n",
      "Epoch 611/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8807 - acc: 0.7903 - val_loss: 0.9866 - val_acc: 0.7300\n",
      "Epoch 612/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8810 - acc: 0.7916 - val_loss: 0.9820 - val_acc: 0.7300\n",
      "Epoch 613/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8801 - acc: 0.7907 - val_loss: 0.9885 - val_acc: 0.7300\n",
      "Epoch 614/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8806 - acc: 0.7933 - val_loss: 1.0019 - val_acc: 0.7260\n",
      "Epoch 615/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8792 - acc: 0.7893 - val_loss: 0.9916 - val_acc: 0.7280\n",
      "Epoch 616/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8794 - acc: 0.7934 - val_loss: 0.9890 - val_acc: 0.7380\n",
      "Epoch 617/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8795 - acc: 0.7920 - val_loss: 0.9853 - val_acc: 0.7340\n",
      "Epoch 618/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8792 - acc: 0.7910 - val_loss: 0.9773 - val_acc: 0.7360\n",
      "Epoch 619/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8788 - acc: 0.7930 - val_loss: 1.0052 - val_acc: 0.7270\n",
      "Epoch 620/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.8790 - acc: 0.7913 - val_loss: 0.9834 - val_acc: 0.7300\n",
      "Epoch 621/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8784 - acc: 0.7923 - val_loss: 0.9862 - val_acc: 0.7370\n",
      "Epoch 622/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8774 - acc: 0.7929 - val_loss: 0.9816 - val_acc: 0.7390\n",
      "Epoch 623/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8777 - acc: 0.7920 - val_loss: 0.9824 - val_acc: 0.7340\n",
      "Epoch 624/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8779 - acc: 0.7954 - val_loss: 0.9786 - val_acc: 0.7310\n",
      "Epoch 625/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8771 - acc: 0.7924 - val_loss: 0.9740 - val_acc: 0.7310\n",
      "Epoch 626/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8756 - acc: 0.7924 - val_loss: 0.9793 - val_acc: 0.7370\n",
      "Epoch 627/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8784 - acc: 0.7933 - val_loss: 0.9817 - val_acc: 0.7380\n",
      "Epoch 628/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8781 - acc: 0.7901 - val_loss: 0.9789 - val_acc: 0.7330\n",
      "Epoch 629/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8753 - acc: 0.7930 - val_loss: 0.9747 - val_acc: 0.7360\n",
      "Epoch 630/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.8752 - acc: 0.7930 - val_loss: 0.9810 - val_acc: 0.7390\n",
      "Epoch 631/1000\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.8766 - acc: 0.7946 - val_loss: 0.9915 - val_acc: 0.7260\n",
      "Epoch 632/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8755 - acc: 0.7923 - val_loss: 0.9814 - val_acc: 0.7320\n",
      "Epoch 633/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8771 - acc: 0.7911 - val_loss: 0.9834 - val_acc: 0.7390\n",
      "Epoch 634/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8762 - acc: 0.7916 - val_loss: 0.9743 - val_acc: 0.7380\n",
      "Epoch 635/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8748 - acc: 0.7930 - val_loss: 0.9852 - val_acc: 0.7410\n",
      "Epoch 636/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8754 - acc: 0.7930 - val_loss: 0.9843 - val_acc: 0.7290\n",
      "Epoch 637/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8768 - acc: 0.7923 - val_loss: 0.9766 - val_acc: 0.7400\n",
      "Epoch 638/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8742 - acc: 0.7956 - val_loss: 0.9796 - val_acc: 0.7330\n",
      "Epoch 639/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8738 - acc: 0.7916 - val_loss: 1.0146 - val_acc: 0.7310\n",
      "Epoch 640/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8762 - acc: 0.7927 - val_loss: 0.9998 - val_acc: 0.7270\n",
      "Epoch 641/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8751 - acc: 0.7944 - val_loss: 0.9798 - val_acc: 0.7320\n",
      "Epoch 642/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8735 - acc: 0.7933 - val_loss: 0.9826 - val_acc: 0.7360\n",
      "Epoch 643/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 0.8747 - acc: 0.7914 - val_loss: 0.9888 - val_acc: 0.7350\n",
      "Epoch 644/1000\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.8739 - acc: 0.7957 - val_loss: 1.0432 - val_acc: 0.7190\n",
      "Epoch 645/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8735 - acc: 0.7931 - val_loss: 0.9880 - val_acc: 0.7340\n",
      "Epoch 646/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8738 - acc: 0.7940 - val_loss: 0.9768 - val_acc: 0.7390\n",
      "Epoch 647/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8731 - acc: 0.7914 - val_loss: 0.9784 - val_acc: 0.7320\n",
      "Epoch 648/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8718 - acc: 0.7916 - val_loss: 0.9787 - val_acc: 0.7390\n",
      "Epoch 649/1000\n",
      "7000/7000 [==============================] - 0s 58us/step - loss: 0.8733 - acc: 0.7936 - val_loss: 0.9790 - val_acc: 0.7330\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8733 - acc: 0.7929 - val_loss: 0.9876 - val_acc: 0.7300\n",
      "Epoch 651/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8712 - acc: 0.7947 - val_loss: 0.9774 - val_acc: 0.7350\n",
      "Epoch 652/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8722 - acc: 0.7906 - val_loss: 0.9748 - val_acc: 0.7350\n",
      "Epoch 653/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8712 - acc: 0.7949 - val_loss: 0.9791 - val_acc: 0.7340\n",
      "Epoch 654/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 0.8704 - acc: 0.7954 - val_loss: 0.9772 - val_acc: 0.7360\n",
      "Epoch 655/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8711 - acc: 0.7953 - val_loss: 0.9740 - val_acc: 0.7410\n",
      "Epoch 656/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8706 - acc: 0.7936 - val_loss: 0.9750 - val_acc: 0.7320\n",
      "Epoch 657/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8693 - acc: 0.7939 - val_loss: 0.9961 - val_acc: 0.7390\n",
      "Epoch 658/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8705 - acc: 0.7941 - val_loss: 0.9855 - val_acc: 0.7290\n",
      "Epoch 659/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8699 - acc: 0.7941 - val_loss: 0.9833 - val_acc: 0.7270\n",
      "Epoch 660/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8717 - acc: 0.7941 - val_loss: 0.9892 - val_acc: 0.7370\n",
      "Epoch 661/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8721 - acc: 0.7940 - val_loss: 0.9765 - val_acc: 0.7350\n",
      "Epoch 662/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8703 - acc: 0.7954 - val_loss: 1.0060 - val_acc: 0.7220\n",
      "Epoch 663/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8712 - acc: 0.7940 - val_loss: 0.9788 - val_acc: 0.7330\n",
      "Epoch 664/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8684 - acc: 0.7934 - val_loss: 1.0017 - val_acc: 0.7280\n",
      "Epoch 665/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8700 - acc: 0.7941 - val_loss: 0.9790 - val_acc: 0.7290\n",
      "Epoch 666/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8685 - acc: 0.7944 - val_loss: 0.9768 - val_acc: 0.7320\n",
      "Epoch 667/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8691 - acc: 0.7903 - val_loss: 0.9939 - val_acc: 0.7320\n",
      "Epoch 668/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8695 - acc: 0.7943 - val_loss: 0.9769 - val_acc: 0.7340\n",
      "Epoch 669/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8679 - acc: 0.7953 - val_loss: 0.9777 - val_acc: 0.7360\n",
      "Epoch 670/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8684 - acc: 0.7981 - val_loss: 0.9795 - val_acc: 0.7330\n",
      "Epoch 671/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8673 - acc: 0.7969 - val_loss: 0.9892 - val_acc: 0.7390\n",
      "Epoch 672/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8682 - acc: 0.7907 - val_loss: 0.9707 - val_acc: 0.7360\n",
      "Epoch 673/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8671 - acc: 0.7936 - val_loss: 0.9864 - val_acc: 0.7390\n",
      "Epoch 674/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8665 - acc: 0.7953 - val_loss: 0.9938 - val_acc: 0.7240\n",
      "Epoch 675/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8678 - acc: 0.7923 - val_loss: 0.9933 - val_acc: 0.7320\n",
      "Epoch 676/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8690 - acc: 0.7950 - val_loss: 0.9808 - val_acc: 0.7330\n",
      "Epoch 677/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8659 - acc: 0.7950 - val_loss: 0.9727 - val_acc: 0.7340\n",
      "Epoch 678/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8660 - acc: 0.7961 - val_loss: 0.9751 - val_acc: 0.7310\n",
      "Epoch 679/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8688 - acc: 0.7957 - val_loss: 0.9741 - val_acc: 0.7330\n",
      "Epoch 680/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8661 - acc: 0.7963 - val_loss: 0.9823 - val_acc: 0.7310\n",
      "Epoch 681/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8642 - acc: 0.7951 - val_loss: 0.9926 - val_acc: 0.7400\n",
      "Epoch 682/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8668 - acc: 0.7933 - val_loss: 0.9770 - val_acc: 0.7360\n",
      "Epoch 683/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8662 - acc: 0.7937 - val_loss: 0.9784 - val_acc: 0.7390\n",
      "Epoch 684/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8659 - acc: 0.7960 - val_loss: 0.9936 - val_acc: 0.7300\n",
      "Epoch 685/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8691 - acc: 0.7920 - val_loss: 0.9897 - val_acc: 0.7370\n",
      "Epoch 686/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8641 - acc: 0.7949 - val_loss: 0.9998 - val_acc: 0.7260\n",
      "Epoch 687/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8674 - acc: 0.7944 - val_loss: 0.9807 - val_acc: 0.7330\n",
      "Epoch 688/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8654 - acc: 0.7966 - val_loss: 0.9761 - val_acc: 0.7400\n",
      "Epoch 689/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8641 - acc: 0.7957 - val_loss: 0.9800 - val_acc: 0.7350\n",
      "Epoch 690/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8651 - acc: 0.7976 - val_loss: 1.0001 - val_acc: 0.7350\n",
      "Epoch 691/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.8651 - acc: 0.7980 - val_loss: 0.9778 - val_acc: 0.7330\n",
      "Epoch 692/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8647 - acc: 0.7943 - val_loss: 0.9779 - val_acc: 0.7310\n",
      "Epoch 693/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8646 - acc: 0.7956 - val_loss: 0.9819 - val_acc: 0.7270\n",
      "Epoch 694/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8625 - acc: 0.7950 - val_loss: 0.9789 - val_acc: 0.7320\n",
      "Epoch 695/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8630 - acc: 0.7964 - val_loss: 0.9814 - val_acc: 0.7270\n",
      "Epoch 696/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8634 - acc: 0.7967 - val_loss: 0.9866 - val_acc: 0.7330\n",
      "Epoch 697/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8621 - acc: 0.7959 - val_loss: 0.9929 - val_acc: 0.7320\n",
      "Epoch 698/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8636 - acc: 0.7966 - val_loss: 0.9704 - val_acc: 0.7340\n",
      "Epoch 699/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8622 - acc: 0.7960 - val_loss: 0.9994 - val_acc: 0.7300\n",
      "Epoch 700/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8633 - acc: 0.7963 - val_loss: 0.9791 - val_acc: 0.7310\n",
      "Epoch 701/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8614 - acc: 0.7959 - val_loss: 0.9773 - val_acc: 0.7360\n",
      "Epoch 702/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8637 - acc: 0.7960 - val_loss: 0.9784 - val_acc: 0.7300\n",
      "Epoch 703/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8623 - acc: 0.7971 - val_loss: 0.9853 - val_acc: 0.7360\n",
      "Epoch 704/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8619 - acc: 0.7970 - val_loss: 0.9890 - val_acc: 0.7380\n",
      "Epoch 705/1000\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.8623 - acc: 0.7974 - val_loss: 0.9849 - val_acc: 0.7370\n",
      "Epoch 706/1000\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.8621 - acc: 0.7970 - val_loss: 0.9721 - val_acc: 0.7390\n",
      "Epoch 707/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8640 - acc: 0.7949 - val_loss: 0.9873 - val_acc: 0.7300\n",
      "Epoch 708/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8610 - acc: 0.7971 - val_loss: 0.9782 - val_acc: 0.7360\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8625 - acc: 0.7959 - val_loss: 0.9787 - val_acc: 0.7330\n",
      "Epoch 710/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8589 - acc: 0.7964 - val_loss: 0.9943 - val_acc: 0.7370\n",
      "Epoch 711/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8600 - acc: 0.7966 - val_loss: 0.9858 - val_acc: 0.7370\n",
      "Epoch 712/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8617 - acc: 0.7977 - val_loss: 0.9755 - val_acc: 0.7320\n",
      "Epoch 713/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8583 - acc: 0.7990 - val_loss: 1.0034 - val_acc: 0.7270\n",
      "Epoch 714/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8613 - acc: 0.7967 - val_loss: 0.9673 - val_acc: 0.7370\n",
      "Epoch 715/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8611 - acc: 0.7977 - val_loss: 0.9776 - val_acc: 0.7350\n",
      "Epoch 716/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8607 - acc: 0.7993 - val_loss: 0.9697 - val_acc: 0.7360\n",
      "Epoch 717/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8593 - acc: 0.7953 - val_loss: 1.0192 - val_acc: 0.7200\n",
      "Epoch 718/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8609 - acc: 0.7974 - val_loss: 1.0040 - val_acc: 0.7300\n",
      "Epoch 719/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 0.8598 - acc: 0.7984 - val_loss: 1.0040 - val_acc: 0.7180\n",
      "Epoch 720/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8607 - acc: 0.7989 - val_loss: 0.9795 - val_acc: 0.7350\n",
      "Epoch 721/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8593 - acc: 0.7979 - val_loss: 0.9836 - val_acc: 0.7370\n",
      "Epoch 722/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8604 - acc: 0.7996 - val_loss: 0.9894 - val_acc: 0.7290\n",
      "Epoch 723/1000\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 0.8585 - acc: 0.7989 - val_loss: 0.9683 - val_acc: 0.7370\n",
      "Epoch 724/1000\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.8582 - acc: 0.7974 - val_loss: 0.9797 - val_acc: 0.7380\n",
      "Epoch 725/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8571 - acc: 0.7994 - val_loss: 0.9804 - val_acc: 0.7290\n",
      "Epoch 726/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8588 - acc: 0.7991 - val_loss: 0.9782 - val_acc: 0.7310\n",
      "Epoch 727/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8602 - acc: 0.7963 - val_loss: 0.9793 - val_acc: 0.7340\n",
      "Epoch 728/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8575 - acc: 0.8011 - val_loss: 0.9775 - val_acc: 0.7340\n",
      "Epoch 729/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 0.8576 - acc: 0.7996 - val_loss: 0.9772 - val_acc: 0.7390\n",
      "Epoch 730/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8584 - acc: 0.7960 - val_loss: 0.9952 - val_acc: 0.7330\n",
      "Epoch 731/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8573 - acc: 0.7981 - val_loss: 0.9941 - val_acc: 0.7330\n",
      "Epoch 732/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8575 - acc: 0.7979 - val_loss: 0.9790 - val_acc: 0.7440\n",
      "Epoch 733/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8584 - acc: 0.8016 - val_loss: 0.9755 - val_acc: 0.7370\n",
      "Epoch 734/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8597 - acc: 0.8001 - val_loss: 0.9732 - val_acc: 0.7300\n",
      "Epoch 735/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8581 - acc: 0.7986 - val_loss: 0.9780 - val_acc: 0.7340\n",
      "Epoch 736/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8592 - acc: 0.7970 - val_loss: 0.9717 - val_acc: 0.7380\n",
      "Epoch 737/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8561 - acc: 0.7981 - val_loss: 0.9703 - val_acc: 0.7400\n",
      "Epoch 738/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8552 - acc: 0.7970 - val_loss: 0.9948 - val_acc: 0.7280\n",
      "Epoch 739/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8574 - acc: 0.7994 - val_loss: 0.9985 - val_acc: 0.7260\n",
      "Epoch 740/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8576 - acc: 0.7991 - val_loss: 0.9792 - val_acc: 0.7390\n",
      "Epoch 741/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8561 - acc: 0.7997 - val_loss: 0.9862 - val_acc: 0.7370\n",
      "Epoch 742/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8547 - acc: 0.7981 - val_loss: 0.9809 - val_acc: 0.7350\n",
      "Epoch 743/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8559 - acc: 0.7976 - val_loss: 0.9710 - val_acc: 0.7330\n",
      "Epoch 744/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8535 - acc: 0.8000 - val_loss: 0.9702 - val_acc: 0.7380\n",
      "Epoch 745/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8556 - acc: 0.7989 - val_loss: 0.9774 - val_acc: 0.7290\n",
      "Epoch 746/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8562 - acc: 0.7999 - val_loss: 0.9918 - val_acc: 0.7270\n",
      "Epoch 747/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8541 - acc: 0.7959 - val_loss: 0.9669 - val_acc: 0.7340\n",
      "Epoch 748/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8541 - acc: 0.8027 - val_loss: 0.9695 - val_acc: 0.7350\n",
      "Epoch 749/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8520 - acc: 0.8007 - val_loss: 0.9855 - val_acc: 0.7370\n",
      "Epoch 750/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8545 - acc: 0.7981 - val_loss: 0.9969 - val_acc: 0.7300\n",
      "Epoch 751/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8542 - acc: 0.7980 - val_loss: 0.9991 - val_acc: 0.7270\n",
      "Epoch 752/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8531 - acc: 0.7983 - val_loss: 0.9680 - val_acc: 0.7380\n",
      "Epoch 753/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8541 - acc: 0.7981 - val_loss: 0.9918 - val_acc: 0.7300\n",
      "Epoch 754/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8551 - acc: 0.7990 - val_loss: 0.9753 - val_acc: 0.7350\n",
      "Epoch 755/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8520 - acc: 0.7993 - val_loss: 1.0102 - val_acc: 0.7320\n",
      "Epoch 756/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8529 - acc: 0.8009 - val_loss: 0.9748 - val_acc: 0.7370\n",
      "Epoch 757/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8516 - acc: 0.8003 - val_loss: 0.9832 - val_acc: 0.7290\n",
      "Epoch 758/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8530 - acc: 0.7989 - val_loss: 0.9842 - val_acc: 0.7350\n",
      "Epoch 759/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8528 - acc: 0.8004 - val_loss: 0.9733 - val_acc: 0.7360\n",
      "Epoch 760/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8540 - acc: 0.7994 - val_loss: 0.9672 - val_acc: 0.7370\n",
      "Epoch 761/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8519 - acc: 0.7983 - val_loss: 0.9779 - val_acc: 0.7330\n",
      "Epoch 762/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8514 - acc: 0.8004 - val_loss: 1.0025 - val_acc: 0.7250\n",
      "Epoch 763/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8536 - acc: 0.7993 - val_loss: 0.9865 - val_acc: 0.7370\n",
      "Epoch 764/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8508 - acc: 0.8010 - val_loss: 1.0075 - val_acc: 0.7330\n",
      "Epoch 765/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8511 - acc: 0.8003 - val_loss: 0.9948 - val_acc: 0.7310\n",
      "Epoch 766/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8512 - acc: 0.7991 - val_loss: 0.9784 - val_acc: 0.7380\n",
      "Epoch 767/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8516 - acc: 0.7987 - val_loss: 0.9864 - val_acc: 0.7290\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8507 - acc: 0.7980 - val_loss: 0.9891 - val_acc: 0.7350\n",
      "Epoch 769/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8502 - acc: 0.8029 - val_loss: 0.9738 - val_acc: 0.7360\n",
      "Epoch 770/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8529 - acc: 0.7980 - val_loss: 0.9900 - val_acc: 0.7330\n",
      "Epoch 771/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8501 - acc: 0.8004 - val_loss: 0.9831 - val_acc: 0.7390\n",
      "Epoch 772/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8498 - acc: 0.8011 - val_loss: 0.9937 - val_acc: 0.7240\n",
      "Epoch 773/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8508 - acc: 0.7997 - val_loss: 0.9712 - val_acc: 0.7380\n",
      "Epoch 774/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8493 - acc: 0.8019 - val_loss: 0.9833 - val_acc: 0.7420\n",
      "Epoch 775/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8486 - acc: 0.7971 - val_loss: 0.9726 - val_acc: 0.7390\n",
      "Epoch 776/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8474 - acc: 0.8023 - val_loss: 0.9664 - val_acc: 0.7370\n",
      "Epoch 777/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8493 - acc: 0.7996 - val_loss: 0.9676 - val_acc: 0.7410\n",
      "Epoch 778/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8484 - acc: 0.7986 - val_loss: 0.9747 - val_acc: 0.7340\n",
      "Epoch 779/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8491 - acc: 0.7996 - val_loss: 0.9717 - val_acc: 0.7350\n",
      "Epoch 780/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8487 - acc: 0.8024 - val_loss: 0.9840 - val_acc: 0.7280\n",
      "Epoch 781/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8478 - acc: 0.8029 - val_loss: 0.9920 - val_acc: 0.7260\n",
      "Epoch 782/1000\n",
      "7000/7000 [==============================] - 0s 57us/step - loss: 0.8485 - acc: 0.8029 - val_loss: 0.9712 - val_acc: 0.7390\n",
      "Epoch 783/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8496 - acc: 0.8020 - val_loss: 1.0002 - val_acc: 0.7250\n",
      "Epoch 784/1000\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 0.8490 - acc: 0.8030 - val_loss: 0.9778 - val_acc: 0.7350\n",
      "Epoch 785/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8529 - acc: 0.7996 - val_loss: 0.9751 - val_acc: 0.7320\n",
      "Epoch 786/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 0.8481 - acc: 0.8010 - val_loss: 0.9952 - val_acc: 0.7230\n",
      "Epoch 787/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8503 - acc: 0.7990 - val_loss: 0.9742 - val_acc: 0.7320\n",
      "Epoch 788/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8473 - acc: 0.8021 - val_loss: 0.9949 - val_acc: 0.7380\n",
      "Epoch 789/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8478 - acc: 0.8003 - val_loss: 0.9744 - val_acc: 0.7390\n",
      "Epoch 790/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8471 - acc: 0.8033 - val_loss: 0.9933 - val_acc: 0.7370\n",
      "Epoch 791/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8478 - acc: 0.8039 - val_loss: 0.9759 - val_acc: 0.7310\n",
      "Epoch 792/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8473 - acc: 0.8017 - val_loss: 0.9948 - val_acc: 0.7340\n",
      "Epoch 793/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8478 - acc: 0.7993 - val_loss: 0.9838 - val_acc: 0.7370\n",
      "Epoch 794/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8449 - acc: 0.8037 - val_loss: 0.9884 - val_acc: 0.7340\n",
      "Epoch 795/1000\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 0.8463 - acc: 0.8029 - val_loss: 0.9953 - val_acc: 0.7290\n",
      "Epoch 796/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8476 - acc: 0.8007 - val_loss: 0.9859 - val_acc: 0.7340\n",
      "Epoch 797/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8498 - acc: 0.8019 - val_loss: 0.9737 - val_acc: 0.7370\n",
      "Epoch 798/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8492 - acc: 0.7999 - val_loss: 0.9856 - val_acc: 0.7410\n",
      "Epoch 799/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8464 - acc: 0.8020 - val_loss: 0.9708 - val_acc: 0.7450\n",
      "Epoch 800/1000\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 0.8485 - acc: 0.7980 - val_loss: 1.0038 - val_acc: 0.7370\n",
      "Epoch 801/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8464 - acc: 0.8037 - val_loss: 0.9866 - val_acc: 0.7300\n",
      "Epoch 802/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8486 - acc: 0.8013 - val_loss: 0.9763 - val_acc: 0.7310\n",
      "Epoch 803/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8444 - acc: 0.8026 - val_loss: 0.9715 - val_acc: 0.7400\n",
      "Epoch 804/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8468 - acc: 0.8033 - val_loss: 0.9708 - val_acc: 0.7370\n",
      "Epoch 805/1000\n",
      "7000/7000 [==============================] - 0s 55us/step - loss: 0.8445 - acc: 0.8041 - val_loss: 0.9743 - val_acc: 0.7350\n",
      "Epoch 806/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8445 - acc: 0.8027 - val_loss: 0.9645 - val_acc: 0.7370\n",
      "Epoch 807/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8488 - acc: 0.8000 - val_loss: 0.9676 - val_acc: 0.7330\n",
      "Epoch 808/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8435 - acc: 0.8020 - val_loss: 0.9835 - val_acc: 0.7340\n",
      "Epoch 809/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8438 - acc: 0.8049 - val_loss: 0.9745 - val_acc: 0.7310\n",
      "Epoch 810/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8450 - acc: 0.8039 - val_loss: 0.9678 - val_acc: 0.7380\n",
      "Epoch 811/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8420 - acc: 0.8027 - val_loss: 0.9747 - val_acc: 0.7440\n",
      "Epoch 812/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8444 - acc: 0.8040 - val_loss: 0.9690 - val_acc: 0.7420\n",
      "Epoch 813/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8465 - acc: 0.7977 - val_loss: 1.0006 - val_acc: 0.7220\n",
      "Epoch 814/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8441 - acc: 0.8023 - val_loss: 0.9834 - val_acc: 0.7340\n",
      "Epoch 815/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8422 - acc: 0.8016 - val_loss: 0.9722 - val_acc: 0.7390\n",
      "Epoch 816/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8440 - acc: 0.8040 - val_loss: 0.9643 - val_acc: 0.7340\n",
      "Epoch 817/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8442 - acc: 0.8024 - val_loss: 1.0080 - val_acc: 0.7320\n",
      "Epoch 818/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8448 - acc: 0.8047 - val_loss: 0.9736 - val_acc: 0.7360\n",
      "Epoch 819/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8423 - acc: 0.8021 - val_loss: 0.9832 - val_acc: 0.7360\n",
      "Epoch 820/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8440 - acc: 0.8026 - val_loss: 0.9893 - val_acc: 0.7300\n",
      "Epoch 821/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 0.8461 - acc: 0.8014 - val_loss: 0.9783 - val_acc: 0.7340\n",
      "Epoch 822/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8437 - acc: 0.8049 - val_loss: 0.9859 - val_acc: 0.7440\n",
      "Epoch 823/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8439 - acc: 0.8021 - val_loss: 0.9827 - val_acc: 0.7390\n",
      "Epoch 824/1000\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.8452 - acc: 0.8010 - val_loss: 0.9680 - val_acc: 0.7420\n",
      "Epoch 825/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 0.8427 - acc: 0.8041 - val_loss: 0.9873 - val_acc: 0.7350\n",
      "Epoch 826/1000\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 0.8430 - acc: 0.8047 - val_loss: 0.9793 - val_acc: 0.7420\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8444 - acc: 0.8033 - val_loss: 0.9634 - val_acc: 0.7430\n",
      "Epoch 828/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8440 - acc: 0.8041 - val_loss: 0.9669 - val_acc: 0.7370\n",
      "Epoch 829/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8423 - acc: 0.8030 - val_loss: 0.9903 - val_acc: 0.7400\n",
      "Epoch 830/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8416 - acc: 0.8047 - val_loss: 0.9684 - val_acc: 0.7330\n",
      "Epoch 831/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8436 - acc: 0.8010 - val_loss: 0.9893 - val_acc: 0.7320\n",
      "Epoch 832/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8401 - acc: 0.8054 - val_loss: 0.9699 - val_acc: 0.7390\n",
      "Epoch 833/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8410 - acc: 0.8029 - val_loss: 0.9741 - val_acc: 0.7380\n",
      "Epoch 834/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8410 - acc: 0.8066 - val_loss: 0.9837 - val_acc: 0.7330\n",
      "Epoch 835/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8413 - acc: 0.8014 - val_loss: 0.9705 - val_acc: 0.7320\n",
      "Epoch 836/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8403 - acc: 0.8040 - val_loss: 0.9769 - val_acc: 0.7380\n",
      "Epoch 837/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8395 - acc: 0.8039 - val_loss: 0.9727 - val_acc: 0.7320\n",
      "Epoch 838/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8402 - acc: 0.8036 - val_loss: 1.0167 - val_acc: 0.7170\n",
      "Epoch 839/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8454 - acc: 0.7979 - val_loss: 1.0078 - val_acc: 0.7240\n",
      "Epoch 840/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8422 - acc: 0.8026 - val_loss: 0.9946 - val_acc: 0.7310\n",
      "Epoch 841/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8394 - acc: 0.8036 - val_loss: 0.9628 - val_acc: 0.7400\n",
      "Epoch 842/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8413 - acc: 0.8019 - val_loss: 0.9698 - val_acc: 0.7310\n",
      "Epoch 843/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8407 - acc: 0.8053 - val_loss: 1.0150 - val_acc: 0.7290\n",
      "Epoch 844/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8389 - acc: 0.8007 - val_loss: 0.9831 - val_acc: 0.7380\n",
      "Epoch 845/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8417 - acc: 0.8021 - val_loss: 0.9631 - val_acc: 0.7420\n",
      "Epoch 846/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8391 - acc: 0.8031 - val_loss: 0.9641 - val_acc: 0.7380\n",
      "Epoch 847/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8378 - acc: 0.8041 - val_loss: 0.9623 - val_acc: 0.7400\n",
      "Epoch 848/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8382 - acc: 0.8054 - val_loss: 0.9873 - val_acc: 0.7460\n",
      "Epoch 849/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8384 - acc: 0.8043 - val_loss: 0.9640 - val_acc: 0.7370\n",
      "Epoch 850/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8365 - acc: 0.8034 - val_loss: 0.9835 - val_acc: 0.7340\n",
      "Epoch 851/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8393 - acc: 0.8024 - val_loss: 1.0002 - val_acc: 0.7260\n",
      "Epoch 852/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8398 - acc: 0.8021 - val_loss: 0.9785 - val_acc: 0.7320\n",
      "Epoch 853/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8368 - acc: 0.8016 - val_loss: 1.0059 - val_acc: 0.7220\n",
      "Epoch 854/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8403 - acc: 0.8024 - val_loss: 0.9909 - val_acc: 0.7350\n",
      "Epoch 855/1000\n",
      "7000/7000 [==============================] - 0s 54us/step - loss: 0.8369 - acc: 0.8083 - val_loss: 0.9943 - val_acc: 0.7390\n",
      "Epoch 856/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 0.8380 - acc: 0.8037 - val_loss: 1.0318 - val_acc: 0.7170\n",
      "Epoch 857/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8400 - acc: 0.8051 - val_loss: 0.9717 - val_acc: 0.7360\n",
      "Epoch 858/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8369 - acc: 0.8070 - val_loss: 0.9661 - val_acc: 0.7450\n",
      "Epoch 859/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8380 - acc: 0.8051 - val_loss: 0.9692 - val_acc: 0.7350\n",
      "Epoch 860/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8354 - acc: 0.8060 - val_loss: 0.9646 - val_acc: 0.7390\n",
      "Epoch 861/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8353 - acc: 0.8057 - val_loss: 0.9873 - val_acc: 0.7380\n",
      "Epoch 862/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8352 - acc: 0.8057 - val_loss: 0.9720 - val_acc: 0.7420\n",
      "Epoch 863/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8338 - acc: 0.8100 - val_loss: 0.9622 - val_acc: 0.7410\n",
      "Epoch 864/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8358 - acc: 0.8076 - val_loss: 0.9773 - val_acc: 0.7400\n",
      "Epoch 865/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8381 - acc: 0.8044 - val_loss: 0.9808 - val_acc: 0.7350\n",
      "Epoch 866/1000\n",
      "7000/7000 [==============================] - 0s 57us/step - loss: 0.8394 - acc: 0.8050 - val_loss: 0.9690 - val_acc: 0.7370\n",
      "Epoch 867/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8385 - acc: 0.8054 - val_loss: 0.9690 - val_acc: 0.7390\n",
      "Epoch 868/1000\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 0.8346 - acc: 0.8087 - val_loss: 0.9761 - val_acc: 0.7400\n",
      "Epoch 869/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8353 - acc: 0.8050 - val_loss: 0.9633 - val_acc: 0.7360\n",
      "Epoch 870/1000\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 0.8347 - acc: 0.8087 - val_loss: 0.9762 - val_acc: 0.7430\n",
      "Epoch 871/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8386 - acc: 0.8024 - val_loss: 0.9740 - val_acc: 0.7430\n",
      "Epoch 872/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8370 - acc: 0.8046 - val_loss: 0.9650 - val_acc: 0.7360\n",
      "Epoch 873/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8358 - acc: 0.8054 - val_loss: 0.9934 - val_acc: 0.7260\n",
      "Epoch 874/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8366 - acc: 0.8051 - val_loss: 0.9898 - val_acc: 0.7270\n",
      "Epoch 875/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 0.8335 - acc: 0.8043 - val_loss: 0.9982 - val_acc: 0.7240\n",
      "Epoch 876/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8366 - acc: 0.8046 - val_loss: 0.9818 - val_acc: 0.7370\n",
      "Epoch 877/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8355 - acc: 0.8030 - val_loss: 0.9853 - val_acc: 0.7400\n",
      "Epoch 878/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8337 - acc: 0.8086 - val_loss: 0.9774 - val_acc: 0.7470\n",
      "Epoch 879/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8345 - acc: 0.8024 - val_loss: 1.0296 - val_acc: 0.7200\n",
      "Epoch 880/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8347 - acc: 0.8069 - val_loss: 0.9653 - val_acc: 0.7390\n",
      "Epoch 881/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8344 - acc: 0.8081 - val_loss: 1.0408 - val_acc: 0.7150\n",
      "Epoch 882/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8373 - acc: 0.8036 - val_loss: 0.9764 - val_acc: 0.7360\n",
      "Epoch 883/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8340 - acc: 0.8077 - val_loss: 0.9742 - val_acc: 0.7400\n",
      "Epoch 884/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8352 - acc: 0.8069 - val_loss: 0.9951 - val_acc: 0.7270\n",
      "Epoch 885/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8348 - acc: 0.8039 - val_loss: 0.9748 - val_acc: 0.7430\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8345 - acc: 0.8069 - val_loss: 0.9919 - val_acc: 0.7350\n",
      "Epoch 887/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8338 - acc: 0.8041 - val_loss: 0.9962 - val_acc: 0.7350\n",
      "Epoch 888/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8346 - acc: 0.8056 - val_loss: 0.9822 - val_acc: 0.7350\n",
      "Epoch 889/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8325 - acc: 0.8070 - val_loss: 0.9630 - val_acc: 0.7350\n",
      "Epoch 890/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8327 - acc: 0.8057 - val_loss: 0.9674 - val_acc: 0.7360\n",
      "Epoch 891/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8351 - acc: 0.8053 - val_loss: 0.9639 - val_acc: 0.7420\n",
      "Epoch 892/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8363 - acc: 0.8039 - val_loss: 1.0119 - val_acc: 0.7260\n",
      "Epoch 893/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8348 - acc: 0.8056 - val_loss: 0.9805 - val_acc: 0.7380\n",
      "Epoch 894/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8337 - acc: 0.8053 - val_loss: 0.9752 - val_acc: 0.7350\n",
      "Epoch 895/1000\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.8321 - acc: 0.8056 - val_loss: 0.9883 - val_acc: 0.7360\n",
      "Epoch 896/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8321 - acc: 0.8064 - val_loss: 0.9604 - val_acc: 0.7380\n",
      "Epoch 897/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8342 - acc: 0.8059 - val_loss: 0.9688 - val_acc: 0.7410\n",
      "Epoch 898/1000\n",
      "7000/7000 [==============================] - 0s 56us/step - loss: 0.8313 - acc: 0.8103 - val_loss: 0.9854 - val_acc: 0.7330\n",
      "Epoch 899/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8328 - acc: 0.8076 - val_loss: 0.9707 - val_acc: 0.7390\n",
      "Epoch 900/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8316 - acc: 0.8067 - val_loss: 0.9637 - val_acc: 0.7380\n",
      "Epoch 901/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8316 - acc: 0.8066 - val_loss: 0.9785 - val_acc: 0.7340\n",
      "Epoch 902/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8363 - acc: 0.8020 - val_loss: 0.9724 - val_acc: 0.7360\n",
      "Epoch 903/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8300 - acc: 0.8054 - val_loss: 0.9660 - val_acc: 0.7360\n",
      "Epoch 904/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8289 - acc: 0.8070 - val_loss: 0.9805 - val_acc: 0.7280\n",
      "Epoch 905/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8344 - acc: 0.8053 - val_loss: 0.9764 - val_acc: 0.7390\n",
      "Epoch 906/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8325 - acc: 0.8039 - val_loss: 0.9628 - val_acc: 0.7360\n",
      "Epoch 907/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8282 - acc: 0.8090 - val_loss: 0.9697 - val_acc: 0.7380\n",
      "Epoch 908/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8311 - acc: 0.8046 - val_loss: 0.9793 - val_acc: 0.7270\n",
      "Epoch 909/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8314 - acc: 0.8053 - val_loss: 0.9748 - val_acc: 0.7290\n",
      "Epoch 910/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8349 - acc: 0.8053 - val_loss: 0.9754 - val_acc: 0.7340\n",
      "Epoch 911/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8331 - acc: 0.8056 - val_loss: 1.0113 - val_acc: 0.7080\n",
      "Epoch 912/1000\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 0.8293 - acc: 0.8069 - val_loss: 0.9862 - val_acc: 0.7340\n",
      "Epoch 913/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8320 - acc: 0.8061 - val_loss: 0.9748 - val_acc: 0.7340\n",
      "Epoch 914/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8284 - acc: 0.8057 - val_loss: 0.9661 - val_acc: 0.7350\n",
      "Epoch 915/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8282 - acc: 0.8089 - val_loss: 0.9763 - val_acc: 0.7390\n",
      "Epoch 916/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8285 - acc: 0.8060 - val_loss: 0.9723 - val_acc: 0.7410\n",
      "Epoch 917/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8293 - acc: 0.8059 - val_loss: 1.0087 - val_acc: 0.7180\n",
      "Epoch 918/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8321 - acc: 0.8027 - val_loss: 0.9698 - val_acc: 0.7430\n",
      "Epoch 919/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8288 - acc: 0.8113 - val_loss: 0.9615 - val_acc: 0.7420\n",
      "Epoch 920/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8269 - acc: 0.8120 - val_loss: 1.0173 - val_acc: 0.7220\n",
      "Epoch 921/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8308 - acc: 0.8071 - val_loss: 0.9946 - val_acc: 0.7240\n",
      "Epoch 922/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8337 - acc: 0.8057 - val_loss: 0.9883 - val_acc: 0.7230\n",
      "Epoch 923/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.8316 - acc: 0.8051 - val_loss: 0.9904 - val_acc: 0.7400\n",
      "Epoch 924/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8305 - acc: 0.8066 - val_loss: 0.9778 - val_acc: 0.7360\n",
      "Epoch 925/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8286 - acc: 0.8077 - val_loss: 0.9651 - val_acc: 0.7390\n",
      "Epoch 926/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8276 - acc: 0.8076 - val_loss: 0.9733 - val_acc: 0.7350\n",
      "Epoch 927/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8277 - acc: 0.8086 - val_loss: 0.9658 - val_acc: 0.7420\n",
      "Epoch 928/1000\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.8293 - acc: 0.8081 - val_loss: 0.9628 - val_acc: 0.7410\n",
      "Epoch 929/1000\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 0.8284 - acc: 0.8059 - val_loss: 0.9636 - val_acc: 0.7400\n",
      "Epoch 930/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8281 - acc: 0.8113 - val_loss: 0.9795 - val_acc: 0.7300\n",
      "Epoch 931/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8245 - acc: 0.8111 - val_loss: 1.0441 - val_acc: 0.7100\n",
      "Epoch 932/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8313 - acc: 0.8083 - val_loss: 0.9660 - val_acc: 0.7420\n",
      "Epoch 933/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8321 - acc: 0.8041 - val_loss: 0.9604 - val_acc: 0.7400\n",
      "Epoch 934/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8254 - acc: 0.8090 - val_loss: 0.9787 - val_acc: 0.7370\n",
      "Epoch 935/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8279 - acc: 0.8071 - val_loss: 0.9809 - val_acc: 0.7470\n",
      "Epoch 936/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8275 - acc: 0.8109 - val_loss: 0.9935 - val_acc: 0.7360\n",
      "Epoch 937/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8262 - acc: 0.8087 - val_loss: 1.0039 - val_acc: 0.7270\n",
      "Epoch 938/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8270 - acc: 0.8061 - val_loss: 0.9776 - val_acc: 0.7270\n",
      "Epoch 939/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8282 - acc: 0.8076 - val_loss: 0.9917 - val_acc: 0.7410\n",
      "Epoch 940/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8260 - acc: 0.8076 - val_loss: 0.9778 - val_acc: 0.7430\n",
      "Epoch 941/1000\n",
      "7000/7000 [==============================] - 0s 51us/step - loss: 0.8237 - acc: 0.8083 - val_loss: 1.0168 - val_acc: 0.7190\n",
      "Epoch 942/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8310 - acc: 0.8071 - val_loss: 0.9697 - val_acc: 0.7380\n",
      "Epoch 943/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8312 - acc: 0.8059 - val_loss: 0.9725 - val_acc: 0.7370\n",
      "Epoch 944/1000\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.8230 - acc: 0.8096 - val_loss: 0.9622 - val_acc: 0.7380\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8285 - acc: 0.8066 - val_loss: 0.9878 - val_acc: 0.7180\n",
      "Epoch 946/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8300 - acc: 0.8076 - val_loss: 0.9721 - val_acc: 0.7270\n",
      "Epoch 947/1000\n",
      "7000/7000 [==============================] - 0s 50us/step - loss: 0.8245 - acc: 0.8111 - val_loss: 0.9807 - val_acc: 0.7370\n",
      "Epoch 948/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8227 - acc: 0.8100 - val_loss: 0.9908 - val_acc: 0.7250\n",
      "Epoch 949/1000\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.8285 - acc: 0.8090 - val_loss: 0.9863 - val_acc: 0.7330\n",
      "Epoch 950/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8263 - acc: 0.8084 - val_loss: 0.9702 - val_acc: 0.7430\n",
      "Epoch 951/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8244 - acc: 0.8069 - val_loss: 0.9837 - val_acc: 0.7380\n",
      "Epoch 952/1000\n",
      "7000/7000 [==============================] - 0s 49us/step - loss: 0.8264 - acc: 0.8094 - val_loss: 0.9691 - val_acc: 0.7430\n",
      "Epoch 953/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8251 - acc: 0.8076 - val_loss: 1.0016 - val_acc: 0.7140\n",
      "Epoch 954/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8287 - acc: 0.8070 - val_loss: 0.9583 - val_acc: 0.7400\n",
      "Epoch 955/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8253 - acc: 0.8109 - val_loss: 0.9662 - val_acc: 0.7390\n",
      "Epoch 956/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8239 - acc: 0.8106 - val_loss: 0.9784 - val_acc: 0.7400\n",
      "Epoch 957/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8231 - acc: 0.8131 - val_loss: 1.0140 - val_acc: 0.7240\n",
      "Epoch 958/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8289 - acc: 0.8086 - val_loss: 0.9729 - val_acc: 0.7350\n",
      "Epoch 959/1000\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8294 - acc: 0.8046 - val_loss: 0.9618 - val_acc: 0.7410\n",
      "Epoch 960/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8248 - acc: 0.8087 - val_loss: 0.9951 - val_acc: 0.7340\n",
      "Epoch 961/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8241 - acc: 0.8066 - val_loss: 1.0131 - val_acc: 0.7230\n",
      "Epoch 962/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8244 - acc: 0.8057 - val_loss: 0.9698 - val_acc: 0.7340\n",
      "Epoch 963/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8237 - acc: 0.8097 - val_loss: 0.9940 - val_acc: 0.7320\n",
      "Epoch 964/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8251 - acc: 0.8106 - val_loss: 0.9645 - val_acc: 0.7320\n",
      "Epoch 965/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8231 - acc: 0.8140 - val_loss: 0.9943 - val_acc: 0.7340\n",
      "Epoch 966/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8247 - acc: 0.8086 - val_loss: 0.9809 - val_acc: 0.7390\n",
      "Epoch 967/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8218 - acc: 0.8129 - val_loss: 0.9723 - val_acc: 0.7390\n",
      "Epoch 968/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8230 - acc: 0.8093 - val_loss: 0.9848 - val_acc: 0.7320\n",
      "Epoch 969/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8241 - acc: 0.8060 - val_loss: 0.9666 - val_acc: 0.7420\n",
      "Epoch 970/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8228 - acc: 0.8106 - val_loss: 1.0166 - val_acc: 0.7130\n",
      "Epoch 971/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8247 - acc: 0.8089 - val_loss: 0.9710 - val_acc: 0.7440\n",
      "Epoch 972/1000\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8210 - acc: 0.8121 - val_loss: 0.9652 - val_acc: 0.7350\n",
      "Epoch 973/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8262 - acc: 0.8094 - val_loss: 1.0054 - val_acc: 0.7230\n",
      "Epoch 974/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8233 - acc: 0.8107 - val_loss: 1.0211 - val_acc: 0.7130\n",
      "Epoch 975/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8283 - acc: 0.8059 - val_loss: 0.9765 - val_acc: 0.7430\n",
      "Epoch 976/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8228 - acc: 0.8104 - val_loss: 1.0043 - val_acc: 0.7250\n",
      "Epoch 977/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8208 - acc: 0.8111 - val_loss: 0.9788 - val_acc: 0.7320\n",
      "Epoch 978/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8309 - acc: 0.8060 - val_loss: 0.9794 - val_acc: 0.7340\n",
      "Epoch 979/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8197 - acc: 0.8116 - val_loss: 1.0215 - val_acc: 0.7170\n",
      "Epoch 980/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8229 - acc: 0.8059 - val_loss: 0.9798 - val_acc: 0.7400\n",
      "Epoch 981/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8214 - acc: 0.8131 - val_loss: 0.9655 - val_acc: 0.7370\n",
      "Epoch 982/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8225 - acc: 0.8099 - val_loss: 0.9718 - val_acc: 0.7350\n",
      "Epoch 983/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8205 - acc: 0.8114 - val_loss: 0.9954 - val_acc: 0.7280\n",
      "Epoch 984/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8192 - acc: 0.8103 - val_loss: 0.9721 - val_acc: 0.7280\n",
      "Epoch 985/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8219 - acc: 0.8120 - val_loss: 0.9800 - val_acc: 0.7320\n",
      "Epoch 986/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8277 - acc: 0.8066 - val_loss: 0.9806 - val_acc: 0.7400\n",
      "Epoch 987/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8234 - acc: 0.8071 - val_loss: 0.9810 - val_acc: 0.7440\n",
      "Epoch 988/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8222 - acc: 0.8094 - val_loss: 0.9724 - val_acc: 0.7350\n",
      "Epoch 989/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8205 - acc: 0.8113 - val_loss: 0.9627 - val_acc: 0.7370\n",
      "Epoch 990/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8251 - acc: 0.8076 - val_loss: 0.9652 - val_acc: 0.7340\n",
      "Epoch 991/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8235 - acc: 0.8126 - val_loss: 0.9688 - val_acc: 0.7390\n",
      "Epoch 992/1000\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.8210 - acc: 0.8101 - val_loss: 0.9674 - val_acc: 0.7410\n",
      "Epoch 993/1000\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.8198 - acc: 0.8109 - val_loss: 0.9777 - val_acc: 0.7250\n",
      "Epoch 994/1000\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.8203 - acc: 0.8111 - val_loss: 0.9849 - val_acc: 0.7410\n",
      "Epoch 995/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8224 - acc: 0.8099 - val_loss: 0.9687 - val_acc: 0.7380\n",
      "Epoch 996/1000\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.8207 - acc: 0.8123 - val_loss: 0.9694 - val_acc: 0.7320\n",
      "Epoch 997/1000\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.8238 - acc: 0.8083 - val_loss: 0.9620 - val_acc: 0.7430\n",
      "Epoch 998/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8214 - acc: 0.8084 - val_loss: 0.9773 - val_acc: 0.7320\n",
      "Epoch 999/1000\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8210 - acc: 0.8121 - val_loss: 0.9669 - val_acc: 0.7410\n",
      "Epoch 1000/1000\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.8225 - acc: 0.8137 - val_loss: 1.0074 - val_acc: 0.7360\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAIABJREFUeJzt3Xd8FHX++PHXm5AQSIAACSCELhYIoUWKYkdF5ADbCcqdisrpieXK79TTU/TOclb09Gsvd4pgR0TAglhQqdJ76KGGFkoSSMj798dM1s2yu9mUZZPs+/l47CM7M5/5zHtmNvOe+UwTVcUYY4wBqBXpAIwxxlQdlhSMMcZ4WFIwxhjjYUnBGGOMhyUFY4wxHpYUjDHGeFhSqCJEJEZEDopI68osW9WJyDsiMsb9fo6ILAulbDmmU2OWmTn+KvLbq24sKZSTu4Ep/hSJSJ5X9zVlrU9Vj6pqoqpuqsyy5SEip4nILyJyQERWikj/cEzHl6p+q6qdK6MuEZkpItd51R3WZRYNfJepV/9TRWSSiGSLyB4RmSoiHSMQoqkElhTKyd3AJKpqIrAJ+I1Xv3G+5UWk9vGPstz+D5gENAAGAlsiG44JRERqiUik/48bAhOBk4FmwELgk+MZQFX9/6oi66dMqlWw1YmI/EtE3hOR8SJyABghIn1FZJaI7BORbSLynIjEuuVri4iKSFu3+x13+FR3j/1nEWlX1rLu8ItFZLWI5IjIf0TkR397fF4KgY3qWKeqK0qZ1zUiMsCrO87dY0x3/yk+FJHt7nx/KyKnBqinv4hs8OruKSIL3XkaD9TxGtZERKa4e6d7ReQzEWnpDvs30Bd4yT1yG+tnmSW5yy1bRDaIyD0iIu6wG0XkOxF5xo15nYhcGGT+73PLHBCRZSIy2Gf4H9wjrgMislREurr924jIRDeGXSLyrNv/XyLyltf4J4qIenXPFJF/isjPwCGgtRvzCncaa0XkRp8YLnOX5X4RyRSRC0VkuIjM9il3l4h8GGhe/VHVWar6hqruUdUC4Bmgs4g09LOs+onIFu8NpYhcKSK/uN/7iHOUul9EdojIE/6mWfxbEZG/i8h24FW3/2ARWeSut5kikuY1TobX72mCiHwgvzZd3igi33qVLfF78Zl2wN+eO/yY9VOW5RlplhTC61LgXZw9qfdwNrZ3AMnAGcAA4A9Bxr8a+AfQGOdo5J9lLSsiTYH3gf/nTnc90KuUuOcATxVvvEIwHhju1X0xsFVVF7vdk4GOQHNgKfB2aRWKSB3gU+ANnHn6FBjqVaQWzoagNdAGKACeBVDVu4CfgZvdI7c7/Uzi/4B6QHvgPOAG4Pdew08HlgBNcDZyrwcJdzXO+mwIPAy8KyLN3PkYDtwHXINz5HUZsEecPdvPgUygLdAKZz2F6nfASLfOLGAHcInbfRPwHxFJd2M4HWc5/gVIAs4FNuLu3UvJpp4RhLB+SnEWkKWqOX6G/Yizrs726nc1zv8JwH+AJ1S1AXAiECxBpQKJOL+BP4rIaTi/iRtx1tsbwKfuTkodnPl9Def39BElf09lEfC358V3/VQfqmqfCn6ADUB/n37/Ar4pZby/Ah+432sDCrR1u98BXvIqOxhYWo6yI4EfvIYJsA24LkBMI4B5OM1GWUC62/9iYHaAcU4BcoB4t/s94O8Byia7sSd4xT7G/d4f2OB+Pw/YDIjXuHOKy/qpNwPI9uqe6T2P3ssMiMVJ0Cd5Db8V+Nr9fiOw0mtYA3fc5BB/D0uBS9zv04Fb/ZQ5E9gOxPgZ9i/gLa/uE51/1RLzdn8pMUwuni5OQnsiQLlXgQfd792AXUBsgLIllmmAMq2BrcCVQco8Brzifk8CcoFUt/sn4H6gSSnT6Q/kA3E+8/KAT7m1OAn7PGCTz7BZXr+9G4Fv/f1efH+nIf72gq6fqvyxI4Xw2uzdISKniMjnblPKfuAhnI1kINu9vufi7BWVtWwL7zjU+dUG23O5A3hOVafgbCi/dPc4Twe+9jeCqq7E+ee7REQSgUG4e37iXPXzuNu8sh9nzxiCz3dx3FluvMU2Fn8RkQQReU1ENrn1fhNCncWaAjHe9bnfW3p1+y5PCLD8ReQ6ryaLfThJsjiWVjjLxlcrnAR4NMSYffn+tgaJyGxxmu32AReGEAPAf3GOYsDZIXhPnSagMnOPSr8EnlXVD4IUfRe4XJym08txdjaKf5PXA52AVSIyR0QGBqlnh6oe8epuA9xVvB7c5XACznptwbG/+82UQ4i/vXLVXRVYUggv30fQvoyzF3miOofH9+PsuYfTNpzDbABERCi58fNVG2cvGlX9FLgLJxmMAMYGGa+4CelSYKGqbnD7/x7nqOM8nOaVE4tDKUvcLu+22b8B7YBe7rI8z6dssMf/7gSO4mxEvOsu8wl1EWkPvAjcgrN3mwSs5Nf52wx08DPqZqCNiMT4GXYIp2mrWHM/ZbzPMdTFaWZ5FGjmxvBlCDGgqjPdOs7AWX/lajoSkSY4v5MPVfXfwcqq06y4DbiIkk1HqOoqVR2Gk7ifAj4SkfhAVfl0b8Y56kny+tRT1ffx/3tq5fU9lGVerLTfnr/Yqg1LCsdXfZxmlkPinGwNdj6hskwGeojIb9x27DuAlCDlPwDGiEgX92TgSuAIUBcI9M8JTlK4GBiF1z85zjwfBnbj/NM9HGLcM4FaIjLaPel3JdDDp95cYK+7QbrfZ/wdOOcLjuHuCX8IPCIiieKclP8TThNBWSXibACycXLujThHCsVeA/4mIt3F0VFEWuGc89jtxlBPROq6G2Zwrt45W0RaiUgScHcpMdQB4twYjorIIOB8r+GvAzeKyLninPhPFZGTvYa/jZPYDqnqrFKmFSsi8V6fWPeE8pc4zaX3lTJ+sfE4y7wvXucNROR3IpKsqkU4/ysKFIVY5yvAreJcUi3uuv2NiCTg/J5iROQW9/d0OdDTa9xFQLr7u68LPBBkOqX99qo1SwrH11+Aa4EDOEcN74V7gqq6A7gKeBpnI9QBWICzofbn38D/cC5J3YNzdHAjzj/x5yLSIMB0snDORfSh5AnTN3HamLcCy3DajEOJ+zDOUcdNwF6cE7QTvYo8jXPksdutc6pPFWOB4W4zwtN+JvFHnGS3HvgOpxnlf6HE5hPnYuA5nPMd23ASwmyv4eNxlul7wH7gY6CRqhbiNLOdirOHuwm4wh1tGs4lnUvceieVEsM+nA3sJzjr7AqcnYHi4T/hLMfncDa0Myi5l/w/II3QjhJeAfK8Pq+60+uBk3i8799pEaSed3H2sL9S1b1e/QcCK8S5Yu9J4CqfJqKAVHU2zhHbizi/mdU4R7jev6eb3WG/Babg/h+o6nLgEeBbYBXwfZBJlfbbq9akZJOtqenc5oqtwBWq+kOk4zGR5+5J7wTSVHV9pOM5XkRkPjBWVSt6tVWNYkcKUUBEBohIQ/eyvH/gnDOYE+GwTNVxK/BjTU8I4jxGpZnbfHQDzlHdl5GOq6qpkncBmkrXDxiH0+68DBjqHk6bKCciWTjX2Q+JdCzHwak4zXgJOFdjXe42rxov1nxkjDHGw5qPjDHGeFS75qPk5GRt27ZtpMMwxphqZf78+btUNdjl6EA1TApt27Zl3rx5kQ7DGGOqFRHZWHopaz4yxhjjxZKCMcYYD0sKxhhjPCwpGGOM8bCkYIwxxsOSgjHGGA9LCsYYYzwsKRhjzHG2ZvcaVJWfN//MF5lflBi2/eB2Dh45CDivSy7SIlSVx398nB0Hw/+opmp385oxxlSmTTmbqBdbj+R6wd/mqqp8te4rzm17LrExsSWGvTTvJSaunMhnwz9j7d61PDbzMYaeMpS8gjyGdxnOxn0beX3B64zsPpL/zP4PT896mncufYcRn4woUU+7pHas3+c8rPaU5FNYuWtlieHxteO5vfftlTDXgYX1gXgiMgB4Fud9uK+p6mM+w1vjvNwkyS1zt/tu4IAyMjLU7mg2xoRiX/4+8gvzaZ5Y8u2a2w9u56bPbuK6rtdxxQdXePoPSxvG4/0fp+tLXdmbv5e7z7ib3qm96d2yN7O3zObS9y7l8lMvp1lCM/63+H98NvwzRnw8gi0HnDe5/rbzb3l/2fslphUXE8eRoyG9J6hUBf8ooHat8u3Li8h8Vc0otVy4koL7MpfVwAU4L8yeCwx333BUXOYVYIGqviginYApqto2WL2WFIyp/gqLChGEmFolX1FdpEXkFuSSGJcIwP7D+8nJz6F5YnNmZc1iY85GTm91OoVFhSzcvpC1e9Yya8ssujTtwsacjezJ28PhwsPkHM5h3tZftxPjLhvHv77/F+0atWPbgW20btiaT1d9elznuSyS4pPYl78PgM+v/pxxS8ZxVuuz+ENG+d/gG2pSCGfzUS8gU1XXuQFNwHlm+3KvMgoUv96xIc4bwYwx1ZSqoij5hfkUFhXy0ryXqB9Xn1lbZvHCwBfYcXAHh48eputLXenfvj+vDHqFF+e9SLukdny/6Xv25e9j8urJpU/Ix6RVQd9YyjUfXwPAil0rAFiwfYFn2A3db+CnzT95hvmTGJdIi/ot+GPGH7nzizt/rbfLNWTtz+K7jd/Ron4L/tDzDww5eQivL3idGRtm8Palb5MYl8h3G77j6i5Xc+ToEeJrxzNx5USe/PlJxl40lpOanMSaPWtIik/il22/sPXAVu7scydb9m+hXaN2AAzsOLDMy6S8wnmkcAUwQFVvdLt/B/RW1dFeZU7AefNRI5wXX/RX1fl+6hqF80J4Wrdu3XPjxpCe62SMKacpa6bwy7ZfuPfMe9mTt4eYWjHUj6vP1MypTFo1iVE9R7Enbw8N6jRgdtZsDhUcYv3e9by24LWIxt29eXcWbF/AtV2vBeC/i/5bYnibhm345KpPeHb2s/Ru2ZubM25GRACnqWnqmqkM7DiQNxa8wcCOAzmpyUmICEVaRC1xrst5ed7L9GrZi27Nu3nG/XbDt2S0yPAc4VRFVaH56ErgIp+k0EtVb/Mq82c3hqdEpC/wOs57YosC1WvNRybaqapnY1Rs7Z61zNw0k2lrp3H/WfezMWcjGS0yeGHOCyzYvoCru1xN1v4skuslc8OkG6gfV5+9+Xt58JwHeeDbB2jfqD2ntzqdpTuX0japLRNXTjwu8zLopEGkpaTx9fqvOTX5VAqLChm/dDyxtWIpKCrgtd+8xuCTB/PMrGcYnjactXvX0ie1D8M/Gs5D5zzEycknExcTx5AJQ7j/rPs5v/35x0zjwOEDXPrepYwdMJa0pmnHZb6qoqqQFPoCY1T1Irf7HgBVfdSrzDKco4nNbvc6oI+q7gxUryUFU90dLTrKkaNH+DnrZ85pew61pBaqyncbv2PBtgX0a92PE+qfwIJtC2hQpwEJcQk8N/s53l78NunN0lm8YzHtG7WnW/NuLM9efswVKuEWWyuWJvWaMKLLCE6ofwJNE5ryjxn/4NHzH6VL0y68s/gd/nH2P6hbuy4zNszgwOEDDD55MAePHPTsSb+37D2GnjKU+NrxfqfhL/GZiqkKSaE2zonm84EtOCear1bVZV5lpgLvqepbInIqMB1oqUGCsqRgqqpF2xeRGJfIppxNHNWjTFo1iVYNWtGzRU/GLxnPsuxlLN25lANHDhwzbt3adckrzKv0mFIbpJK1PwuAVg1a0b5Re1bvXk3LBi3ZnLOZgqIC9uTt4fXBr3NlpyupU7sOwz4cxicrPyG5XjIp9VL47rrvyC3I5cV5L3JTj5vo0LhDpcdpwi/iScENYiAwFudy0zdU9WEReQiYp6qT3CuOXgUScU46/01VvwxWpyUFE25Hi45y8MhBVu5aycnJJ9OwTkM+X/M5Hyz/gPZJ7ck5nMMzs54B4O/9/k6DOg34ZsM3fLk26E+3TP7Q8w+0b9Se5dnLSW2QSsfGHTm33bmsyF7BF2u/YMw5Yzh05BB1Y+uy4+AOGsY3pOBoAUt2LuG0Fqfx4+YfGXrKUMC5omdX7i6aJjQNef7zC/NJiEuotPkxkVclkkI4WFIw5bUrdxd1a9flwJED/N/c/yOtaRqpDVJ5dOajZO7JZOWulfRJ7cOsrFklxmtctzF78vaUaVqC0LNFTzqndCa/MJ/8wnxyC3JpVLcRnZI7MSJ9BB0ad2BzzmaS4pNIiEvgx00/0imlEw3qNDjm5ihjKqoqXJJqzHE1O2s2by9+m27Nu1E/rj4Lty+kQZ0GHDhygEdnPlp6BXBMQgDYm7cXcJpidh7a6bkR6aYeN9G/fX9um3obf+37V67rdh0pCaW+AreEVg1beb6f2ebMMo1rTDhYUjBV3o6DO5iwdAI39LiByasnk9oglZz8HJ6b8xy/7fRbHvvxMTL3ZJar7thasXRu2pk+Lfvw3rL32Ju/lxMST2DoKUNZvXs1d51xF+e3P99zOSI4J0GX7lxKl2ZdAOcuVmNqCms+MhGzfu96z805Bw4f4IdNPyAITeo14d8//puPV3xc5jrPaXsOi7Yv4vbet3PxiRczb+s8+rbqS9dmXdmUs8nT9g7OXaOxMbElNvjG1FTWfGSqjHV717Fm9xpGTx3N6NNGc9GJF/HwDw/zzuJ3AKgXW4+jRUc5fPRwqXW1adiGfq378eXaL3n70rfZnbebj1d8zGWnXsbVXa4+pnzv1N6e78UJyBgTmB0pmEqTuSeTuJg48gryWLxjMW2S2vDthm+56+u7Sh03o0WG51k1o3qMomvzrlzQ/gJ+zvqZ36X/DhGxa9eNqQA7UjBhN2fLHH7/ye/p2KQjyfWSeWvhWyGNNyxtGIM6DuLPX/6Zvql9efGSFzmh/gl+y3Zs0tHz3RKCMeFnScGUqkiLEITvNn5HgzoN+HLtl9wz/R7P8FW7V/kdb8CJAxjUcRDDuwxHEBrVbVRi+DXp14Q1bmNM2VlSMCVMXTOVnYd2MnHVRDbs20BKvRSmr59OUYDHUbVu2JpbMm7hgvYXkBiXSMcmHe3ErTHVmCWFKJVbkMuyncvYdnAbQyYMCVr2hMQTSG2QSkaLDJonNqdZQjNu6HEDMRJjTTrG1DCWFKLIO4vf4fk5zzN7y+yAZRrXbUyHRh1oGN+QMWeP4YzWZxzHCI2pGuRBQR+o+EU4lVXP8WRJoQY6WnSUnYd28sgPjzAlcwqtGrRiVtYsv5d8Dj1lKENPHkpqg1Tq16lPr5a9IhCxqekqe+PoW593d2nTCqVsZcValnr8xRKJpGJJoZpTVRZsX8DTPz/Nvvx9pDVN498//rtEmXV71wFwXrvzeOS8R+jZoidr96wlMS6Rlg1aRiLsqCAPOk1rVWlPMdhGJpwboPLUW97lpw9owHF959E3OQTaMHvX6y8e3zK+sXhPJ1B/f/X7qy/c7D6Famrjvo08+dOTrNy9kq/XfV1iWJuGbchokcHQU4Yy+OTBFGkReQV5pCSklPul3+FQlr0773Eg+D9V8fDSxgu2Z1baP6y/PVV/cfkO946trHUEiiVYGe/6vPmLwXfZhTK+77x5D/fXP5BA8fqur1DqCRRTadMNNH6gfsF+f8GmEajeUJZpRdhTUmuYnPwcdhzawXtL32PMd2OOuRpowuUT6HFCD0SEDo06VPoJ4IruRfrb4ATai6qI0jZoof4jlrah9J1msCRSWvILlhz81ec7XmUMDzQf/uaxtD3fUDbugcr7Lo9AZQPV77teAvFXtrREEEx5mrKCLV/fmMq68+R3epYUqq9DRw7x0+afeHfpu5yeejrT1k7z+xyg9GbpfH7159SJqVPmp3OWR1n3xouVtqfq27+0jU+gePyNH0o8pf3Tlrah8zdfvnUEE2yDVJ490fKsl7JseCsj0ZYlOZVWT7CNcGl76OXZwFZow1yB+fIuU65pW1KofnYe2smr81/lvhn3legfFxNHvdh6pDdLJyk+iX+e+086NOpQaS9BCfbPVda999I2XGXdwww2fmkxB9vTCrZx843Pn2B7ub71VfRoKJR5CtQd6MjEX3yhJLhg81GWpBEokftOJ1h3oI1zaXvnpW1YQ9nTDzWG0tZFsBiKx6ssoSYFVLVafXr27Kk1yU2TblLGoAkPJyhjKPG5c+qd+vait3Vf3r5Kny5j8PwN5eNbNlAdgaYRSv9gcZZ1voL1C1RnsLgCzWOwOoKV9R4WbDkFqyfYcg1lOYQi1GVX3voCDfP9XtHpBqvfX7nyLr9Qfx+Byoa6rssD542XpW5jI76RL+unuieF1btW68iJI/W0V07TpMeSSvzjJz6SqDd/drPuOrRLc4/kVto0Q9nYBNqglHWjUJ4NblnqLy2u8vzjlvcfrizTrMh0gtVT2Rvtyqqjsh2vHYiKKE/yCjVRVYZQk4I1Hx0HRVrEMz8/ww+bfuDTVZ8CIAhtk9qS2iCV0b1GM+TkIdSpXSfkOgO1TYbS5BPoMN97uL9plFdl1FNZsVR0eqGeRCytnuo6Tqj1QuU2fVRUuJqaQi1blmUStvVi5xQiK/tQNk/9/BTbD27nv4v+W2LY307/G4/1f6zcVwiVdhK3uL9v+Yq0cVYlkY470tM3x0c4k2Ykfj+WFCJAVflm/Te8sfANPlr+kecO4rq169KzRU9uPe1WTk0+lfRm6aUmhFBOToZyZYhv2Zq2MauJ82SiU7iPsKpEUhCRAcCzQAzwmqo+5jP8GeBct7Me0FRVk4LVWVWTQsHRAsZ8O4ZHZj7i6Xdm6zP54MoPaJbYrNTxS7tKpLL28G0jakzlKk/TVCREPCmISAywGrgAyALmAsNVdXmA8rcB3VV1ZLB6q2JSmLtlLr8Z/xt2HNpB04Sm3NH7Dg4XHubes+4lLiYu5HqCNQsFunY72GWDoU6zKv1wjTHhURXevNYLyFTVdW5AE4AhgN+kAAwHHghjPJVKVcnOzWbAOwNYsH0BACc2PpHZN86mcd3GZaor0DX5xd+9+/uWOyauMm7gLSEYY7yFMym0BDZ7dWcBvf0VFJE2QDvgmzDGU2lUlT9+/kdemv8S4DQTvXfFewFfKekrUNthZd69aIwx5RHOpODv7Gigrdww4ENVPeq3IpFRwCiA1q1bV0505bRh3wYGjhvIil0rABh70Vju6HNHqeP5O/lbWnIoLmPJwRhzvITzvYlZQCuv7lRga4Cyw4DxgSpS1VdUNUNVM1JSwv+Mn0DW713PBW9f4EkIs26YFVJCgMBNPaXdV2AJwRhzPIXzSGEu0FFE2gFbcDb8V/sWEpGTgUbAz2GMpUKOFh3lw+UfcvPnN5NbkMu0a6Zx0YkXhTx+oOfrBOtnjDGRELakoKqFIjIa+ALnktQ3VHWZiDyEc7v1JLfocGCCVuEbJu6fcT+PzHyEGIlh1o2zyGjh/wR+KHcJG2NMVWY3r5XicOFhkp9I5uCRgyz4wwK6Ne92TJloulnMGFM9VYVLUmuE8UvHc/DIQaZcPaVEQgjl0RKWEIwx1U04TzRXe0eLjvLYzMdIa5rGgBMHePr7XjVUfMLYmyUEY0x1ZEcKQfyy7RdW7V7FW0PeQkT8njC2ZiJjTE1iRwoBFBwtoNdrvQA8Vxp5JwKwm8yMMTWPJYUAvt3wLQDntTuP5onNAf+PozDGmJrEkkIAD33/EHVr12Xy8MmAHRUYY6KDnVPwY+H2hczcNJPz251P3di6ds7AGBM17EjBj2mZ0wB48sInLSEYY6KKJQU/3lz4Jr1a9qL7y92BwG8+M8aYmsaSgo/NOZtZvXs1Q04ecszVRsYYU9NZUvDx0ryXiJEY7v3mXms6MsZEHUsKPr5a9xVntD7DkoExJipZUvBy5OgRlu5cSo/mPewowRgTlSwpePl+4/fkFeZxXrvzLCEYY6KSJQUvc7bMAWDwhMF2xZExJipZUvCyLHuZ57sdKRhjopElBS+bczZzVpuzLCEYY6KWJQVXkRaRuSeT1AapkQ7FGGMixpKCa86WOWw7uI13l7wb6VCMMSZiLCm4NuVsAmDJLUsiHIkxxkSOJQXXjoM7ADzvTjDGmGhkScG1ef9mateqTeO6jSMdijHGRExYk4KIDBCRVSKSKSJ3ByjzWxFZLiLLRCRiDfozNsygT2ofYh6KiVQIxhgTcWF7yY6IxAAvABcAWcBcEZmkqsu9ynQE7gHOUNW9ItI0XPGUZlPOJnYe2hmpyRtjTJUQziOFXkCmqq5T1SPABGCIT5mbgBdUdS+AqkZkq1ykRezK3cW9Z95r9ygYY6JaOJNCS2CzV3eW28/bScBJIvKjiMwSkQH+KhKRUSIyT0TmZWdnV3qge/L2UKRFPPzDw5VetzHGVCfhTAr+Hh7kuxteG+gInAMMB14TkaRjRlJ9RVUzVDUjJSWl0gPNPlT5icYYY6qjcCaFLKCVV3cqsNVPmU9VtUBV1wOrcJLEcZWd6ySFr3731fGetDHGVCnhTApzgY4i0k5E4oBhwCSfMhOBcwFEJBmnOWldGGPyq/gEc0q9yj8KMcaY6iRsSUFVC4HRwBfACuB9VV0mIg+JyGC32BfAbhFZDswA/p+q7g5XTIEUNx81TYjYxU/GGFMlhO2SVABVnQJM8el3v9d3Bf7sfiImc08m8bXjSUmwIwVjTHSzO5qBxTsXk1+YT+1aYc2RxhhT5VlSAHbl7op0CMYYUyVYUgD2H97PNV2uiXQYxhgTcZYUgHV719GgToNIh2GMMRFnSQGIi4mzpGCMMVhS4HDhYY4cPWJJwRhjsKTA/sP7ASwpGGMMlhQsKRhjjBdLCpYUjDHGw5KCJQVjjPGwpGBJwRhjPCwpWFIwxhgPSwqWFIwxxsOSgiUFY4zxiPqkkHM4h9q1alO3dt1Ih2KMMREX9Ulh/+H9NKjTABF/r5Q2xpjoYknBTQrGGGMsKVhSMMYYL5YULCkYY4xH1L9/cv/h/czfNj/SYRhjTJUQ0pGCiHQQkTru93NE5HYRSQpvaMfH/sP7GZY2LNJhGGNMlRBq89FHwFERORF4HWgHvFvaSCIyQERWiUimiNztZ/h1IpItIgvdz41lir4S7D+8nwZx1nxkjDEQevNRkaoWisilwFhV/Y+ILAg2gojEAC8AFwBZwFwRmaSqy32Kvqeqo8sceSWxcwrGGPOrUI8UCkRkOHAtMNntF1vKOL2ATFVdp6pHgAnAkPKFGR4FRwvIK8yzpGBb0S5MAAAZe0lEQVSMMa5Qk8L1QF/gYVVdLyLtgHdKGaclsNmrO8vt5+tyEVksIh+KSCt/FYnIKBGZJyLzsrOzQwy5dAeOHADsERfGGFMspKSgqstV9XZVHS8ijYD6qvpYKaP5u0VYfbo/A9qqajrwNfDfANN/RVUzVDUjJSUllJBDYs89MsaYkkK9+uhbEWkgIo2BRcCbIvJ0KaNlAd57/qnAVu8CqrpbVQ+7na8CPUMLu3JYUjDGmJJCbT5qqKr7gcuAN1W1J9C/lHHmAh1FpJ2IxAHDgEneBUTkBK/OwcCKEOOpFDn5OYAlBWOMKRbq1Ue13Q34b4F7QxnBvVppNPAFEAO8oarLROQhYJ6qTgJuF5HBQCGwB7iurDNQEcXnFOrXqX88J2uMMVVWqEnhIZyN+4+qOldE2gNrShtJVacAU3z63e/1/R7gntDDrVx5BXkA1IutF6kQjDGmSgkpKajqB8AHXt3rgMvDFdTxkluQC2DvUjDGGFeoJ5pTReQTEdkpIjtE5CMRSQ13cOGWV2hHCsYY4y3UE81v4pwkboFzr8Fnbr9qrbj5qG6sHSkYYwyEnhRSVPVNVS10P28BlXfDQIQUNx/ZkYIxxjhCTQq7RGSEiMS4nxHA7nAGdjzkFeYhCHVi6kQ6FGOMqRJCTQojcS5H3Q5sA67AefRFtZZbkEt87Xh7P7MxxrhCfczFJlUdrKopqtpUVYfi3MhWreUV5FnTkTHGeKnI6zj/XGlRREhuYa6dZDbGGC8VSQrVvs3FjhSMMaakiiQF3yeeVjt5hXl245oxxngJekeziBzA/8ZfgGq/Nc0tsOYjY4zxFjQpqGqNflKcNR8ZY0xJoT4Qr0bKLcjlx80/RjoMY4ypMipyTqHayyvM48pOV0Y6DGOMqTKiOykU5Nk5BWOM8RLVSeHAkQMkxiZGOgxjjKkyojYpqCr7D++nYXzDSIdijDFVRtQmhbzCPAqLCu39zMYY4yVqk8L+w/sBaFjHjhSMMaZY1CcFO1IwxphfRW1SOHD4AAD169To+/OMMaZMwpoURGSAiKwSkUwRuTtIuStEREUkI5zxeMsvzAewZx8ZY4yXsCUFEYkBXgAuBjoBw0Wkk59y9YHbgdnhisWf4qQQXzv+eE7WGGOqtHAeKfQCMlV1naoeASYAQ/yU+yfwOJAfxliOYUnBGGOOFc6k0BLY7NWd5fbzEJHuQCtVnRysIhEZJSLzRGRednZ2pQRXnBTq1Lb3MxtjTLFwJgV/L+HxPIZbRGoBzwB/Ka0iVX1FVTNUNSMlJaVSgrMjBWOMOVY4k0IW0MqrOxXY6tVdH0gDvhWRDUAfYNLxOtlsScEYY44VzqQwF+goIu1EJA4YBkwqHqiqOaqarKptVbUtMAsYrKrzwhiThyUFY4w5VtiSgqoWAqOBL4AVwPuqukxEHhKRweGabqgsKRhjzLHC+pIdVZ0CTPHpd3+AsueEMxZflhSMMeZYUXtH88EjB6ldqzaxtWIjHYoxxlQZUZsU9uTtoXHdxoj4u0jKGGOiU9Qmhb35e2kU3yjSYRhjTJUStUmh+EjBGGPMr6I2KezL30dSfFKkwzDGmColapNCbkEuCXEJkQ7DGGOqlKhNCvmF+XY5qjHG+IjapJBXmGfvUjDGGB9RmxS2H9xuRwrGGOMjapNCnZg6dqRgjDE+ojIpqCqHjx62IwVjjPERlUnB837mWDtSMMYYb1GdFOxIwRhjSorKpHDgyAEAO6dgjDE+ojIp/LT5JwC6n9A9wpEYY0zVEpVJYduBbQCc1OSkCEdijDFVS1QmhdyCXADqxdaLcCTGGFO1RG1SiJEYe8GOMcb4iNqkUC+2nr1gxxhjfER1UjDGGFNSdCaFQksKxhjjT1iTgogMEJFVIpIpInf7GX6ziCwRkYUiMlNEOoUznmLvLH7HkoIxxvgRtqQgIjHAC8DFQCdguJ+N/ruq2kVVuwGPA0+HKx5vF3W4yJKCMcb4Ec4jhV5ApqquU9UjwARgiHcBVd3v1ZkAaBjj8didt9vez2yMMX6EMym0BDZ7dWe5/UoQkVtFZC3OkcLt/ioSkVEiMk9E5mVnZ1c4sF25u0hJSKlwPcYYU9OEMyn4u97zmCMBVX1BVTsAdwH3+atIVV9R1QxVzUhJqfjGPPtQNsl1kytcjzHG1DThTApZQCuv7lRga5DyE4ChYYwHgLyCPA4VHLIjBWOM8SOcSWEu0FFE2olIHDAMmORdQEQ6enVeAqwJYzyA03QEkFLPkoIxxviqHa6KVbVQREYDXwAxwBuqukxEHgLmqeokYLSI9AcKgL3AteGKp1hxUkiuZ81HxhjjK2xJAUBVpwBTfPrd7/X9jnBO35/sXOdEtTUfGWPMsaLujubsQ05SsCMFY4w5VtQlBTunYIwxgUVdUihuPmpUt1GEIzHGmKon6pLCrtxdpNRLoZZE3awbY0ypom7L+PL8l+0kszHGBBDWq4+qojNbn2lHCcYYE0DUbR135e6yK4+MMSYASwrGGGM8oiopqCp78vbQpG6TSIdijDFVUlSdUzhUcIijepSk+KRIh2JMRBQUFJCVlUV+fn6kQzFhEh8fT2pqKrGxseUaP6qSwr78fQCWFEzUysrKon79+rRt2xYRf0+3N9WZqrJ7926ysrJo165dueqIquYjSwom2uXn59OkSRNLCDWUiNCkSZMKHQlGVVLYm7cXsLuZTXSzhFCzVXT9RlVSsCMFY4wJzpKCMea42b17N926daNbt240b96cli1berqPHDkSUh3XX389q1atClrmhRdeYNy4cZURcqW77777GDt27DH9r732WlJSUujWrVsEovpVVJ1o3pvvNB9ZUjAmMpo0acLChQsBGDNmDImJifz1r38tUUZVUVVq1fK/z/rmm2+WOp1bb7214sEeZyNHjuTWW29l1KhREY0jqpJC8ZFCwzoNIxyJMZF357Q7Wbh9YaXW2a15N8YOOHYvuDSZmZkMHTqUfv36MXv2bCZPnsyDDz7IL7/8Ql5eHldddRX33++8n6tfv348//zzpKWlkZyczM0338zUqVOpV68en376KU2bNuW+++4jOTmZO++8k379+tGvXz+++eYbcnJyePPNNzn99NM5dOgQv//978nMzKRTp06sWbOG11577Zg99QceeIApU6aQl5dHv379ePHFFxERVq9ezc0338zu3buJiYnh448/pm3btjzyyCOMHz+eWrVqMWjQIB5++OGQlsHZZ59NZmZmmZddZYu65qPEuERiY8p3/a4xJnyWL1/ODTfcwIIFC2jZsiWPPfYY8+bNY9GiRXz11VcsX778mHFycnI4++yzWbRoEX379uWNN97wW7eqMmfOHJ544gkeeughAP7zn//QvHlzFi1axN13382CBQv8jnvHHXcwd+5clixZQk5ODtOmTQNg+PDh/OlPf2LRokX89NNPNG3alM8++4ypU6cyZ84cFi1axF/+8pdKWjrHT9QdKVjTkTGO8uzRh1OHDh047bTTPN3jx4/n9ddfp7CwkK1bt7J8+XI6depUYpy6dety8cUXA9CzZ09++OEHv3VfdtllnjIbNmwAYObMmdx1110AdO3alc6dO/sdd/r06TzxxBPk5+eza9cuevbsSZ8+fdi1axe/+c1vAOeGMYCvv/6akSNHUrduXQAaN25cnkURUVGVFPbm77WkYEwVlZCQ4Pm+Zs0ann32WebMmUNSUhIjRozwe+19XFyc53tMTAyFhYV+665Tp84xZVS11Jhyc3MZPXo0v/zyCy1btuS+++7zxOHv0k9VrfaX/EZd85ElBWOqvv3791O/fn0aNGjAtm3b+OKLLyp9Gv369eP9998HYMmSJX6bp/Ly8qhVqxbJyckcOHCAjz76CIBGjRqRnJzMZ599Bjg3Bebm5nLhhRfy+uuvk5eXB8CePXsqPe5wC2tSEJEBIrJKRDJF5G4/w/8sIstFZLGITBeRNuGMZ1/+PhrF241rxlR1PXr0oFOnTqSlpXHTTTdxxhlnVPo0brvtNrZs2UJ6ejpPPfUUaWlpNGxY8iKUJk2acO2115KWlsall15K7969PcPGjRvHU089RXp6Ov369SM7O5tBgwYxYMAAMjIy6NatG88884zfaY8ZM4bU1FRSU1Np27YtAFdeeSVnnnkmy5cvJzU1lbfeeqvS5zkUEsohVLkqFokBVgMXAFnAXGC4qi73KnMuMFtVc0XkFuAcVb0qWL0ZGRk6b968csXU/tn2nN7qdN657J1yjW9MdbdixQpOPfXUSIdRJRQWFlJYWEh8fDxr1qzhwgsvZM2aNdSuXf1b1f2tZxGZr6oZpY0bzrnvBWSq6jo3oAnAEMCTFFR1hlf5WcCIMMZDbkEuCbEJpRc0xtR4Bw8e5Pzzz6ewsBBV5eWXX64RCaGiwrkEWgKbvbqzgN4BygLcAEz1N0BERgGjAFq3bl3ugHYc2kG92HrlHt8YU3MkJSUxf/78SIdR5YTznIK/U/B+26pEZASQATzhb7iqvqKqGaqakZKSUq5gVJVaUouEODtSMMaYQMJ5pJAFtPLqTgW2+hYSkf7AvcDZqno4XMEcPnqYIi2y5iNjjAkinEcKc4GOItJOROKAYcAk7wIi0h14GRisqjvDGAuHjhwCsOYjY4wJImxJQVULgdHAF8AK4H1VXSYiD4nIYLfYE0Ai8IGILBSRSQGqq7DcglwAaz4yxpggwnqfgqpOUdWTVLWDqj7s9rtfVSe53/urajNV7eZ+BgevsfwOFdiRgjGRds455xxzI9rYsWP54x//GHS8xMREALZu3coVV1wRsO7SLlcfO3Ysubm5nu6BAweyb9++UEI/rr799lsGDRp0TP/nn3+eE088ERFh165dYZl21NzR7DlSsHMKxkTM8OHDmTBhQol+EyZMYPjw4SGN36JFCz788MNyT983KUyZMoWkpOrzlIMzzjiDr7/+mjZtwnefb9QkheJzCtZ8ZEzZyYOV8zyfK664gsmTJ3P4sHNNyYYNG9i6dSv9+vXz3DfQo0cPunTpwqeffnrM+Bs2bCAtLQ1wHkExbNgw0tPTueqqqzyPlgC45ZZbyMjIoHPnzjzwwAMAPPfcc2zdupVzzz2Xc889F4C2bdt69riffvpp0tLSSEtL87wEZ8OGDZx66qncdNNNdO7cmQsvvLDEdIp99tln9O7dm+7du9O/f3927NgBOPdCXH/99XTp0oX09HTPYzKmTZtGjx496Nq1K+eff37Iy6979+6eO6DDpviFFtXl07NnTy2PaWumKWPQHzf9WK7xjakJli9fHukQdODAgTpx4kRVVX300Uf1r3/9q6qqFhQUaE5OjqqqZmdna4cOHbSoqEhVVRMSElRVdf369dq5c2dVVX3qqaf0+uuvV1XVRYsWaUxMjM6dO1dVVXfv3q2qqoWFhXr22WfrokWLVFW1TZs2mp2d7YmluHvevHmalpamBw8e1AMHDminTp30l19+0fXr12tMTIwuWLBAVVWvvPJKffvtt4+Zpz179nhiffXVV/XPf/6zqqr+7W9/0zvuuKNEuZ07d2pqaqquW7euRKzeZsyYoZdccknAZeg7H778rWdgnoawjY2eIwX3nII1HxkTWd5NSN5NR6rK3//+d9LT0+nfvz9btmzx7HH78/333zNihPMQhPT0dNLT0z3D3n//fXr06EH37t1ZtmyZ34fdeZs5cyaXXnopCQkJJCYmctlll3kew92uXTvPi3e8H73tLSsri4suuoguXbrwxBNPsGzZMsB5lLb3W+AaNWrErFmzOOuss2jXrh1Q9R6vHT1JwS5JNaZKGDp0KNOnT/e8Va1Hjx6A84C57Oxs5s+fz8KFC2nWrJnfx2V78/eY6vXr1/Pkk08yffp0Fi9ezCWXXFJqPRrkGXDFj92GwI/nvu222xg9ejRLlizh5Zdf9kxP/TxK21+/qiRqkoJdkmpM1ZCYmMg555zDyJEjS5xgzsnJoWnTpsTGxjJjxgw2btwYtJ6zzjqLcePGAbB06VIWL14MOI/dTkhIoGHDhuzYsYOpU399ek79+vU5cOCA37omTpxIbm4uhw4d4pNPPuHMM88MeZ5ycnJo2bIlAP/97389/S+88EKef/55T/fevXvp27cv3333HevXrweq3uO1oyYpWPORMVXH8OHDWbRoEcOGDfP0u+aaa5g3bx4ZGRmMGzeOU045JWgdt9xyCwcPHiQ9PZ3HH3+cXr16Ac5b1Lp3707nzp0ZOXJkicdujxo1iosvvthzorlYjx49uO666+jVqxe9e/fmxhtvpHv37iHPz5gxYzyPvk5OTvb0v++++9i7dy9paWl07dqVGTNmkJKSwiuvvMJll11G165dueoq/w+Gnj59uufx2qmpqfz8888899xzpKamkpWVRXp6OjfeeGPIMYYqbI/ODpfyPjr705Wf8vbitxl/+Xh7R7OJWvbo7OhQVR+dXaUMOWUIQ04ZEukwjDGmSoua5iNjjDGls6RgTJSpbk3Gpmwqun4tKRgTReLj49m9e7clhhpKVdm9ezfx8fHlriNqzikYY/BcuZKdnR3pUEyYxMfHk5qaWu7xLSkYE0ViY2M9d9Ia4481HxljjPGwpGCMMcbDkoIxxhiPandHs4hkA8EfihJYMhCe1xVVXTbP0cHmOTpUZJ7bqGpKaYWqXVKoCBGZF8pt3jWJzXN0sHmODsdjnq35yBhjjIclBWOMMR7RlhReiXQAEWDzHB1snqND2Oc5qs4pGGOMCS7ajhSMMcYEYUnBGGOMR1QkBREZICKrRCRTRO6OdDyVRURaicgMEVkhIstE5A63f2MR+UpE1rh/G7n9RUSec5fDYhHpEdk5KD8RiRGRBSIy2e1uJyKz3Xl+T0Ti3P513O5Md3jbSMZdXiKSJCIfishKd333renrWUT+5P6ul4rIeBGJr2nrWUTeEJGdIrLUq1+Z16uIXOuWXyMi11YkphqfFEQkBngBuBjoBAwXkU6RjarSFAJ/UdVTgT7Are683Q1MV9WOwHS3G5xl0NH9jAJePP4hV5o7gBVe3f8GnnHneS9wg9v/BmCvqp4IPOOWq46eBaap6ilAV5x5r7HrWURaArcDGaqaBsQAw6h56/ktYIBPvzKtVxFpDDwA9AZ6AQ8UJ5JyUdUa/QH6Al94dd8D3BPpuMI0r58CFwCrgBPcficAq9zvLwPDvcp7ylWnD5Dq/rOcB0wGBOcuz9q+6xz4Aujrfq/tlpNIz0MZ57cBsN437pq8noGWwGagsbveJgMX1cT1DLQFlpZ3vQLDgZe9+pcoV9ZPjT9S4NcfV7Est1+N4h4udwdmA81UdRuA+7epW6ymLIuxwN+AIre7CbBPVQvdbu/58syzOzzHLV+dtAeygTfdJrPXRCSBGryeVXUL8CSwCdiGs97mU7PXc7GyrtdKXd/RkBTET78adR2uiCQCHwF3qur+YEX99KtWy0JEBgE7VXW+d28/RTWEYdVFbaAH8KKqdgcO8WuTgj/Vfp7d5o8hQDugBZCA03ziqyat59IEmsdKnfdoSApZQCuv7lRga4RiqXQiEouTEMap6sdu7x0icoI7/ARgp9u/JiyLM4DBIrIBmIDThDQWSBKR4pdGec+XZ57d4Q2BPccz4EqQBWSp6my3+0OcJFGT13N/YL2qZqtqAfAxcDo1ez0XK+t6rdT1HQ1JYS7Q0b1qIQ7nZNWkCMdUKUREgNeBFar6tNegSUDxFQjX4pxrKO7/e/cqhj5ATvFhanWhqveoaqqqtsVZl9+o6jXADOAKt5jvPBcviyvc8tVqD1JVtwObReRkt9f5wHJq8HrGaTbqIyL13N958TzX2PXspazr9QvgQhFp5B5hXej2K59In2Q5TidyBgKrgbXAvZGOpxLnqx/OYeJiYKH7GYjTljodWOP+beyWF5wrsdYCS3Cu7Ij4fFRg/s8BJrvf2wNzgEzgA6CO2z/e7c50h7ePdNzlnNduwDx3XU8EGtX09Qw8CKwElgJvA3Vq2noGxuOcMynA2eO/oTzrFRjpznsmcH1FYrLHXBhjjPGIhuYjY4wxIbKkYIwxxsOSgjHGGA9LCsYYYzwsKRhjjPGwpGCMS0SOishCr0+lPVFXRNp6PwnTmKqqdulFjIkaearaLdJBGBNJdqRgTClEZIOI/FtE5rifE93+bURkuvts++ki0trt30xEPhGRRe7ndLeqGBF51X1HwJciUtctf7uILHfrmRCh2TQGsKRgjLe6Ps1HV3kN26+qvYDncZ61hPv9f6qaDowDnnP7Pwd8p6pdcZ5RtMzt3xF4QVU7A/uAy93+dwPd3XpuDtfMGRMKu6PZGJeIHFTVRD/9NwDnqeo69wGE21W1iYjswnnufYHbf5uqJotINpCqqoe96mgLfKXOi1MQkbuAWFX9l4hMAw7iPL5ioqoeDPOsGhOQHSkYExoN8D1QGX8Oe30/yq/n9C7BeaZNT2C+11NAjTnuLCkYE5qrvP7+7H7/CedJrQDXADPd79OBW8DzLukGgSoVkVpAK1WdgfPioCTgmKMVY44X2yMx5ld1RWShV/c0VS2+LLWOiMzG2ZEa7va7HXhDRP4fzpvRrnf73wG8IiI34BwR3ILzJEx/YoB3RKQhzlMwn1HVfZU2R8aUkZ1TMKYU7jmFDFXdFelYjAk3az4yxhjjYUcKxhhjPOxIwRhjjIclBWOMMR6WFIwxxnhYUjDGGONhScEYY4zH/wd81GG2uoEDgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 72us/step\n",
      "2000/2000 [==============================] - 0s 65us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8268521186964852, 0.8081428571428572]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.962368935585022, 0.7435]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best result you've achieved so far, but you were training for quite a while! Next, experiment with dropout regularization to see if it offers any advantages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7000/7000 [==============================] - 1s 121us/step - loss: 1.9910 - acc: 0.1394 - val_loss: 1.9494 - val_acc: 0.1440\n",
      "Epoch 2/200\n",
      "7000/7000 [==============================] - 1s 85us/step - loss: 1.9630 - acc: 0.1504 - val_loss: 1.9371 - val_acc: 0.1630\n",
      "Epoch 3/200\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.9504 - acc: 0.1599 - val_loss: 1.9306 - val_acc: 0.1760\n",
      "Epoch 4/200\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 1.9456 - acc: 0.1650 - val_loss: 1.9255 - val_acc: 0.1850\n",
      "Epoch 5/200\n",
      "7000/7000 [==============================] - 1s 76us/step - loss: 1.9379 - acc: 0.1703 - val_loss: 1.9212 - val_acc: 0.2000\n",
      "Epoch 6/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 1.9327 - acc: 0.1760 - val_loss: 1.9170 - val_acc: 0.2100\n",
      "Epoch 7/200\n",
      "7000/7000 [==============================] - 0s 69us/step - loss: 1.9250 - acc: 0.1896 - val_loss: 1.9129 - val_acc: 0.2170\n",
      "Epoch 8/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 1.9259 - acc: 0.1901 - val_loss: 1.9084 - val_acc: 0.2280\n",
      "Epoch 9/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 1.9216 - acc: 0.1806 - val_loss: 1.9040 - val_acc: 0.2330\n",
      "Epoch 10/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.9170 - acc: 0.1964 - val_loss: 1.8989 - val_acc: 0.2420\n",
      "Epoch 11/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 1.9095 - acc: 0.1949 - val_loss: 1.8934 - val_acc: 0.2480\n",
      "Epoch 12/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.9039 - acc: 0.2091 - val_loss: 1.8873 - val_acc: 0.2600\n",
      "Epoch 13/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.9028 - acc: 0.2103 - val_loss: 1.8811 - val_acc: 0.2640\n",
      "Epoch 14/200\n",
      "7000/7000 [==============================] - 0s 58us/step - loss: 1.8991 - acc: 0.2119 - val_loss: 1.8735 - val_acc: 0.2780\n",
      "Epoch 15/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.8972 - acc: 0.2117 - val_loss: 1.8664 - val_acc: 0.2880\n",
      "Epoch 16/200\n",
      "7000/7000 [==============================] - 0s 56us/step - loss: 1.8859 - acc: 0.2243 - val_loss: 1.8590 - val_acc: 0.3010\n",
      "Epoch 17/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.8728 - acc: 0.2269 - val_loss: 1.8491 - val_acc: 0.3140\n",
      "Epoch 18/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.8735 - acc: 0.2280 - val_loss: 1.8392 - val_acc: 0.3260\n",
      "Epoch 19/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.8605 - acc: 0.2400 - val_loss: 1.8285 - val_acc: 0.3300\n",
      "Epoch 20/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.8511 - acc: 0.2527 - val_loss: 1.8147 - val_acc: 0.3510\n",
      "Epoch 21/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.8446 - acc: 0.2537 - val_loss: 1.8022 - val_acc: 0.3680\n",
      "Epoch 22/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.8350 - acc: 0.2629 - val_loss: 1.7872 - val_acc: 0.3780\n",
      "Epoch 23/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.8206 - acc: 0.2701 - val_loss: 1.7710 - val_acc: 0.3900\n",
      "Epoch 24/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.8138 - acc: 0.2736 - val_loss: 1.7548 - val_acc: 0.4040\n",
      "Epoch 25/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.7928 - acc: 0.2929 - val_loss: 1.7359 - val_acc: 0.4220\n",
      "Epoch 26/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.7842 - acc: 0.2947 - val_loss: 1.7186 - val_acc: 0.4380\n",
      "Epoch 27/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.7718 - acc: 0.3121 - val_loss: 1.6999 - val_acc: 0.4580\n",
      "Epoch 28/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.7481 - acc: 0.3157 - val_loss: 1.6790 - val_acc: 0.4720\n",
      "Epoch 29/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.7372 - acc: 0.3169 - val_loss: 1.6568 - val_acc: 0.4850\n",
      "Epoch 30/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.7240 - acc: 0.3279 - val_loss: 1.6366 - val_acc: 0.5110\n",
      "Epoch 31/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.7074 - acc: 0.3336 - val_loss: 1.6117 - val_acc: 0.5180\n",
      "Epoch 32/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.6949 - acc: 0.3490 - val_loss: 1.5878 - val_acc: 0.5270\n",
      "Epoch 33/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.6566 - acc: 0.3689 - val_loss: 1.5627 - val_acc: 0.5380\n",
      "Epoch 34/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.6547 - acc: 0.3583 - val_loss: 1.5412 - val_acc: 0.5480\n",
      "Epoch 35/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.6391 - acc: 0.3740 - val_loss: 1.5178 - val_acc: 0.5480\n",
      "Epoch 36/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.6343 - acc: 0.3750 - val_loss: 1.4967 - val_acc: 0.5620\n",
      "Epoch 37/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.6092 - acc: 0.3873 - val_loss: 1.4734 - val_acc: 0.5690\n",
      "Epoch 38/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.5909 - acc: 0.3949 - val_loss: 1.4521 - val_acc: 0.5810\n",
      "Epoch 39/200\n",
      "7000/7000 [==============================] - 1s 77us/step - loss: 1.5798 - acc: 0.3996 - val_loss: 1.4279 - val_acc: 0.5840\n",
      "Epoch 40/200\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 1.5460 - acc: 0.4171 - val_loss: 1.4022 - val_acc: 0.5980\n",
      "Epoch 41/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.5375 - acc: 0.4199 - val_loss: 1.3787 - val_acc: 0.5980\n",
      "Epoch 42/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.5155 - acc: 0.4266 - val_loss: 1.3598 - val_acc: 0.6060\n",
      "Epoch 43/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.5185 - acc: 0.4184 - val_loss: 1.3430 - val_acc: 0.6140\n",
      "Epoch 44/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.4977 - acc: 0.4370 - val_loss: 1.3219 - val_acc: 0.6170\n",
      "Epoch 45/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.4683 - acc: 0.4480 - val_loss: 1.3008 - val_acc: 0.6190\n",
      "Epoch 46/200\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 1.4706 - acc: 0.4474 - val_loss: 1.2843 - val_acc: 0.6190\n",
      "Epoch 47/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.4476 - acc: 0.4539 - val_loss: 1.2662 - val_acc: 0.6220\n",
      "Epoch 48/200\n",
      "7000/7000 [==============================] - 1s 76us/step - loss: 1.4286 - acc: 0.4651 - val_loss: 1.2467 - val_acc: 0.6350\n",
      "Epoch 49/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.4318 - acc: 0.4587 - val_loss: 1.2339 - val_acc: 0.6390\n",
      "Epoch 50/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.4099 - acc: 0.4690 - val_loss: 1.2178 - val_acc: 0.6420\n",
      "Epoch 51/200\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 1.3962 - acc: 0.4694 - val_loss: 1.2049 - val_acc: 0.6400\n",
      "Epoch 52/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.3713 - acc: 0.4864 - val_loss: 1.1860 - val_acc: 0.6460\n",
      "Epoch 53/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.3760 - acc: 0.4817 - val_loss: 1.1757 - val_acc: 0.6440\n",
      "Epoch 54/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.3621 - acc: 0.4914 - val_loss: 1.1591 - val_acc: 0.6530\n",
      "Epoch 55/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.3519 - acc: 0.4960 - val_loss: 1.1495 - val_acc: 0.6500\n",
      "Epoch 56/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 1.3454 - acc: 0.4974 - val_loss: 1.1386 - val_acc: 0.6570\n",
      "Epoch 57/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 1.3238 - acc: 0.5001 - val_loss: 1.1214 - val_acc: 0.6590\n",
      "Epoch 58/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.3135 - acc: 0.4996 - val_loss: 1.1111 - val_acc: 0.6630\n",
      "Epoch 59/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.3155 - acc: 0.5091 - val_loss: 1.0993 - val_acc: 0.6650\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 66us/step - loss: 1.3095 - acc: 0.5129 - val_loss: 1.0902 - val_acc: 0.6670\n",
      "Epoch 61/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.2821 - acc: 0.5229 - val_loss: 1.0762 - val_acc: 0.6680\n",
      "Epoch 62/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.2747 - acc: 0.5251 - val_loss: 1.0634 - val_acc: 0.6710\n",
      "Epoch 63/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.2575 - acc: 0.5330 - val_loss: 1.0518 - val_acc: 0.6730\n",
      "Epoch 64/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.2629 - acc: 0.5244 - val_loss: 1.0435 - val_acc: 0.6790\n",
      "Epoch 65/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.2466 - acc: 0.5341 - val_loss: 1.0337 - val_acc: 0.6790\n",
      "Epoch 66/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.2545 - acc: 0.5311 - val_loss: 1.0264 - val_acc: 0.6800\n",
      "Epoch 67/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 1.2311 - acc: 0.5397 - val_loss: 1.0184 - val_acc: 0.6800\n",
      "Epoch 68/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.2161 - acc: 0.5449 - val_loss: 1.0091 - val_acc: 0.6890\n",
      "Epoch 69/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.2216 - acc: 0.5429 - val_loss: 1.0007 - val_acc: 0.6900\n",
      "Epoch 70/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.2116 - acc: 0.5449 - val_loss: 0.9949 - val_acc: 0.6900\n",
      "Epoch 71/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.2132 - acc: 0.5486 - val_loss: 0.9860 - val_acc: 0.6960\n",
      "Epoch 72/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.1962 - acc: 0.5514 - val_loss: 0.9809 - val_acc: 0.6950\n",
      "Epoch 73/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 1.1956 - acc: 0.5527 - val_loss: 0.9735 - val_acc: 0.6950\n",
      "Epoch 74/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.1852 - acc: 0.5680 - val_loss: 0.9672 - val_acc: 0.6930\n",
      "Epoch 75/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.1798 - acc: 0.5626 - val_loss: 0.9579 - val_acc: 0.6990\n",
      "Epoch 76/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.1665 - acc: 0.5664 - val_loss: 0.9512 - val_acc: 0.6990\n",
      "Epoch 77/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 1.1430 - acc: 0.5833 - val_loss: 0.9403 - val_acc: 0.6970\n",
      "Epoch 78/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.1699 - acc: 0.5650 - val_loss: 0.9382 - val_acc: 0.6970\n",
      "Epoch 79/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.1634 - acc: 0.5700 - val_loss: 0.9323 - val_acc: 0.6970\n",
      "Epoch 80/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.1352 - acc: 0.5816 - val_loss: 0.9265 - val_acc: 0.7020\n",
      "Epoch 81/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.1349 - acc: 0.5880 - val_loss: 0.9198 - val_acc: 0.7010\n",
      "Epoch 82/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.1291 - acc: 0.5796 - val_loss: 0.9138 - val_acc: 0.6990\n",
      "Epoch 83/200\n",
      "7000/7000 [==============================] - 1s 96us/step - loss: 1.1119 - acc: 0.5830 - val_loss: 0.9065 - val_acc: 0.7050\n",
      "Epoch 84/200\n",
      "7000/7000 [==============================] - 1s 73us/step - loss: 1.1324 - acc: 0.5816 - val_loss: 0.9016 - val_acc: 0.7020\n",
      "Epoch 85/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.1177 - acc: 0.5864 - val_loss: 0.8981 - val_acc: 0.7050\n",
      "Epoch 86/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 1.1156 - acc: 0.5837 - val_loss: 0.8941 - val_acc: 0.7060\n",
      "Epoch 87/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.1184 - acc: 0.5889 - val_loss: 0.8907 - val_acc: 0.7060\n",
      "Epoch 88/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 1.0918 - acc: 0.5917 - val_loss: 0.8853 - val_acc: 0.7070\n",
      "Epoch 89/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.0868 - acc: 0.5901 - val_loss: 0.8782 - val_acc: 0.7100\n",
      "Epoch 90/200\n",
      "7000/7000 [==============================] - 1s 78us/step - loss: 1.0771 - acc: 0.6027 - val_loss: 0.8719 - val_acc: 0.7110\n",
      "Epoch 91/200\n",
      "7000/7000 [==============================] - 0s 70us/step - loss: 1.0801 - acc: 0.6064 - val_loss: 0.8674 - val_acc: 0.7120\n",
      "Epoch 92/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.0792 - acc: 0.6026 - val_loss: 0.8631 - val_acc: 0.7100\n",
      "Epoch 93/200\n",
      "7000/7000 [==============================] - 1s 75us/step - loss: 1.0730 - acc: 0.6036 - val_loss: 0.8580 - val_acc: 0.7080\n",
      "Epoch 94/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 1.0679 - acc: 0.6061 - val_loss: 0.8533 - val_acc: 0.7130\n",
      "Epoch 95/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 1.0540 - acc: 0.6121 - val_loss: 0.8482 - val_acc: 0.7130\n",
      "Epoch 96/200\n",
      "7000/7000 [==============================] - 1s 74us/step - loss: 1.0516 - acc: 0.6127 - val_loss: 0.8463 - val_acc: 0.7150\n",
      "Epoch 97/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 1.0521 - acc: 0.6129 - val_loss: 0.8401 - val_acc: 0.7160\n",
      "Epoch 98/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.0412 - acc: 0.6110 - val_loss: 0.8368 - val_acc: 0.7150\n",
      "Epoch 99/200\n",
      "7000/7000 [==============================] - 0s 58us/step - loss: 1.0301 - acc: 0.6206 - val_loss: 0.8321 - val_acc: 0.7220\n",
      "Epoch 100/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.0382 - acc: 0.6127 - val_loss: 0.8291 - val_acc: 0.7200\n",
      "Epoch 101/200\n",
      "7000/7000 [==============================] - 0s 68us/step - loss: 1.0257 - acc: 0.6266 - val_loss: 0.8246 - val_acc: 0.7160\n",
      "Epoch 102/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.0308 - acc: 0.6254 - val_loss: 0.8240 - val_acc: 0.7190\n",
      "Epoch 103/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 1.0463 - acc: 0.6203 - val_loss: 0.8218 - val_acc: 0.7230\n",
      "Epoch 104/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 1.0089 - acc: 0.6361 - val_loss: 0.8153 - val_acc: 0.7230\n",
      "Epoch 105/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 1.0296 - acc: 0.6213 - val_loss: 0.8137 - val_acc: 0.7210\n",
      "Epoch 106/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.0076 - acc: 0.6274 - val_loss: 0.8104 - val_acc: 0.7230\n",
      "Epoch 107/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 1.0087 - acc: 0.6249 - val_loss: 0.8053 - val_acc: 0.7240\n",
      "Epoch 108/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 1.0174 - acc: 0.6243 - val_loss: 0.8049 - val_acc: 0.7240\n",
      "Epoch 109/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 1.0044 - acc: 0.6314 - val_loss: 0.8036 - val_acc: 0.7270\n",
      "Epoch 110/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.9924 - acc: 0.6301 - val_loss: 0.7967 - val_acc: 0.7260\n",
      "Epoch 111/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.9990 - acc: 0.6267 - val_loss: 0.7951 - val_acc: 0.7260\n",
      "Epoch 112/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.9997 - acc: 0.6281 - val_loss: 0.7918 - val_acc: 0.7270\n",
      "Epoch 113/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.9938 - acc: 0.6327 - val_loss: 0.7894 - val_acc: 0.7270\n",
      "Epoch 114/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.9908 - acc: 0.6354 - val_loss: 0.7864 - val_acc: 0.7270\n",
      "Epoch 115/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.9820 - acc: 0.6420 - val_loss: 0.7863 - val_acc: 0.7280\n",
      "Epoch 116/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.9743 - acc: 0.6436 - val_loss: 0.7819 - val_acc: 0.7290\n",
      "Epoch 117/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.9864 - acc: 0.6357 - val_loss: 0.7788 - val_acc: 0.7330\n",
      "Epoch 118/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.9703 - acc: 0.6403 - val_loss: 0.7786 - val_acc: 0.7340\n",
      "Epoch 119/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 0.9835 - acc: 0.6401 - val_loss: 0.7734 - val_acc: 0.7310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.9813 - acc: 0.6426 - val_loss: 0.7724 - val_acc: 0.7320\n",
      "Epoch 121/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.9582 - acc: 0.6509 - val_loss: 0.7719 - val_acc: 0.7320\n",
      "Epoch 122/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.9679 - acc: 0.6444 - val_loss: 0.7662 - val_acc: 0.7310\n",
      "Epoch 123/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 0.9474 - acc: 0.6577 - val_loss: 0.7626 - val_acc: 0.7290\n",
      "Epoch 124/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.9408 - acc: 0.6513 - val_loss: 0.7614 - val_acc: 0.7340\n",
      "Epoch 125/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.9512 - acc: 0.6513 - val_loss: 0.7580 - val_acc: 0.7320\n",
      "Epoch 126/200\n",
      "7000/7000 [==============================] - 0s 69us/step - loss: 0.9399 - acc: 0.6503 - val_loss: 0.7543 - val_acc: 0.7330\n",
      "Epoch 127/200\n",
      "7000/7000 [==============================] - 1s 97us/step - loss: 0.9469 - acc: 0.6554 - val_loss: 0.7529 - val_acc: 0.7340\n",
      "Epoch 128/200\n",
      "7000/7000 [==============================] - 0s 69us/step - loss: 0.9348 - acc: 0.6529 - val_loss: 0.7481 - val_acc: 0.7340\n",
      "Epoch 129/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.9287 - acc: 0.6641 - val_loss: 0.7474 - val_acc: 0.7330\n",
      "Epoch 130/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 0.9466 - acc: 0.6520 - val_loss: 0.7471 - val_acc: 0.7330\n",
      "Epoch 131/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.9477 - acc: 0.6527 - val_loss: 0.7455 - val_acc: 0.7360\n",
      "Epoch 132/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.9398 - acc: 0.6531 - val_loss: 0.7463 - val_acc: 0.7350\n",
      "Epoch 133/200\n",
      "7000/7000 [==============================] - 1s 82us/step - loss: 0.9180 - acc: 0.6617 - val_loss: 0.7414 - val_acc: 0.7410\n",
      "Epoch 134/200\n",
      "7000/7000 [==============================] - 1s 76us/step - loss: 0.9195 - acc: 0.6631 - val_loss: 0.7378 - val_acc: 0.7400\n",
      "Epoch 135/200\n",
      "7000/7000 [==============================] - 0s 69us/step - loss: 0.9169 - acc: 0.6684 - val_loss: 0.7375 - val_acc: 0.7380\n",
      "Epoch 136/200\n",
      "7000/7000 [==============================] - 0s 59us/step - loss: 0.9079 - acc: 0.6710 - val_loss: 0.7348 - val_acc: 0.7370\n",
      "Epoch 137/200\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.9043 - acc: 0.6656 - val_loss: 0.7336 - val_acc: 0.7380\n",
      "Epoch 138/200\n",
      "7000/7000 [==============================] - 0s 69us/step - loss: 0.9177 - acc: 0.6603 - val_loss: 0.7320 - val_acc: 0.7370\n",
      "Epoch 139/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.8906 - acc: 0.6757 - val_loss: 0.7305 - val_acc: 0.7360\n",
      "Epoch 140/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.9111 - acc: 0.6620 - val_loss: 0.7273 - val_acc: 0.7390\n",
      "Epoch 141/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.9177 - acc: 0.6601 - val_loss: 0.7315 - val_acc: 0.7370\n",
      "Epoch 142/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.8994 - acc: 0.6687 - val_loss: 0.7254 - val_acc: 0.7380\n",
      "Epoch 143/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.8865 - acc: 0.6739 - val_loss: 0.7232 - val_acc: 0.7380\n",
      "Epoch 144/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 0.9044 - acc: 0.6676 - val_loss: 0.7219 - val_acc: 0.7370\n",
      "Epoch 145/200\n",
      "7000/7000 [==============================] - 0s 68us/step - loss: 0.9138 - acc: 0.6644 - val_loss: 0.7237 - val_acc: 0.7390\n",
      "Epoch 146/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 0.8951 - acc: 0.6801 - val_loss: 0.7205 - val_acc: 0.7380\n",
      "Epoch 147/200\n",
      "7000/7000 [==============================] - 0s 68us/step - loss: 0.8879 - acc: 0.6716 - val_loss: 0.7187 - val_acc: 0.7390\n",
      "Epoch 148/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.8871 - acc: 0.6760 - val_loss: 0.7167 - val_acc: 0.7390\n",
      "Epoch 149/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 0.8828 - acc: 0.6759 - val_loss: 0.7146 - val_acc: 0.7410\n",
      "Epoch 150/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.8810 - acc: 0.6711 - val_loss: 0.7146 - val_acc: 0.7410\n",
      "Epoch 151/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 0.8825 - acc: 0.6781 - val_loss: 0.7126 - val_acc: 0.7410\n",
      "Epoch 152/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.8905 - acc: 0.6713 - val_loss: 0.7139 - val_acc: 0.7410\n",
      "Epoch 153/200\n",
      "7000/7000 [==============================] - 0s 69us/step - loss: 0.8735 - acc: 0.6820 - val_loss: 0.7103 - val_acc: 0.7430\n",
      "Epoch 154/200\n",
      "7000/7000 [==============================] - 0s 68us/step - loss: 0.8877 - acc: 0.6709 - val_loss: 0.7081 - val_acc: 0.7480\n",
      "Epoch 155/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.8674 - acc: 0.6786 - val_loss: 0.7077 - val_acc: 0.7450\n",
      "Epoch 156/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.8535 - acc: 0.6894 - val_loss: 0.7064 - val_acc: 0.7450\n",
      "Epoch 157/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.8677 - acc: 0.6813 - val_loss: 0.7034 - val_acc: 0.7490\n",
      "Epoch 158/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.8672 - acc: 0.6811 - val_loss: 0.7049 - val_acc: 0.7460\n",
      "Epoch 159/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.8565 - acc: 0.6869 - val_loss: 0.7023 - val_acc: 0.7490\n",
      "Epoch 160/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 0.8852 - acc: 0.6824 - val_loss: 0.7031 - val_acc: 0.7440\n",
      "Epoch 161/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 0.8768 - acc: 0.6820 - val_loss: 0.6999 - val_acc: 0.7460\n",
      "Epoch 162/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 0.8649 - acc: 0.6764 - val_loss: 0.6988 - val_acc: 0.7480\n",
      "Epoch 163/200\n",
      "7000/7000 [==============================] - 0s 67us/step - loss: 0.8641 - acc: 0.6850 - val_loss: 0.6969 - val_acc: 0.7500\n",
      "Epoch 164/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 0.8558 - acc: 0.6843 - val_loss: 0.6967 - val_acc: 0.7470\n",
      "Epoch 165/200\n",
      "7000/7000 [==============================] - 0s 68us/step - loss: 0.8495 - acc: 0.6896 - val_loss: 0.6934 - val_acc: 0.7470\n",
      "Epoch 166/200\n",
      "7000/7000 [==============================] - 0s 68us/step - loss: 0.8553 - acc: 0.6826 - val_loss: 0.6904 - val_acc: 0.7460\n",
      "Epoch 167/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 0.8507 - acc: 0.6880 - val_loss: 0.6924 - val_acc: 0.7470\n",
      "Epoch 168/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.8434 - acc: 0.6904 - val_loss: 0.6908 - val_acc: 0.7480\n",
      "Epoch 169/200\n",
      "7000/7000 [==============================] - 1s 77us/step - loss: 0.8490 - acc: 0.6874 - val_loss: 0.6922 - val_acc: 0.7440\n",
      "Epoch 170/200\n",
      "7000/7000 [==============================] - 1s 89us/step - loss: 0.8343 - acc: 0.6967 - val_loss: 0.6873 - val_acc: 0.7470\n",
      "Epoch 171/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.8463 - acc: 0.6874 - val_loss: 0.6895 - val_acc: 0.7460\n",
      "Epoch 172/200\n",
      "7000/7000 [==============================] - 0s 69us/step - loss: 0.8470 - acc: 0.6874 - val_loss: 0.6922 - val_acc: 0.7440\n",
      "Epoch 173/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 0.8434 - acc: 0.6953 - val_loss: 0.6861 - val_acc: 0.7450\n",
      "Epoch 174/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.8208 - acc: 0.7023 - val_loss: 0.6826 - val_acc: 0.7510\n",
      "Epoch 175/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.8421 - acc: 0.6937 - val_loss: 0.6845 - val_acc: 0.7470\n",
      "Epoch 176/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.8280 - acc: 0.6970 - val_loss: 0.6852 - val_acc: 0.7480\n",
      "Epoch 177/200\n",
      "7000/7000 [==============================] - 1s 84us/step - loss: 0.8413 - acc: 0.6916 - val_loss: 0.6840 - val_acc: 0.7490\n",
      "Epoch 178/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.8247 - acc: 0.6971 - val_loss: 0.6827 - val_acc: 0.7480\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 65us/step - loss: 0.8055 - acc: 0.7053 - val_loss: 0.6802 - val_acc: 0.7540\n",
      "Epoch 180/200\n",
      "7000/7000 [==============================] - 1s 81us/step - loss: 0.8132 - acc: 0.7023 - val_loss: 0.6817 - val_acc: 0.7480\n",
      "Epoch 181/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 0.8165 - acc: 0.7021 - val_loss: 0.6773 - val_acc: 0.7510\n",
      "Epoch 182/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.8052 - acc: 0.7047 - val_loss: 0.6783 - val_acc: 0.7520\n",
      "Epoch 183/200\n",
      "7000/7000 [==============================] - 1s 83us/step - loss: 0.8154 - acc: 0.7000 - val_loss: 0.6746 - val_acc: 0.7520\n",
      "Epoch 184/200\n",
      "7000/7000 [==============================] - 0s 70us/step - loss: 0.8238 - acc: 0.6994 - val_loss: 0.6757 - val_acc: 0.7510\n",
      "Epoch 185/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 0.8055 - acc: 0.7031 - val_loss: 0.6720 - val_acc: 0.7500\n",
      "Epoch 186/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.8026 - acc: 0.7049 - val_loss: 0.6713 - val_acc: 0.7530\n",
      "Epoch 187/200\n",
      "7000/7000 [==============================] - 0s 62us/step - loss: 0.8188 - acc: 0.7030 - val_loss: 0.6696 - val_acc: 0.7540\n",
      "Epoch 188/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.8109 - acc: 0.7071 - val_loss: 0.6722 - val_acc: 0.7560\n",
      "Epoch 189/200\n",
      "7000/7000 [==============================] - 0s 64us/step - loss: 0.7855 - acc: 0.7079 - val_loss: 0.6689 - val_acc: 0.7540\n",
      "Epoch 190/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.7945 - acc: 0.7094 - val_loss: 0.6674 - val_acc: 0.7540\n",
      "Epoch 191/200\n",
      "7000/7000 [==============================] - 0s 66us/step - loss: 0.7989 - acc: 0.7019 - val_loss: 0.6698 - val_acc: 0.7590\n",
      "Epoch 192/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.8102 - acc: 0.7050 - val_loss: 0.6674 - val_acc: 0.7580\n",
      "Epoch 193/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 0.7887 - acc: 0.7131 - val_loss: 0.6654 - val_acc: 0.7590\n",
      "Epoch 194/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.8040 - acc: 0.7063 - val_loss: 0.6683 - val_acc: 0.7530\n",
      "Epoch 195/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.8030 - acc: 0.7027 - val_loss: 0.6649 - val_acc: 0.7570\n",
      "Epoch 196/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.7918 - acc: 0.7059 - val_loss: 0.6635 - val_acc: 0.7590\n",
      "Epoch 197/200\n",
      "7000/7000 [==============================] - 0s 65us/step - loss: 0.7948 - acc: 0.7133 - val_loss: 0.6656 - val_acc: 0.7530\n",
      "Epoch 198/200\n",
      "7000/7000 [==============================] - 0s 63us/step - loss: 0.7989 - acc: 0.7064 - val_loss: 0.6647 - val_acc: 0.7580\n",
      "Epoch 199/200\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.8013 - acc: 0.7040 - val_loss: 0.6652 - val_acc: 0.7540\n",
      "Epoch 200/200\n",
      "7000/7000 [==============================] - 0s 60us/step - loss: 0.7767 - acc: 0.7187 - val_loss: 0.6615 - val_acc: 0.7590\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 1s 73us/step\n",
      "2000/2000 [==============================] - 0s 62us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4835193752220699, 0.8417142856461661]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6462417569160461, 0.758]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. You actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple your data set, and see what happens. Note that you are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "random_sample() takes at most 1 positional argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-65f98fb75e0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# train test split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_hot_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.random_sample\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: random_sample() takes at most 1 positional argument (2 given)"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "np.random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = np.random.sample(2000, 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7000/7000 [==============================] - 1s 100us/step - loss: 1.9643 - acc: 0.1583 - val_loss: 1.9249 - val_acc: 0.2010\n",
      "Epoch 2/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.9347 - acc: 0.1831 - val_loss: 1.9114 - val_acc: 0.2230\n",
      "Epoch 3/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.9180 - acc: 0.2067 - val_loss: 1.8988 - val_acc: 0.2450\n",
      "Epoch 4/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 1.9028 - acc: 0.2247 - val_loss: 1.8846 - val_acc: 0.2620\n",
      "Epoch 5/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.8870 - acc: 0.2426 - val_loss: 1.8680 - val_acc: 0.2720\n",
      "Epoch 6/120\n",
      "7000/7000 [==============================] - 0s 48us/step - loss: 1.8697 - acc: 0.2557 - val_loss: 1.8502 - val_acc: 0.2860\n",
      "Epoch 7/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.8508 - acc: 0.2699 - val_loss: 1.8306 - val_acc: 0.2870\n",
      "Epoch 8/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.8301 - acc: 0.2846 - val_loss: 1.8100 - val_acc: 0.3100\n",
      "Epoch 9/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 1.8074 - acc: 0.3011 - val_loss: 1.7888 - val_acc: 0.3340\n",
      "Epoch 10/120\n",
      "7000/7000 [==============================] - 0s 41us/step - loss: 1.7829 - acc: 0.3221 - val_loss: 1.7660 - val_acc: 0.3600\n",
      "Epoch 11/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.7556 - acc: 0.3466 - val_loss: 1.7370 - val_acc: 0.3720\n",
      "Epoch 12/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 1.7261 - acc: 0.3584 - val_loss: 1.7103 - val_acc: 0.3960\n",
      "Epoch 13/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 1.6939 - acc: 0.3877 - val_loss: 1.6769 - val_acc: 0.4080\n",
      "Epoch 14/120\n",
      "7000/7000 [==============================] - 0s 45us/step - loss: 1.6584 - acc: 0.4080 - val_loss: 1.6438 - val_acc: 0.4250\n",
      "Epoch 15/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.6205 - acc: 0.4356 - val_loss: 1.6064 - val_acc: 0.4480\n",
      "Epoch 16/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.5794 - acc: 0.4571 - val_loss: 1.5686 - val_acc: 0.4750\n",
      "Epoch 17/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.5362 - acc: 0.4980 - val_loss: 1.5273 - val_acc: 0.4920\n",
      "Epoch 18/120\n",
      "7000/7000 [==============================] - 0s 30us/step - loss: 1.4912 - acc: 0.5224 - val_loss: 1.4837 - val_acc: 0.5180\n",
      "Epoch 19/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.4446 - acc: 0.5474 - val_loss: 1.4399 - val_acc: 0.5440\n",
      "Epoch 20/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.3966 - acc: 0.5681 - val_loss: 1.3974 - val_acc: 0.5710\n",
      "Epoch 21/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.3484 - acc: 0.5861 - val_loss: 1.3579 - val_acc: 0.5910\n",
      "Epoch 22/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.3004 - acc: 0.6106 - val_loss: 1.3101 - val_acc: 0.6140\n",
      "Epoch 23/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 1.2534 - acc: 0.6290 - val_loss: 1.2650 - val_acc: 0.6180\n",
      "Epoch 24/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 1.2082 - acc: 0.6404 - val_loss: 1.2279 - val_acc: 0.6310\n",
      "Epoch 25/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 1.1645 - acc: 0.6557 - val_loss: 1.1897 - val_acc: 0.6350\n",
      "Epoch 26/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.1228 - acc: 0.6709 - val_loss: 1.1518 - val_acc: 0.6480\n",
      "Epoch 27/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 1.0841 - acc: 0.6797 - val_loss: 1.1181 - val_acc: 0.6460\n",
      "Epoch 28/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.0476 - acc: 0.6890 - val_loss: 1.0851 - val_acc: 0.6490\n",
      "Epoch 29/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 1.0145 - acc: 0.6971 - val_loss: 1.0549 - val_acc: 0.6590\n",
      "Epoch 30/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.9828 - acc: 0.7039 - val_loss: 1.0317 - val_acc: 0.6590\n",
      "Epoch 31/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.9536 - acc: 0.7104 - val_loss: 1.0089 - val_acc: 0.6700\n",
      "Epoch 32/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.9267 - acc: 0.7154 - val_loss: 0.9819 - val_acc: 0.6820\n",
      "Epoch 33/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.9022 - acc: 0.7196 - val_loss: 0.9619 - val_acc: 0.6860\n",
      "Epoch 34/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8788 - acc: 0.7226 - val_loss: 0.9437 - val_acc: 0.6860\n",
      "Epoch 35/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.8578 - acc: 0.7277 - val_loss: 0.9239 - val_acc: 0.6880\n",
      "Epoch 36/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.8383 - acc: 0.7321 - val_loss: 0.9106 - val_acc: 0.6960\n",
      "Epoch 37/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.8194 - acc: 0.7357 - val_loss: 0.8976 - val_acc: 0.6920\n",
      "Epoch 38/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.8027 - acc: 0.7383 - val_loss: 0.8822 - val_acc: 0.6900\n",
      "Epoch 39/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.7869 - acc: 0.7431 - val_loss: 0.8715 - val_acc: 0.6980\n",
      "Epoch 40/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.7719 - acc: 0.7461 - val_loss: 0.8587 - val_acc: 0.6990\n",
      "Epoch 41/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.7581 - acc: 0.7474 - val_loss: 0.8472 - val_acc: 0.6950\n",
      "Epoch 42/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.7452 - acc: 0.7521 - val_loss: 0.8379 - val_acc: 0.7000\n",
      "Epoch 43/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.7329 - acc: 0.7520 - val_loss: 0.8336 - val_acc: 0.7010\n",
      "Epoch 44/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.7212 - acc: 0.7574 - val_loss: 0.8221 - val_acc: 0.7010\n",
      "Epoch 45/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.7106 - acc: 0.7601 - val_loss: 0.8142 - val_acc: 0.7020\n",
      "Epoch 46/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.7002 - acc: 0.7647 - val_loss: 0.8086 - val_acc: 0.7040\n",
      "Epoch 47/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.6902 - acc: 0.7659 - val_loss: 0.7989 - val_acc: 0.7030\n",
      "Epoch 48/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.6809 - acc: 0.7697 - val_loss: 0.7954 - val_acc: 0.7060\n",
      "Epoch 49/120\n",
      "7000/7000 [==============================] - 0s 53us/step - loss: 0.6719 - acc: 0.7703 - val_loss: 0.7897 - val_acc: 0.7080\n",
      "Epoch 50/120\n",
      "7000/7000 [==============================] - 0s 52us/step - loss: 0.6634 - acc: 0.7743 - val_loss: 0.7856 - val_acc: 0.7110\n",
      "Epoch 51/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.6554 - acc: 0.7800 - val_loss: 0.7803 - val_acc: 0.7130\n",
      "Epoch 52/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.6474 - acc: 0.7773 - val_loss: 0.7744 - val_acc: 0.7140\n",
      "Epoch 53/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.6398 - acc: 0.7807 - val_loss: 0.7710 - val_acc: 0.7120\n",
      "Epoch 54/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.6328 - acc: 0.7844 - val_loss: 0.7705 - val_acc: 0.7150\n",
      "Epoch 55/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.6256 - acc: 0.7869 - val_loss: 0.7704 - val_acc: 0.7150\n",
      "Epoch 56/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.6187 - acc: 0.7896 - val_loss: 0.7594 - val_acc: 0.7150\n",
      "Epoch 57/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.6124 - acc: 0.7899 - val_loss: 0.7634 - val_acc: 0.7200\n",
      "Epoch 58/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.6055 - acc: 0.7944 - val_loss: 0.7593 - val_acc: 0.7220\n",
      "Epoch 59/120\n",
      "7000/7000 [==============================] - 0s 40us/step - loss: 0.5997 - acc: 0.7956 - val_loss: 0.7504 - val_acc: 0.7210\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.5935 - acc: 0.7987 - val_loss: 0.7526 - val_acc: 0.7180\n",
      "Epoch 61/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.5879 - acc: 0.7979 - val_loss: 0.7473 - val_acc: 0.7260\n",
      "Epoch 62/120\n",
      "7000/7000 [==============================] - 0s 30us/step - loss: 0.5825 - acc: 0.8017 - val_loss: 0.7434 - val_acc: 0.7250\n",
      "Epoch 63/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.5765 - acc: 0.8026 - val_loss: 0.7436 - val_acc: 0.7250\n",
      "Epoch 64/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.5713 - acc: 0.8041 - val_loss: 0.7368 - val_acc: 0.7270\n",
      "Epoch 65/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.5660 - acc: 0.8069 - val_loss: 0.7352 - val_acc: 0.7290\n",
      "Epoch 66/120\n",
      "7000/7000 [==============================] - 0s 42us/step - loss: 0.5606 - acc: 0.8090 - val_loss: 0.7308 - val_acc: 0.7280\n",
      "Epoch 67/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.5557 - acc: 0.8109 - val_loss: 0.7316 - val_acc: 0.7250\n",
      "Epoch 68/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.5506 - acc: 0.8119 - val_loss: 0.7348 - val_acc: 0.7310\n",
      "Epoch 69/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.5457 - acc: 0.8129 - val_loss: 0.7301 - val_acc: 0.7280\n",
      "Epoch 70/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.5409 - acc: 0.8159 - val_loss: 0.7304 - val_acc: 0.7220\n",
      "Epoch 71/120\n",
      "7000/7000 [==============================] - 0s 61us/step - loss: 0.5364 - acc: 0.8169 - val_loss: 0.7213 - val_acc: 0.7320\n",
      "Epoch 72/120\n",
      "7000/7000 [==============================] - 0s 46us/step - loss: 0.5317 - acc: 0.8187 - val_loss: 0.7255 - val_acc: 0.7290\n",
      "Epoch 73/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.5272 - acc: 0.8211 - val_loss: 0.7198 - val_acc: 0.7260\n",
      "Epoch 74/120\n",
      "7000/7000 [==============================] - 0s 43us/step - loss: 0.5229 - acc: 0.8214 - val_loss: 0.7217 - val_acc: 0.7290\n",
      "Epoch 75/120\n",
      "7000/7000 [==============================] - 0s 39us/step - loss: 0.5184 - acc: 0.8270 - val_loss: 0.7197 - val_acc: 0.7270\n",
      "Epoch 76/120\n",
      "7000/7000 [==============================] - 0s 44us/step - loss: 0.5143 - acc: 0.8266 - val_loss: 0.7165 - val_acc: 0.7310\n",
      "Epoch 77/120\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.5099 - acc: 0.8273 - val_loss: 0.7184 - val_acc: 0.7320\n",
      "Epoch 78/120\n",
      "7000/7000 [==============================] - 0s 38us/step - loss: 0.5057 - acc: 0.8287 - val_loss: 0.7142 - val_acc: 0.7310\n",
      "Epoch 79/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.5017 - acc: 0.8309 - val_loss: 0.7186 - val_acc: 0.7290\n",
      "Epoch 80/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.4978 - acc: 0.8336 - val_loss: 0.7159 - val_acc: 0.7350\n",
      "Epoch 81/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.4940 - acc: 0.8331 - val_loss: 0.7114 - val_acc: 0.7350\n",
      "Epoch 82/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.4896 - acc: 0.8374 - val_loss: 0.7097 - val_acc: 0.7350\n",
      "Epoch 83/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.4862 - acc: 0.8384 - val_loss: 0.7091 - val_acc: 0.7340\n",
      "Epoch 84/120\n",
      "7000/7000 [==============================] - 0s 30us/step - loss: 0.4823 - acc: 0.8373 - val_loss: 0.7087 - val_acc: 0.7350\n",
      "Epoch 85/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.4782 - acc: 0.8393 - val_loss: 0.7129 - val_acc: 0.7320\n",
      "Epoch 86/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.4746 - acc: 0.8420 - val_loss: 0.7068 - val_acc: 0.7400\n",
      "Epoch 87/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.4710 - acc: 0.8419 - val_loss: 0.7139 - val_acc: 0.7360\n",
      "Epoch 88/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.4676 - acc: 0.8447 - val_loss: 0.7071 - val_acc: 0.7300\n",
      "Epoch 89/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.4641 - acc: 0.8459 - val_loss: 0.7120 - val_acc: 0.7310\n",
      "Epoch 90/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.4602 - acc: 0.8479 - val_loss: 0.7028 - val_acc: 0.7350\n",
      "Epoch 91/120\n",
      "7000/7000 [==============================] - 0s 31us/step - loss: 0.4565 - acc: 0.8471 - val_loss: 0.7056 - val_acc: 0.7290\n",
      "Epoch 92/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.4526 - acc: 0.8513 - val_loss: 0.7019 - val_acc: 0.7380\n",
      "Epoch 93/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.4492 - acc: 0.8506 - val_loss: 0.7010 - val_acc: 0.7300\n",
      "Epoch 94/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.4463 - acc: 0.8491 - val_loss: 0.7090 - val_acc: 0.7380\n",
      "Epoch 95/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.4428 - acc: 0.8529 - val_loss: 0.7042 - val_acc: 0.7370\n",
      "Epoch 96/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.4392 - acc: 0.8534 - val_loss: 0.7058 - val_acc: 0.7330\n",
      "Epoch 97/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.4364 - acc: 0.8549 - val_loss: 0.7014 - val_acc: 0.7300\n",
      "Epoch 98/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.4328 - acc: 0.8553 - val_loss: 0.7005 - val_acc: 0.7380\n",
      "Epoch 99/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.4299 - acc: 0.8570 - val_loss: 0.7000 - val_acc: 0.7310\n",
      "Epoch 100/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.4263 - acc: 0.8573 - val_loss: 0.7050 - val_acc: 0.7240\n",
      "Epoch 101/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.4233 - acc: 0.8599 - val_loss: 0.7071 - val_acc: 0.7380\n",
      "Epoch 102/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.4205 - acc: 0.8626 - val_loss: 0.6991 - val_acc: 0.7320\n",
      "Epoch 103/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.4169 - acc: 0.8626 - val_loss: 0.7022 - val_acc: 0.7380\n",
      "Epoch 104/120\n",
      "7000/7000 [==============================] - 0s 47us/step - loss: 0.4139 - acc: 0.8634 - val_loss: 0.7029 - val_acc: 0.7320\n",
      "Epoch 105/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.4111 - acc: 0.8631 - val_loss: 0.7023 - val_acc: 0.7390\n",
      "Epoch 106/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.4076 - acc: 0.8647 - val_loss: 0.7089 - val_acc: 0.7380\n",
      "Epoch 107/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.4050 - acc: 0.8664 - val_loss: 0.6973 - val_acc: 0.7330\n",
      "Epoch 108/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.4022 - acc: 0.8683 - val_loss: 0.7034 - val_acc: 0.7280\n",
      "Epoch 109/120\n",
      "7000/7000 [==============================] - 0s 32us/step - loss: 0.3990 - acc: 0.8723 - val_loss: 0.7020 - val_acc: 0.7250\n",
      "Epoch 110/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.3963 - acc: 0.8699 - val_loss: 0.6998 - val_acc: 0.7340\n",
      "Epoch 111/120\n",
      "7000/7000 [==============================] - 0s 37us/step - loss: 0.3928 - acc: 0.8721 - val_loss: 0.7035 - val_acc: 0.7260\n",
      "Epoch 112/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.3904 - acc: 0.8749 - val_loss: 0.6966 - val_acc: 0.7370\n",
      "Epoch 113/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.3874 - acc: 0.8749 - val_loss: 0.6981 - val_acc: 0.7240\n",
      "Epoch 114/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.3842 - acc: 0.8763 - val_loss: 0.7017 - val_acc: 0.7380\n",
      "Epoch 115/120\n",
      "7000/7000 [==============================] - 0s 33us/step - loss: 0.3818 - acc: 0.8761 - val_loss: 0.7003 - val_acc: 0.7260\n",
      "Epoch 116/120\n",
      "7000/7000 [==============================] - 0s 35us/step - loss: 0.3787 - acc: 0.8793 - val_loss: 0.6987 - val_acc: 0.7310\n",
      "Epoch 117/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.3759 - acc: 0.8789 - val_loss: 0.7001 - val_acc: 0.7390\n",
      "Epoch 118/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.3731 - acc: 0.8786 - val_loss: 0.6972 - val_acc: 0.7320\n",
      "Epoch 119/120\n",
      "7000/7000 [==============================] - 0s 34us/step - loss: 0.3705 - acc: 0.8809 - val_loss: 0.7023 - val_acc: 0.7400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7000/7000 [==============================] - 0s 36us/step - loss: 0.3677 - acc: 0.8823 - val_loss: 0.6984 - val_acc: 0.7370\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000/7000 [==============================] - 0s 70us/step\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-77d72b37b950>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mresults_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_train_final\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresults_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.29492314792401864, 0.8997272727272727]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5750258494615554, 0.805]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, you were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). your test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, you not only built an initial deep-learning model, you then used a validation set to tune your model using various types of regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
